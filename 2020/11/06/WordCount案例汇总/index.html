<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="WordCount案例汇总"><meta name="keywords" content="Spark,Hadoop,Scala,Flink"><meta name="author" content="惊羽"><meta name="copyright" content="惊羽"><title>WordCount案例汇总 | 惊羽的博客</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80%E8%AF%B4%E6%98%8E"><span class="toc-number">1.</span> <span class="toc-text">前言说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce"><span class="toc-number">2.</span> <span class="toc-text">MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MapTask"><span class="toc-number">2.1.</span> <span class="toc-text">MapTask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReduceTask"><span class="toc-number">2.2.</span> <span class="toc-text">ReduceTask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WordCountMain%E7%AE%80%E5%86%99%E7%89%88"><span class="toc-number">2.3.</span> <span class="toc-text">WordCountMain简写版</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WordCountMain-jar%E5%8C%85%E7%89%88"><span class="toc-number">2.4.</span> <span class="toc-text">WordCountMain jar包版</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scala"><span class="toc-number">3.</span> <span class="toc-text">Scala</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MainActor"><span class="toc-number">3.1.</span> <span class="toc-text">MainActor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WordCountActor"><span class="toc-number">3.2.</span> <span class="toc-text">WordCountActor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WordCountPackage"><span class="toc-number">3.3.</span> <span class="toc-text">WordCountPackage</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark"><span class="toc-number">4.</span> <span class="toc-text">Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkCore"><span class="toc-number">4.1.</span> <span class="toc-text">SparkCore</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkSQL"><span class="toc-number">4.2.</span> <span class="toc-text">SparkSQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming"><span class="toc-number">4.3.</span> <span class="toc-text">SparkStreaming</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming-amp-Kafka"><span class="toc-number">4.4.</span> <span class="toc-text">SparkStreaming &amp; Kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StructuredStreaming"><span class="toc-number">4.5.</span> <span class="toc-text">StructuredStreaming</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FLink"><span class="toc-number">5.</span> <span class="toc-text">FLink</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E5%A4%84%E7%90%86-DataSet"><span class="toc-number">5.1.</span> <span class="toc-text">批处理 DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E5%A4%84%E7%90%86-DataStream"><span class="toc-number">5.2.</span> <span class="toc-text">流处理 DataStream</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">惊羽</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">23</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">16</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">8</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/post/wordcount.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">惊羽的博客</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">WordCount案例汇总</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-06</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C/">日常工作</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount</p>
<p>经典永不过时，使用的很多函数和方法也是常用的。</p>
<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desc</span>: 自定义的Map规则, 用来实现把: k1, v1 -&gt; k2, v2, 需要 继承Mapper类, 重写map方法.</span></span><br><span class="line"><span class="comment"> * 各个数据解释:</span></span><br><span class="line"><span class="comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span></span><br><span class="line"><span class="comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span></span><br><span class="line"><span class="comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span></span><br><span class="line"><span class="comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapTask</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写map方法,用来将K1 V2 转换成 K2 V2</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key     k1</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value   v1</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context 内容对象,用来写出K2,V2</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1.获取行偏移量,没有什么用处,我们用于测试看看的</span></span><br><span class="line">        <span class="keyword">long</span> index = key.get();</span><br><span class="line">        System.out.println(<span class="string">&quot;行偏移量是: &quot;</span> + index);</span><br><span class="line">        <span class="comment">//2.获取整行数据</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">//3.读取并做非空校验,判断值是否相等,也判断地址值是否相等</span></span><br><span class="line">        <span class="keyword">if</span> (line != <span class="keyword">null</span> &amp;&amp; !<span class="string">&quot;&quot;</span>.equals(line)) &#123;</span><br><span class="line">            <span class="comment">//4.切割获取K2,V2</span></span><br><span class="line">            String[] str = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str.length; i++) &#123;</span><br><span class="line">                String s = str[i];</span><br><span class="line">                context.write(<span class="keyword">new</span> Text(s), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desc</span>: 自定义的Reduce规则, 用来实现把: k2, v2的集合 -&gt; k3, v3, 需要 继承Reducer类, 重写reduce方法.</span></span><br><span class="line"><span class="comment"> * 各个数据解释:</span></span><br><span class="line"><span class="comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span></span><br><span class="line"><span class="comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span></span><br><span class="line"><span class="comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span></span><br><span class="line"><span class="comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * shuffle阶段: 分区, 排序, 规约, 分组之后, 数据如下:</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span></span><br><span class="line"><span class="comment"> * v2(的集合): 每个单词的所有次数的集合, 例如: &#123;1, 1&#125;,  &#123;1, 1, 1&#125;, &#123;1, 1&#125;</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * k3: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span></span><br><span class="line"><span class="comment"> * v3: 每个单词的总次数, 例如: 2, 3, 2</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WorkCountReduceTask</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写reduce方法</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写reduce方法,用于把k2,v2 转换成k3,v3</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key     k2</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values  v2的集合(已经经过了分组)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context 内容对象,用来写k3,v3</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1.获取k3,就是每个单词</span></span><br><span class="line">        String word = key.toString();</span><br><span class="line">        <span class="comment">//2.获取v3,就是单词出现的次数</span></span><br><span class="line">        <span class="comment">//2.1先对v2集合求和</span></span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//2.2写出v3</span></span><br><span class="line">        <span class="comment">//context.write(new Text(word),new IntWritable(count));</span></span><br><span class="line">        <span class="comment">//因为v2和v3是一样的,我们可以优化一下</span></span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WordCountMain简写版"><a href="#WordCountMain简写版" class="headerlink" title="WordCountMain简写版"></a>WordCountMain简写版</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里写的是驱动类, 即: 封装MR程序的核心8步的. 它有两种写法:</span></span><br><span class="line"><span class="comment"> * 1. 官方示例版, 即: 完整版.   理解即可, 因为稍显复杂, 用的人较少.</span></span><br><span class="line"><span class="comment"> * 2. 简化版.  推荐掌握.</span></span><br><span class="line"><span class="comment"> * 这里是简化版写法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WorkCountMain</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">new</span> Configuration(), <span class="string">&quot;wordcountMR&quot;</span>);</span><br><span class="line">        <span class="comment">//2.封装MR程序核心8步</span></span><br><span class="line">        <span class="comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job, <span class="keyword">new</span> Path(<span class="string">&quot;file:///d:/test/wordcount/input/wordcount.txt&quot;</span>));</span><br><span class="line">        <span class="comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span></span><br><span class="line">        job.setMapperClass(WordCountMapTask.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//2.3 分区,用默认的</span></span><br><span class="line">        <span class="comment">//2.4 排序,用默认的</span></span><br><span class="line">        <span class="comment">//2.5 规约,用默认的</span></span><br><span class="line">        <span class="comment">//2.6 分组,用默认的</span></span><br><span class="line">        <span class="comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span></span><br><span class="line">        job.setReducerClass(WorkCountReduceTask.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">&quot;file:///d:/test/wordcount/output&quot;</span>));</span><br><span class="line">        <span class="comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span></span><br><span class="line">        <span class="keyword">boolean</span> flag = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span></span><br><span class="line">        System.exit(flag ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WordCountMain-jar包版"><a href="#WordCountMain-jar包版" class="headerlink" title="WordCountMain jar包版"></a>WordCountMain jar包版</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这个代码一会儿是要打包成jar包, 然后放到Yarn集群中运行的, 需要做如下的几件事儿:</span></span><br><span class="line"><span class="comment"> * 1. 在驱动类中设置 jar包的启动类.</span></span><br><span class="line"><span class="comment"> * job.setJarByClass(WordCountMain3.class);</span></span><br><span class="line"><span class="comment"> * 2. 修改数据源文件 和 目的地文件的路径, 改为: 外部传入.</span></span><br><span class="line"><span class="comment"> * TextInputFormat.addInputPath(job, new Path(args[0]));</span></span><br><span class="line"><span class="comment"> * TextOutputFormat.setOutputPath(job, new Path(args[1]));</span></span><br><span class="line"><span class="comment"> * 3. 对我们当前的工程进行打包动作, 打包成: 胖jar, 具体操作为: 取消pom.xml文件中最后一个插件的注释, 然后打包即可.</span></span><br><span class="line"><span class="comment"> * 细节: 修改jar包名字为: wordcount.jar, 方便我们操作.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 4. 在HDFS集群中创建:   /wordcount/input/ 目录</span></span><br><span class="line"><span class="comment"> * 5. 把wordcount.txt 上传到该目录下.</span></span><br><span class="line"><span class="comment"> * 6. 把之前打好的 jar包也上传到 Linux系统中.</span></span><br><span class="line"><span class="comment"> * 7. 运行该jar包即可, 记得: 传入 数据源文件路径, 目的地目录路径.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 名词解释:</span></span><br><span class="line"><span class="comment"> * 胖jar: 指的是一个jar包中还包含有其他的jar包, 这样的jar包就称之为: 胖jar.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 问题1: 为什么需要打包成 胖jar?</span></span><br><span class="line"><span class="comment"> * 答案:</span></span><br><span class="line"><span class="comment"> * 因为目前我们的工程需要依赖 Hadoop环境, 而我们已经在pom.xml文件中配置了,</span></span><br><span class="line"><span class="comment"> * 如果运行的环境中(例如: Linux系统等)没有hadoop环境, 并且我们打包时也没有把hadoop环境打包进去,</span></span><br><span class="line"><span class="comment"> * 将来运行jar包的时候就会出错.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 问题2: 当前工程一定要打包成 胖jar吗?</span></span><br><span class="line"><span class="comment"> * 答案: 不用, 因为我们的 jar包一会儿是放到 Yarn集群中运行的, 它已经自带Hadoop环境, 所以这里可以不打包 胖jar, 只打包我们自己的代码.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WorkCountMain3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">new</span> Configuration(), <span class="string">&quot;wordcountMR&quot;</span>);</span><br><span class="line">        <span class="comment">//细节1: 在驱动类中设置 jar包的启动类.</span></span><br><span class="line">        job.setJarByClass(WorkCountMain3.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.封装MR程序核心8步</span></span><br><span class="line">        <span class="comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        <span class="comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span></span><br><span class="line">        job.setMapperClass(WordCountMapTask.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//2.3 分区,用默认的</span></span><br><span class="line">        <span class="comment">//2.4 排序,用默认的</span></span><br><span class="line">        <span class="comment">//2.5 规约,用默认的</span></span><br><span class="line">        <span class="comment">//2.6 分组,用默认的</span></span><br><span class="line">        <span class="comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span></span><br><span class="line">        job.setReducerClass(WorkCountReduceTask.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">        <span class="comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span></span><br><span class="line">        <span class="keyword">boolean</span> flag = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span></span><br><span class="line">        System.exit(flag ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>基本流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">基本流程</span><br><span class="line"><span class="number">1.</span>传输文件路径到自定义的Actor类,并接收返回值,解析返回值得出统计结果</span><br><span class="line"><span class="number">2.</span>自定义Actor类, 接收文件路径并做解析统计单词再返回给发送者</span><br><span class="line"></span><br><span class="line">需要分别定义<span class="number">3</span>个类</span><br><span class="line">Main入口</span><br><span class="line"><span class="number">1.</span>用于发送文件路径,封装在自定义的单例类里面</span><br><span class="line"><span class="number">2.</span>接收返回值,并做判断是否完成传输, 如果完成就开始解析</span><br><span class="line"><span class="number">3.</span>通过apply方法解析结果,合并结果得出最后结果</span><br><span class="line"></span><br><span class="line">自定义的Actor类</span><br><span class="line"><span class="number">1.</span>接收文件路径信息,做分析统计</span><br><span class="line"><span class="number">2.</span>把结果封装在单例类中,返回给发送者</span><br><span class="line"></span><br><span class="line">自定义的单例类</span><br><span class="line"><span class="number">1.</span>用于封装发送信息的单例类</span><br><span class="line"><span class="number">2.</span>用于返回统计的单例类</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="MainActor"><a href="#MainActor" class="headerlink" title="MainActor"></a>MainActor</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">`<span class="keyword">package</span> com.test.day04.wordcount</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File</span><br><span class="line"><span class="keyword">import</span> scala.actors.Future</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1.发送文件名给WordCountActor</span></span><br><span class="line"><span class="comment"> * 2.接收WordCountActor返回结果并合并</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">object MainActor &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//发送文件名给WordCountActor</span></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">//1.获取文件名</span></span><br><span class="line">    val fileDir = <span class="keyword">new</span> File(<span class="string">&quot;./data&quot;</span>)</span><br><span class="line">    val files: Array[File] = fileDir.listFiles()</span><br><span class="line">    <span class="comment">// 测试是成功获取文件名</span></span><br><span class="line">    <span class="comment">// files.foreach(println(_))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.发送给wordcountactor</span></span><br><span class="line">    val future_Array: Array[Future[Any]] = files.map(f = file =&gt; &#123;</span><br><span class="line">      val name = file.toString</span><br><span class="line">      <span class="comment">//每一个文件名新建对应的线程</span></span><br><span class="line">      val actor = <span class="keyword">new</span> WordCountActor</span><br><span class="line">      <span class="comment">//开启线程并发送给我认定任务</span></span><br><span class="line">      actor.start()</span><br><span class="line">      <span class="comment">//发送的消息封装在这里面并获取结果</span></span><br><span class="line">      val future: Future[Any] = actor !! WordCountTask(name)</span><br><span class="line">      future</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//接收WordCountActor返回结果并合并</span></span><br><span class="line">    <span class="comment">//先判断是否全部文件都处理完毕都有结果,是再处理</span></span><br><span class="line">    <span class="keyword">while</span> (!(future_Array.filter((x) =&gt; &#123;</span><br><span class="line">      !x.isSet</span><br><span class="line">    &#125;)).isEmpty) &#123;&#125;</span><br><span class="line">    <span class="comment">//走到这里, 证明我们可以处理,使用apply获取数据</span></span><br><span class="line">    <span class="comment">//里面的键值对就是多个文件统计结果, 我们还需要合并去重</span></span><br><span class="line">    val wordCount: Array[Map[String, Int]] = future_Array.map((x) =&gt; &#123;</span><br><span class="line">      val results: Any = x.apply()</span><br><span class="line">      val result = results.asInstanceOf[WordCountResult]</span><br><span class="line">      val map: Map[String, Int] = result.map</span><br><span class="line">      map</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//wordCount.foreach(println(_))</span></span><br><span class="line">    <span class="comment">//测试结果</span></span><br><span class="line">    <span class="comment">// Map(e -&gt; 2, f -&gt; 1, a -&gt; 1, b -&gt; 1, c -&gt; 1)</span></span><br><span class="line">    <span class="comment">// Map(e -&gt; 1, a -&gt; 2, b -&gt; 1, c -&gt; 2, d -&gt; 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//合并结果, 先合并成一个Array</span></span><br><span class="line">    val flatten: Array[(String, Int)] = wordCount.flatten</span><br><span class="line">    <span class="comment">//根据Map的key值分组</span></span><br><span class="line">    val wordGroup: Map[String, Array[(String, Int)]] = flatten.groupBy((x) =&gt; &#123;</span><br><span class="line">      x._1</span><br><span class="line">    &#125;)</span><br><span class="line">    val finalResult: Map[String, Int] = wordGroup.map((x) =&gt; &#123;</span><br><span class="line">      val name = x._1</span><br><span class="line">      val size = x._2.size</span><br><span class="line">      name -&gt; size</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    finalResult.foreach(println(_))</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WordCountActor"><a href="#WordCountActor" class="headerlink" title="WordCountActor"></a>WordCountActor</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day04.wordcount</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.actors.Actor</span><br><span class="line"><span class="keyword">import</span> scala.io.Source</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1.接收MainActor的文件名称并进行单词统计</span></span><br><span class="line"><span class="comment"> * 2.将单词统计结果返回给MainActor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountActor</span> <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</span><br><span class="line">  <span class="function">override def <span class="title">act</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">//接收消息</span></span><br><span class="line">    loop &#123;</span><br><span class="line">      react &#123;</span><br><span class="line">        <span class="function"><span class="keyword">case</span> <span class="title">WordCountTask</span><span class="params">(filename)</span> </span>=&gt;</span><br><span class="line">          println(<span class="string">&quot;收到了文件名: &quot;</span> + filename)</span><br><span class="line">          <span class="comment">//解析消息, 通过Source解析消息, 定义文件来源再转化成列表</span></span><br><span class="line">          <span class="comment">//一个元素就是一个一行数据</span></span><br><span class="line">          val words: List[String] = Source.fromFile(filename).getLines().toList</span><br><span class="line">          <span class="comment">//切割获取每一条数据并合并成一个list集合</span></span><br><span class="line">          val word_List: List[String] = words.flatMap((x) =&gt; &#123;</span><br><span class="line">            x.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//按照单词进行分组, 然后聚合统计</span></span><br><span class="line">          val word_Tuples: List[(String, Int)] = word_List.map((x) =&gt; &#123;</span><br><span class="line">            (x, <span class="number">1</span>)</span><br><span class="line">          &#125;)</span><br><span class="line">          val word_Map: Map[String, List[(String, Int)]] = word_Tuples.groupBy((x) =&gt; &#123;</span><br><span class="line">            x._1</span><br><span class="line">          &#125;)</span><br><span class="line">          val wordCountMap: Map[String, Int] = word_Map.map((x) =&gt; &#123;</span><br><span class="line">            val name: String = x._1</span><br><span class="line">            val size: Int = x._2.size</span><br><span class="line">            name -&gt; size</span><br><span class="line">          &#125;)</span><br><span class="line"></span><br><span class="line">          <span class="comment">//把统计结果反馈给Mainactor,装进WordCount</span></span><br><span class="line">          sender ! WordCountResult(wordCountMap)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WordCountPackage"><a href="#WordCountPackage" class="headerlink" title="WordCountPackage"></a>WordCountPackage</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day04.wordcount</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1.定义一个样例类, 描述单词统计信息</span></span><br><span class="line"><span class="comment"> * 2.定义一个样例类封装单词统计结果</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">object WordCountPackage &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.定义一个样例类, 描述单词统计信息</span></span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">WordCountTask</span><span class="params">(filename: String)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">//2.定义一个样例类封装单词统计结果</span></span></span><br><span class="line"><span class="function">  <span class="keyword">case</span> class <span class="title">WordCountResult</span><span class="params">(map: Map[String, Int])</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><ul>
<li>基本流程</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>创建上下文对象</span><br><span class="line"><span class="number">2.</span>读取文件</span><br><span class="line"><span class="number">3.</span>flatMap获取到每个单词</span><br><span class="line"><span class="number">4.</span>map将RDD变成 key-value结构</span><br><span class="line"><span class="number">5.</span>reduceByKey 求和统计</span><br><span class="line"><span class="number">6.</span>打印输出</span><br><span class="line"><span class="number">7.</span>关闭上下文对象</span><br></pre></td></tr></table></figure>

<ul>
<li>本地版</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//2.加载文本文件words.txt,生成一个RDD</span></span><br><span class="line">    <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;src/main/data/words.txt&quot;</span>)</span><br><span class="line">    <span class="comment">//3.对RRD进行扁平化成单词</span></span><br><span class="line">    <span class="keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;</span><br><span class="line">      x.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//4.继续对每个单词标记为1</span></span><br><span class="line">    <span class="keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">//5继续reduceByKey进行分组统计</span></span><br><span class="line">    <span class="keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">//6.生成最后的RDD, 将结果打印到控制台</span></span><br><span class="line">    ouputRDD.foreach(println(_))</span><br><span class="line">    <span class="comment">//7.关闭上下文</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Linux版</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount_Linux</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//0.创建输入路径和输出路径</span></span><br><span class="line">    <span class="keyword">val</span> input_path = args(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> output_path = args(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//1.创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//2.加载文本文件words.txt,生成一个RDD</span></span><br><span class="line">    <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(input_path)</span><br><span class="line">    <span class="comment">//3.对RRD进行扁平化成单词</span></span><br><span class="line">    <span class="keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;</span><br><span class="line">      x.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//4.继续对每个单词标记为1</span></span><br><span class="line">    <span class="keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">//5继续reduceByKey进行分组统计</span></span><br><span class="line">    <span class="keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">//6.生成最后的RDD, 将结果上传到HDFS</span></span><br><span class="line">    ouputRDD.saveAsTextFile(output_path)</span><br><span class="line">    <span class="comment">//7.关闭上下文</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.sparksql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: Jface</span></span><br><span class="line"><span class="comment"> * @Date: 2021/9/9 23:34</span></span><br><span class="line"><span class="comment"> * @Desc: 使用 SparkSQL 读取文本文件做 Wordcount，分别使用 DSL 和 SQL 风格实现</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Wordcount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.构建上下文对象，并导包</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">4</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">//2.读取文本文件，获取 DataSet</span></span><br><span class="line">    <span class="keyword">val</span> inputDataSet: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">&quot;learnSpark/datas/wordcount.data&quot;</span>)</span><br><span class="line">    <span class="comment">//测试看看</span></span><br><span class="line">    <span class="comment">//inputDataSet.printSchema()</span></span><br><span class="line">    <span class="comment">//inputDataSet.show()</span></span><br><span class="line">    <span class="comment">//3.使用 DSL风格实现，导包</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="comment">//3.1 过滤脏数据</span></span><br><span class="line">    <span class="keyword">val</span> resultDataset01: <span class="type">Dataset</span>[<span class="type">Row</span>] = inputDataSet.where($<span class="string">&quot;value&quot;</span>.isNotNull &amp;&amp; length(trim($<span class="string">&quot;value&quot;</span>)) &gt; <span class="number">0</span>)</span><br><span class="line">      <span class="comment">//3.2 切割并把 value 行转成列</span></span><br><span class="line">      .select(explode(split(trim($<span class="string">&quot;value&quot;</span>), <span class="string">&quot;\\s+&quot;</span>)).as(<span class="string">&quot;word&quot;</span>))</span><br><span class="line">      <span class="comment">//3.3 分组并聚合</span></span><br><span class="line">      .groupBy($<span class="string">&quot;word&quot;</span>)</span><br><span class="line">      .agg(count($<span class="string">&quot;word&quot;</span>).as(<span class="string">&quot;total&quot;</span>))</span><br><span class="line">      <span class="comment">//3.4 倒序并只求前5条信息~</span></span><br><span class="line">      .orderBy($<span class="string">&quot;total&quot;</span>.desc)</span><br><span class="line">      .limit(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//resultDataset01.printSchema()</span></span><br><span class="line">    <span class="comment">//resultDataset01.show(10, truncate = false)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.使用 SQL 风格实现</span></span><br><span class="line">    <span class="comment">//4.1 注册临时视图</span></span><br><span class="line">    <span class="comment">//4.2 编写 SQL 并执行</span></span><br><span class="line">    inputDataSet.createOrReplaceTempView(<span class="string">&quot;tmp_view_lines&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> resultDataSet02: <span class="type">Dataset</span>[<span class="type">Row</span>] = spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |with tmp as</span></span><br><span class="line"><span class="string">        | (select explode(split(trim(value), &quot;\\s+&quot;)) as word</span></span><br><span class="line"><span class="string">        |from tmp_view_lines</span></span><br><span class="line"><span class="string">        |where value is not null and length(trim(value)) &gt; 0 )</span></span><br><span class="line"><span class="string">        |select t.word ,count(1) as total</span></span><br><span class="line"><span class="string">        |from tmp t</span></span><br><span class="line"><span class="string">        |group by t.word</span></span><br><span class="line"><span class="string">        |order by total desc</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">    resultDataSet02.printSchema()</span><br><span class="line">    resultDataSet02.show(<span class="number">5</span>, truncate = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.关闭上下文对象</span></span><br><span class="line">    spark.stop();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h3><ul>
<li>前期准备：安装 netcat</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在Linux上安装 netcat</span></span><br><span class="line">yum install nc -y</span><br><span class="line">yum install nmap -y</span><br><span class="line"><span class="comment">// 向 9999 端口发送数据</span></span><br><span class="line">nc -lk <span class="number">9999</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Wordcount  by UpdateStateByKey</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day06.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Desc: wordcount 案例，通过 UpdateStateByKey 实现宕机后状态恢复</span></span><br><span class="line"><span class="comment"> * 需要利用ncat 发数据， </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">S4ocketWordcountUpdateStateByKeyRecovery</span> </span>&#123;</span><br><span class="line">    <span class="comment">//设置路径</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">CKP</span> =<span class="string">&quot;src/main/data/ckp/&quot;</span>+<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>)</span><br><span class="line">    <span class="comment">//1.创建上下文对象, 指定批处理时间间隔为5秒</span></span><br><span class="line">    <span class="keyword">val</span> creatingFunc =()=&gt;</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setAppName(<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">        .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      <span class="comment">//2. 创建一个接收文本数据流的流对象</span></span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">      <span class="comment">//3.设置checkpoint位置</span></span><br><span class="line">      ssc.checkpoint(<span class="type">CKP</span>)</span><br><span class="line">      <span class="comment">//4.接收socket数据</span></span><br><span class="line">      <span class="keyword">val</span> inputDStream: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">&quot;node1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">      <span class="comment">//<span class="doctag">TODO:</span> 5.wordcount, 并做累计统计</span></span><br><span class="line">      <span class="comment">//自定义一个函数, 实现保存State状态和数据聚合</span></span><br><span class="line">      <span class="comment">//seq里面是value的数组,[1,1,], state是上次的状态, 累计值</span></span><br><span class="line">      <span class="keyword">val</span> updateFunc = (seq: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (!seq.isEmpty) &#123;</span><br><span class="line">          <span class="keyword">val</span> this_value: <span class="type">Int</span> = seq.sum</span><br><span class="line">          <span class="keyword">val</span> last_value: <span class="type">Int</span> = state.getOrElse(<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">val</span> new_state: <span class="type">Int</span> = this_value + last_value</span><br><span class="line">          <span class="type">Some</span>(new_state)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">          state</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//开始做wordcount,并打印输出</span></span><br><span class="line">      <span class="keyword">val</span> wordDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = inputDStream.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">        .map((_, <span class="number">1</span>))</span><br><span class="line">        .updateStateByKey(updateFunc)</span><br><span class="line">      wordDStream.print()</span><br><span class="line">    ssc</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getOrCreate(<span class="type">CKP</span>, creatingFunc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动流式应用</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//让应用一直处于监听状态</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment">//合理关闭流式应用</span></span><br><span class="line">    ssc.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="SparkStreaming-amp-Kafka"><a href="#SparkStreaming-amp-Kafka" class="headerlink" title="SparkStreaming &amp; Kafka"></a>SparkStreaming &amp; Kafka</h3><ul>
<li>自动提交 Offset</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day07.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Set</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Desc: Spark  Kafka自动提交offset</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">S1KafkaAutoCommit</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">//准备kafka连接参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>(</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//key的反序列化规则</span></span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//value的反序列化规则</span></span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;spark&quot;</span>, <span class="comment">//消费者组名称</span></span><br><span class="line">      <span class="comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span></span><br><span class="line">      <span class="comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span></span><br><span class="line">      <span class="comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span></span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>, <span class="comment">//offset重置位置</span></span><br><span class="line">      <span class="string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="string">&quot;1000&quot;</span>, <span class="comment">//自动提交的时间间隔</span></span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>) <span class="comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">&quot;spark_kafka&quot;</span>), kafkaParams)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//连接kafka, 拉取一批数据, 得到DSteam</span></span><br><span class="line">    <span class="keyword">val</span> resutDStream: <span class="type">DStream</span>[<span class="type">Unit</span>] = kafkaDStream.map(x =&gt; &#123;</span><br><span class="line">      println(<span class="string">s&quot;topic=<span class="subst">$&#123;x.topic()&#125;</span>,partition=<span class="subst">$&#123;x.partition()&#125;</span>,offset=<span class="subst">$&#123;x.offset()&#125;</span>,key=<span class="subst">$&#123;x.key()&#125;</span>,value=<span class="subst">$&#123;x.value()&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//打印数据</span></span><br><span class="line">    resutDStream.print()</span><br><span class="line">    <span class="comment">//启动并停留</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment">//合理化关闭</span></span><br><span class="line">    ssc.stop(stopSparkContext = <span class="literal">true</span>, stopGracefully = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>手动提交 Offset</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day07.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">CanCommitOffsets</span>, <span class="type">ConsumerStrategies</span>, <span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 topic</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">S2KafkaCommit</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">//准备kafka连接参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>(</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//key的反序列化规则</span></span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//value的反序列化规则</span></span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;spark&quot;</span>, <span class="comment">//消费者组名称</span></span><br><span class="line">      <span class="comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span></span><br><span class="line">      <span class="comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span></span><br><span class="line">      <span class="comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span></span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>, <span class="comment">//offset重置位置</span></span><br><span class="line">      <span class="string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="string">&quot;1000&quot;</span>, <span class="comment">//自动提交的时间间隔</span></span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>) <span class="comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">&quot;spark_kafka&quot;</span>), kafkaParams)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//连接kafka, 拉取一批数据, 得到DSteam</span></span><br><span class="line">    kafkaDStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">//对每个批次进行处理</span></span><br><span class="line">        <span class="comment">//提取并打印偏移量范围信息</span></span><br><span class="line">        <span class="keyword">val</span> hasOffsetRanges: <span class="type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>]</span><br><span class="line">        <span class="keyword">val</span> offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = hasOffsetRanges.offsetRanges</span><br><span class="line">        println(<span class="string">&quot;它的行偏移量是: &quot;</span>)</span><br><span class="line">        offsetRanges.foreach(println(_))</span><br><span class="line">        <span class="comment">//打印每个批次的具体信息</span></span><br><span class="line">        rdd.foreach(x =&gt; &#123;</span><br><span class="line">          println(<span class="string">s&quot;topic=<span class="subst">$&#123;x.topic()&#125;</span>,partition=<span class="subst">$&#123;x.partition()&#125;</span>,offset=<span class="subst">$&#123;x.offset()&#125;</span>,key=<span class="subst">$&#123;x.key()&#125;</span>,value=<span class="subst">$&#123;x.value()&#125;</span>&quot;</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//手动将偏移量访问信息提交到默认主题</span></span><br><span class="line">        kafkaDStream.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</span><br><span class="line">        println(<span class="string">&quot;成功提交了偏移量信息&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动并停留</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment">//合理化关闭</span></span><br><span class="line">    ssc.stop(<span class="literal">true</span>,   <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>手动提交 Offset 到 MySQL</p>
<ul>
<li>S3KafkaOffsetToMysql</li>
</ul>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day07.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.<span class="type">TopicPartition</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 MySQL</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">S3KafkaOffsetToMysql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">//准备kafka连接参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>(</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//key的反序列化规则</span></span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//value的反序列化规则</span></span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;spark&quot;</span>, <span class="comment">//消费者组名称</span></span><br><span class="line">      <span class="comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span></span><br><span class="line">      <span class="comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span></span><br><span class="line">      <span class="comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span></span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>, <span class="comment">//offset重置位置</span></span><br><span class="line">      <span class="string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="string">&quot;1000&quot;</span>, <span class="comment">//自动提交的时间间隔</span></span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>) <span class="comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//去MySQL查询上次消费的位置</span></span><br><span class="line">    <span class="keyword">val</span> offsetMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = <span class="type">OffsetUtil</span>.getOffsetMap(<span class="string">&quot;spark&quot;</span>, <span class="string">&quot;spark_kafka&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//连接kafka, 拉取一批数据, 得到DSteam</span></span><br><span class="line">    <span class="keyword">var</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">//第一次查询, MySQL没有 offset 数据</span></span><br><span class="line">    <span class="keyword">if</span> (offsetMap.isEmpty) &#123;</span><br><span class="line">      kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">&quot;spark_kafka&quot;</span>), kafkaParams) <span class="comment">//第一次就看 Kafka 发啥</span></span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//第二次查询, MySQL中有 offset 数据</span></span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">&quot;spark_kafka&quot;</span>), kafkaParams, offsetMap) <span class="comment">//第二次开始就从 MySQL 获取</span></span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对每个批次进行处理</span></span><br><span class="line">    kafkaDStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">//提取并打印偏移量范围信息</span></span><br><span class="line">        <span class="keyword">val</span> hasOffsetRanges: <span class="type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>]</span><br><span class="line">        <span class="keyword">val</span> offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = hasOffsetRanges.offsetRanges</span><br><span class="line">        println(<span class="string">&quot;它的行偏移量是: &quot;</span>)</span><br><span class="line">        offsetRanges.foreach(println(_))</span><br><span class="line">        <span class="comment">//打印每个批次的具体信息</span></span><br><span class="line">        rdd.foreach(x =&gt; &#123;</span><br><span class="line">          println(<span class="string">s&quot;topic=<span class="subst">$&#123;x.topic()&#125;</span>,partition=<span class="subst">$&#123;x.partition()&#125;</span>,offset=<span class="subst">$&#123;x.offset()&#125;</span>,key=<span class="subst">$&#123;x.key()&#125;</span>,value=<span class="subst">$&#123;x.value()&#125;</span>&quot;</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//手动将偏移量访问信息提交到MySQL</span></span><br><span class="line">        <span class="type">OffsetUtil</span>.saveOffsetRanges(<span class="string">&quot;spark&quot;</span>, offsetRanges)</span><br><span class="line">        println(<span class="string">&quot;成功提交了偏移量到MySQL&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动并停留</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    <span class="comment">//合理化关闭</span></span><br><span class="line">    ssc.stop(stopSparkContext = <span class="literal">true</span>, stopGracefully = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>OffsetUtil</li>
</ul>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day07.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.<span class="type">TopicPartition</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">OffsetRange</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Map</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">DriverManager</span>, <span class="type">ResultSet</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Desc: 定义一个单例对象, 定义 2 个方法</span></span><br><span class="line"><span class="comment"> *        方法1: 从 MySQL 读取行偏移量</span></span><br><span class="line"><span class="comment"> *        方法2: 将行偏移量保存的 MySQL</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OffsetUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 定义一个单例方法, 将偏移量保存到MySQL数据库</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param groupid     消费者组id</span></span><br><span class="line"><span class="comment">   * @param offsetRange 行偏移量对象</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveOffsetRanges</span></span>(groupid: <span class="type">String</span>, offsetRange: <span class="type">Array</span>[<span class="type">OffsetRange</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> connection = <span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,</span><br><span class="line">      <span class="string">&quot;root&quot;</span>,</span><br><span class="line">      <span class="string">&quot;root&quot;</span>)</span><br><span class="line">    <span class="comment">//replace into表示之前有就替换,没有就插入</span></span><br><span class="line">    <span class="keyword">val</span> ps = connection.prepareStatement(<span class="string">&quot;replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> (o &lt;- offsetRange) &#123;</span><br><span class="line">      ps.setString(<span class="number">1</span>, o.topic)</span><br><span class="line">      ps.setInt(<span class="number">2</span>, o.partition)</span><br><span class="line">      ps.setString(<span class="number">3</span>, groupid)</span><br><span class="line">      ps.setLong(<span class="number">4</span>, o.untilOffset)</span><br><span class="line">      ps.executeUpdate()</span><br><span class="line">    &#125;</span><br><span class="line">    ps.close()</span><br><span class="line">    connection.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 定义一个方法, 用于从 MySQL 中读取行偏移位置</span></span><br><span class="line"><span class="comment">   * @param groupid 消费者组id</span></span><br><span class="line"><span class="comment">   * @param topic   想要消费的数据主题</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getOffsetMap</span></span>(groupid: <span class="type">String</span>, topic: <span class="type">String</span>) = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.从数据库查询对应数据</span></span><br><span class="line">    <span class="keyword">val</span> connection = <span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,</span><br><span class="line">      <span class="string">&quot;root&quot;</span>,</span><br><span class="line">      <span class="string">&quot;root&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ps = connection.prepareStatement(<span class="string">&quot;select * from t_offset where groupid=?  and topic=?&quot;</span>)</span><br><span class="line">    ps.setString(<span class="number">1</span>, groupid)</span><br><span class="line">    ps.setString(<span class="number">2</span>, topic)</span><br><span class="line">    <span class="keyword">val</span> rs: <span class="type">ResultSet</span> = ps.executeQuery()</span><br><span class="line">    <span class="comment">//解析数据, 返回</span></span><br><span class="line">    <span class="keyword">var</span> offsetMap = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line">    <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">      <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(rs.getString(<span class="string">&quot;topic&quot;</span>), rs.getInt(<span class="string">&quot;partition&quot;</span>))</span><br><span class="line"></span><br><span class="line">      offsetMap.put(topicPartition, (rs.getLong(<span class="string">&quot;offset&quot;</span>)))</span><br><span class="line">    &#125;</span><br><span class="line">    rs.close()</span><br><span class="line">    rs.close()</span><br><span class="line">    connection.close()</span><br><span class="line">    offsetMap</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="StructuredStreaming"><a href="#StructuredStreaming" class="headerlink" title="StructuredStreaming"></a>StructuredStreaming</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.day07.structuredStreaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Dataset</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">StreamingQuery</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Desc: wordcount 案例 之 读取文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">S2StructuredStreamingTextFile</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="string">&quot;$&quot;</span>))</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">4</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="comment">//2.读取csv, 得到流式DataFrame, 每行就是每批次的行数据</span></span><br><span class="line">    <span class="comment">//自定义 Schema 信息</span></span><br><span class="line">    <span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;name&quot;</span>, <span class="type">StringType</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>, <span class="type">IntegerType</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;hobby&quot;</span>, <span class="type">StringType</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> inputDF: <span class="type">DataFrame</span> = spark.readStream</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">      .schema(schema)</span><br><span class="line">      .load(<span class="string">&quot;src/main/data/input/persons&quot;</span>)</span><br><span class="line">    <span class="comment">//3.进行wordcount, DSL风格</span></span><br><span class="line">    inputDF.printSchema()</span><br><span class="line">    <span class="comment">//用 DSL 风格实现</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> <span class="type">DF</span>: <span class="type">Dataset</span>[<span class="type">Row</span>] = inputDF.where(<span class="string">&quot;age&lt;25&quot;</span>)</span><br><span class="line">      .groupBy(<span class="string">&quot;hobby&quot;</span>)</span><br><span class="line">      .count()</span><br><span class="line">      .orderBy($<span class="string">&quot;count&quot;</span>.desc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用 SQL 风格实现</span></span><br><span class="line">    inputDF.createOrReplaceTempView(<span class="string">&quot;t_spark&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">DF2</span>: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select</span></span><br><span class="line"><span class="string">        |hobby,</span></span><br><span class="line"><span class="string">        |count(1) as cnt</span></span><br><span class="line"><span class="string">        |from t_spark</span></span><br><span class="line"><span class="string">        |where age&lt;25</span></span><br><span class="line"><span class="string">        |group by hobby</span></span><br><span class="line"><span class="string">        |order by cnt desc</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> query: <span class="type">StreamingQuery</span> = <span class="type">DF</span>.writeStream</span><br><span class="line">      <span class="comment">//append 默认追加 输出新的数据, 只支持简单查询, 有聚合就不能使用</span></span><br><span class="line">      <span class="comment">//complete:完整模式, 输出完整数据, 支持集合和排序</span></span><br><span class="line">      <span class="comment">//update: 更新模式, 输出有更新的数据,  支持聚合但是不支持排序</span></span><br><span class="line">      .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">      .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;rowNumber&quot;</span>, <span class="number">10</span>)</span><br><span class="line">      .option(<span class="string">&quot;truncate&quot;</span>, <span class="literal">false</span>)</span><br><span class="line">      <span class="comment">//4.启动流式查询</span></span><br><span class="line">      .start()</span><br><span class="line">    <span class="comment">//5.驻留监听</span></span><br><span class="line">    query.awaitTermination()</span><br><span class="line">    <span class="comment">//6.关闭流式查询</span></span><br><span class="line">    query.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="批处理-DataSet"><a href="#批处理-DataSet" class="headerlink" title="批处理 DataSet"></a>批处理 DataSet</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.start;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.Order;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Jface</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2021/9/5 12:37</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desc</span>: 基于Flink引擎实现批处理词频统计WordCount：过滤filter、排序sort等操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">_01WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.准备环境-env</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//2.准备数据-source</span></span><br><span class="line">        DataSource&lt;String&gt; inputDataSet = env.readTextFile(<span class="string">&quot;datas/wc.input&quot;</span>);</span><br><span class="line">        <span class="comment">//3.处理数据-transformation</span></span><br><span class="line">        <span class="comment">//<span class="doctag">TODO:</span> 3.1 过滤脏数据</span></span><br><span class="line">        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataSet.filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span> != line &amp;&amp; line.trim().length() &gt; <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">                <span class="comment">//<span class="doctag">TODO:</span> 3.2 切割</span></span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">for</span> (String s : line.trim().split(<span class="string">&quot;\\s+&quot;</span>)) &#123;</span><br><span class="line">                            out.collect(s);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                <span class="comment">//<span class="doctag">TODO:</span> 3.3 转换二元组</span></span><br><span class="line">                .map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> Tuple2.of(word, <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                <span class="comment">//<span class="doctag">TODO:</span> 3.4 分组求和</span></span><br><span class="line">                .groupBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//4.输出结果-sink</span></span><br><span class="line">        resultDataSet.printToErr();</span><br><span class="line">        <span class="comment">//<span class="doctag">TODO:</span> sort 排序，全局排序需要设置分区数 1</span></span><br><span class="line">        SortPartitionOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sortDataSet = resultDataSet.sortPartition(<span class="string">&quot;f1&quot;</span>, Order.DESCENDING)</span><br><span class="line">                .setParallelism(<span class="number">1</span>);</span><br><span class="line">        sortDataSet.printToErr();</span><br><span class="line">        <span class="comment">//只选择前3的数据</span></span><br><span class="line">        GroupReduceOperator&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; resultDataSet2 = sortDataSet.first(<span class="number">3</span>);</span><br><span class="line">        resultDataSet2.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5.触发执行-execute，没有写出不需要触发执行</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="流处理-DataStream"><a href="#流处理-DataStream" class="headerlink" title="流处理 DataStream"></a>流处理 DataStream</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.stream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desc</span>: 使用 FLink 计算引擎实现实时流式数据处理，监听端口并做 wordcount</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamWordcount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//1.准备环境-env</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//2.准备数据-source</span></span><br><span class="line">        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="string">&quot;192.168.88.161&quot;</span>, <span class="number">9999</span>);</span><br><span class="line">        <span class="comment">//3.处理数据-transformation</span></span><br><span class="line">        <span class="comment">//<span class="doctag">TODO:</span> 切割成单个单词 flatmap</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] arr = value.trim().split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line">                <span class="keyword">for</span> (String s : arr) &#123;</span><br><span class="line">                    out.collect(s);<span class="comment">//将每个单词拆分出去</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//<span class="doctag">TODO:</span> 单词--&gt; 元组形式，map</span></span><br><span class="line">        &#125;).map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value,<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//<span class="doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span></span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//4.输出结果-sink</span></span><br><span class="line">        resultDataSet.print();</span><br><span class="line">        <span class="comment">//5.触发执行-execute</span></span><br><span class="line">        env.execute(StreamWordcount.class.getSimpleName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>流处理 Flink On Yarn</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.submit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desc</span>: 使用 FLink 计算引擎实现流式数据处理，从socket 接收数据并做 wordcount</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Wordcount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//0.使用工具类，解析程序传递参数</span></span><br><span class="line">        ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">        <span class="keyword">if</span> (parameterTool.getNumberOfParameters() != <span class="number">2</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Usage: WordCount --host &lt;host&gt; --port &lt;port&gt; ............&quot;</span>);</span><br><span class="line">            System.exit(-<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        String host = parameterTool.get(<span class="string">&quot;host&quot;</span>);</span><br><span class="line">        parameterTool.getInt(<span class="string">&quot;port&quot;</span>, <span class="number">9999</span>);</span><br><span class="line">        <span class="comment">//1.准备环境-env</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//2.准备数据-source</span></span><br><span class="line">        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="string">&quot;192.168.88.161&quot;</span>, <span class="number">9999</span>);</span><br><span class="line">        <span class="comment">//3.处理数据-transformation</span></span><br><span class="line">        <span class="comment">//<span class="doctag">TODO:</span> 切割成单个单词 flatmap</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataStream = inputDataStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] arr = value.trim().split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line">                <span class="keyword">for</span> (String s : arr) &#123;</span><br><span class="line">                    out.collect(s);<span class="comment">//将每个单词拆分出去</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//<span class="doctag">TODO:</span> 单词--&gt; 元组形式，map</span></span><br><span class="line">        &#125;).map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//<span class="doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span></span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//4.输出结果-sink</span></span><br><span class="line">        resultDataStream.print();</span><br><span class="line">        <span class="comment">//5.触发执行-execute</span></span><br><span class="line">        env.execute(Wordcount.class.getSimpleName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">惊羽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://jface001.github.io/2020/11/06/WordCount案例汇总/">https://jface001.github.io/2020/11/06/WordCount案例汇总/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SAw 4.0</a> 许可协议。转载请注明来自 <a href="https://jface001.github.io">惊羽的博客</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/Scala/">Scala</a><a class="post-meta__tags" href="/tags/Flink/">Flink</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"><i class="fa fa-chevron-left">  </i><span>Redis常见面试题</span></a></div><div class="next-post pull-right"><a href="/2020/10/20/%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%B7%A5%E5%85%B7CommonsConfiguration/"><span>管理配置文件的工具：Commons Configuration</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'fd4471bb76ba37363dfe',
  clientSecret: '945ad45c695de45f4f9ee79068049e1f6ccf5f04',
  repo: 'GitalkRepo',
  owner: 'Jface001',
  admin: 'Jface001',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/post/wordcount.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2022 By 惊羽</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>