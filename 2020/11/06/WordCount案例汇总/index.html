

<!DOCTYPE html>
<html lang="zh-Hans" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="惊羽">
  <meta name="keywords" content="">
  
    <meta name="description" content="前言说明整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount 经典永不过时，使用的很多函数和方法也是常用的。 MapReduceMapTask12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.test;imp">
<meta property="og:type" content="article">
<meta property="og:title" content="WordCount案例汇总">
<meta property="og:url" content="https://jface001.github.io/2020/11/06/WordCount%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB/index.html">
<meta property="og:site_name" content="惊羽的博客">
<meta property="og:description" content="前言说明整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount 经典永不过时，使用的很多函数和方法也是常用的。 MapReduceMapTask12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.test;imp">
<meta property="og:locale">
<meta property="article:published_time" content="2020-11-06T13:10:29.000Z">
<meta property="article:modified_time" content="2022-02-09T15:14:03.816Z">
<meta property="article:author" content="惊羽">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="Scala">
<meta property="article:tag" content="Flink">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>WordCount案例汇总 - 惊羽的博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jface001.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Startseite</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archiv</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Kategorien</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Schlagwörter</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>Über</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="WordCount案例汇总"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2020-11-06 21:10" pubdate>
          November 6, 2020 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          33k wörter
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          272 minuten
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">WordCount案例汇总</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount</p>
<p>经典永不过时，使用的很多函数和方法也是常用的。</p>
<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Map规则, 用来实现把: k1, v1 -&gt; k2, v2, 需要 继承Mapper类, 重写map方法.</span><br><span class="hljs-comment"> * 各个数据解释:</span><br><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><br><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountMapTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 重写map方法,用来将K1 V2 转换成 K2 V2</span><br><span class="hljs-comment">     *</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k1</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> value   v1</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写出K2,V2</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;<br>        <span class="hljs-comment">//1.获取行偏移量,没有什么用处,我们用于测试看看的</span><br>        <span class="hljs-keyword">long</span> index = key.get();<br>        System.out.println(<span class="hljs-string">&quot;行偏移量是: &quot;</span> + index);<br>        <span class="hljs-comment">//2.获取整行数据</span><br>        String line = value.toString();<br>        <span class="hljs-comment">//3.读取并做非空校验,判断值是否相等,也判断地址值是否相等</span><br>        <span class="hljs-keyword">if</span> (line != <span class="hljs-keyword">null</span> &amp;&amp; !<span class="hljs-string">&quot;&quot;</span>.equals(line)) &#123;<br>            <span class="hljs-comment">//4.切割获取K2,V2</span><br>            String[] str = line.split(<span class="hljs-string">&quot; &quot;</span>);<br>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; str.length; i++) &#123;<br>                String s = str[i];<br>                context.write(<span class="hljs-keyword">new</span> Text(s), <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>));<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Reduce规则, 用来实现把: k2, v2的集合 -&gt; k3, v3, 需要 继承Reducer类, 重写reduce方法.</span><br><span class="hljs-comment"> * 各个数据解释:</span><br><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><br><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * shuffle阶段: 分区, 排序, 规约, 分组之后, 数据如下:</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2(的集合): 每个单词的所有次数的集合, 例如: &#123;1, 1&#125;,  &#123;1, 1, 1&#125;, &#123;1, 1&#125;</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * k3: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v3: 每个单词的总次数, 例如: 2, 3, 2</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountReduceTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;<br><br>    <span class="hljs-comment">//重写reduce方法</span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 重写reduce方法,用于把k2,v2 转换成k3,v3</span><br><span class="hljs-comment">     *</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k2</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> values  v2的集合(已经经过了分组)</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写k3,v3</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;<br>        <span class="hljs-comment">//1.获取k3,就是每个单词</span><br>        String word = key.toString();<br>        <span class="hljs-comment">//2.获取v3,就是单词出现的次数</span><br>        <span class="hljs-comment">//2.1先对v2集合求和</span><br>        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (IntWritable value : values) &#123;<br>            count += value.get();<br>        &#125;<br>        <span class="hljs-comment">//2.2写出v3</span><br>        <span class="hljs-comment">//context.write(new Text(word),new IntWritable(count));</span><br>        <span class="hljs-comment">//因为v2和v3是一样的,我们可以优化一下</span><br>        context.write(key, <span class="hljs-keyword">new</span> IntWritable(count));<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountMain简写版"><a href="#WordCountMain简写版" class="headerlink" title="WordCountMain简写版"></a>WordCountMain简写版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 这里写的是驱动类, 即: 封装MR程序的核心8步的. 它有两种写法:</span><br><span class="hljs-comment"> * 1. 官方示例版, 即: 完整版.   理解即可, 因为稍显复杂, 用的人较少.</span><br><span class="hljs-comment"> * 2. 简化版.  推荐掌握.</span><br><span class="hljs-comment"> * 这里是简化版写法</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span><br>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);<br>        <span class="hljs-comment">//2.封装MR程序核心8步</span><br>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span><br>        job.setInputFormatClass(TextInputFormat.class);<br>        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/input/wordcount.txt&quot;</span>));<br>        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span><br>        job.setMapperClass(WordCountMapTask.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.3 分区,用默认的</span><br>        <span class="hljs-comment">//2.4 排序,用默认的</span><br>        <span class="hljs-comment">//2.5 规约,用默认的</span><br>        <span class="hljs-comment">//2.6 分组,用默认的</span><br>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span><br>        job.setReducerClass(WorkCountReduceTask.class);<br>        job.setOutputValueClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span><br>        job.setOutputFormatClass(TextOutputFormat.class);<br>        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/output&quot;</span>));<br>        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span><br>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);<br>        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span><br>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountMain-jar包版"><a href="#WordCountMain-jar包版" class="headerlink" title="WordCountMain jar包版"></a>WordCountMain jar包版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 这个代码一会儿是要打包成jar包, 然后放到Yarn集群中运行的, 需要做如下的几件事儿:</span><br><span class="hljs-comment"> * 1. 在驱动类中设置 jar包的启动类.</span><br><span class="hljs-comment"> * job.setJarByClass(WordCountMain3.class);</span><br><span class="hljs-comment"> * 2. 修改数据源文件 和 目的地文件的路径, 改为: 外部传入.</span><br><span class="hljs-comment"> * TextInputFormat.addInputPath(job, new Path(args[0]));</span><br><span class="hljs-comment"> * TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="hljs-comment"> * 3. 对我们当前的工程进行打包动作, 打包成: 胖jar, 具体操作为: 取消pom.xml文件中最后一个插件的注释, 然后打包即可.</span><br><span class="hljs-comment"> * 细节: 修改jar包名字为: wordcount.jar, 方便我们操作.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 4. 在HDFS集群中创建:   /wordcount/input/ 目录</span><br><span class="hljs-comment"> * 5. 把wordcount.txt 上传到该目录下.</span><br><span class="hljs-comment"> * 6. 把之前打好的 jar包也上传到 Linux系统中.</span><br><span class="hljs-comment"> * 7. 运行该jar包即可, 记得: 传入 数据源文件路径, 目的地目录路径.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 名词解释:</span><br><span class="hljs-comment"> * 胖jar: 指的是一个jar包中还包含有其他的jar包, 这样的jar包就称之为: 胖jar.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 问题1: 为什么需要打包成 胖jar?</span><br><span class="hljs-comment"> * 答案:</span><br><span class="hljs-comment"> * 因为目前我们的工程需要依赖 Hadoop环境, 而我们已经在pom.xml文件中配置了,</span><br><span class="hljs-comment"> * 如果运行的环境中(例如: Linux系统等)没有hadoop环境, 并且我们打包时也没有把hadoop环境打包进去,</span><br><span class="hljs-comment"> * 将来运行jar包的时候就会出错.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 问题2: 当前工程一定要打包成 胖jar吗?</span><br><span class="hljs-comment"> * 答案: 不用, 因为我们的 jar包一会儿是放到 Yarn集群中运行的, 它已经自带Hadoop环境, 所以这里可以不打包 胖jar, 只打包我们自己的代码.</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain3</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span><br>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);<br>        <span class="hljs-comment">//细节1: 在驱动类中设置 jar包的启动类.</span><br>        job.setJarByClass(WorkCountMain3.class);<br><br>        <span class="hljs-comment">//2.封装MR程序核心8步</span><br>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span><br>        job.setInputFormatClass(TextInputFormat.class);<br>        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">0</span>]));<br>        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span><br>        job.setMapperClass(WordCountMapTask.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.3 分区,用默认的</span><br>        <span class="hljs-comment">//2.4 排序,用默认的</span><br>        <span class="hljs-comment">//2.5 规约,用默认的</span><br>        <span class="hljs-comment">//2.6 分组,用默认的</span><br>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span><br>        job.setReducerClass(WorkCountReduceTask.class);<br>        job.setOutputValueClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span><br>        job.setOutputFormatClass(TextOutputFormat.class);<br>        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">1</span>]));<br>        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span><br>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);<br>        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span><br>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>基本流程</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java">基本流程<br><span class="hljs-number">1.</span>传输文件路径到自定义的Actor类,并接收返回值,解析返回值得出统计结果<br><span class="hljs-number">2.</span>自定义Actor类, 接收文件路径并做解析统计单词再返回给发送者<br><br>需要分别定义<span class="hljs-number">3</span>个类<br>Main入口<br><span class="hljs-number">1.</span>用于发送文件路径,封装在自定义的单例类里面<br><span class="hljs-number">2.</span>接收返回值,并做判断是否完成传输, 如果完成就开始解析<br><span class="hljs-number">3.</span>通过apply方法解析结果,合并结果得出最后结果<br><br>自定义的Actor类<br><span class="hljs-number">1.</span>接收文件路径信息,做分析统计<br><span class="hljs-number">2.</span>把结果封装在单例类中,返回给发送者<br><br>自定义的单例类<br><span class="hljs-number">1.</span>用于封装发送信息的单例类<br><span class="hljs-number">2.</span>用于返回统计的单例类<br><br></code></pre></td></tr></table></figure>

<h3 id="MainActor"><a href="#MainActor" class="headerlink" title="MainActor"></a>MainActor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs java">`<span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<br><br><span class="hljs-keyword">import</span> java.io.File<br><span class="hljs-keyword">import</span> scala.actors.Future<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.发送文件名给WordCountActor</span><br><span class="hljs-comment"> * 2.接收WordCountActor返回结果并合并</span><br><span class="hljs-comment"> */</span><br>object MainActor &#123;<br><br>  <span class="hljs-comment">//发送文件名给WordCountActor</span><br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//1.获取文件名</span><br>    val fileDir = <span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;./data&quot;</span>)<br>    val files: Array[File] = fileDir.listFiles()<br>    <span class="hljs-comment">// 测试是成功获取文件名</span><br>    <span class="hljs-comment">// files.foreach(println(_))</span><br><br>    <span class="hljs-comment">//2.发送给wordcountactor</span><br>    val future_Array: Array[Future[Any]] = files.map(f = file =&gt; &#123;<br>      val name = file.toString<br>      <span class="hljs-comment">//每一个文件名新建对应的线程</span><br>      val actor = <span class="hljs-keyword">new</span> WordCountActor<br>      <span class="hljs-comment">//开启线程并发送给我认定任务</span><br>      actor.start()<br>      <span class="hljs-comment">//发送的消息封装在这里面并获取结果</span><br>      val future: Future[Any] = actor !! WordCountTask(name)<br>      future<br>    &#125;)<br><br>    <span class="hljs-comment">//接收WordCountActor返回结果并合并</span><br>    <span class="hljs-comment">//先判断是否全部文件都处理完毕都有结果,是再处理</span><br>    <span class="hljs-keyword">while</span> (!(future_Array.filter((x) =&gt; &#123;<br>      !x.isSet<br>    &#125;)).isEmpty) &#123;&#125;<br>    <span class="hljs-comment">//走到这里, 证明我们可以处理,使用apply获取数据</span><br>    <span class="hljs-comment">//里面的键值对就是多个文件统计结果, 我们还需要合并去重</span><br>    val wordCount: Array[Map[String, Int]] = future_Array.map((x) =&gt; &#123;<br>      val results: Any = x.apply()<br>      val result = results.asInstanceOf[WordCountResult]<br>      val map: Map[String, Int] = result.map<br>      map<br>    &#125;)<br>    <span class="hljs-comment">//wordCount.foreach(println(_))</span><br>    <span class="hljs-comment">//测试结果</span><br>    <span class="hljs-comment">// Map(e -&gt; 2, f -&gt; 1, a -&gt; 1, b -&gt; 1, c -&gt; 1)</span><br>    <span class="hljs-comment">// Map(e -&gt; 1, a -&gt; 2, b -&gt; 1, c -&gt; 2, d -&gt; 3)</span><br><br>    <span class="hljs-comment">//合并结果, 先合并成一个Array</span><br>    val flatten: Array[(String, Int)] = wordCount.flatten<br>    <span class="hljs-comment">//根据Map的key值分组</span><br>    val wordGroup: Map[String, Array[(String, Int)]] = flatten.groupBy((x) =&gt; &#123;<br>      x._1<br>    &#125;)<br>    val finalResult: Map[String, Int] = wordGroup.map((x) =&gt; &#123;<br>      val name = x._1<br>      val size = x._2.size<br>      name -&gt; size<br>    &#125;)<br><br>    finalResult.foreach(println(_))<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountActor"><a href="#WordCountActor" class="headerlink" title="WordCountActor"></a>WordCountActor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<br><br><span class="hljs-keyword">import</span> scala.actors.Actor<br><span class="hljs-keyword">import</span> scala.io.Source<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.接收MainActor的文件名称并进行单词统计</span><br><span class="hljs-comment"> * 2.将单词统计结果返回给MainActor</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountActor</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Actor</span> </span>&#123;<br>  <span class="hljs-function">override def <span class="hljs-title">act</span><span class="hljs-params">()</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//接收消息</span><br>    loop &#123;<br>      react &#123;<br>        <span class="hljs-function"><span class="hljs-keyword">case</span> <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename)</span> </span>=&gt;<br>          println(<span class="hljs-string">&quot;收到了文件名: &quot;</span> + filename)<br>          <span class="hljs-comment">//解析消息, 通过Source解析消息, 定义文件来源再转化成列表</span><br>          <span class="hljs-comment">//一个元素就是一个一行数据</span><br>          val words: List[String] = Source.fromFile(filename).getLines().toList<br>          <span class="hljs-comment">//切割获取每一条数据并合并成一个list集合</span><br>          val word_List: List[String] = words.flatMap((x) =&gt; &#123;<br>            x.split(<span class="hljs-string">&quot; &quot;</span>)<br>          &#125;)<br>          <span class="hljs-comment">//按照单词进行分组, 然后聚合统计</span><br>          val word_Tuples: List[(String, Int)] = word_List.map((x) =&gt; &#123;<br>            (x, <span class="hljs-number">1</span>)<br>          &#125;)<br>          val word_Map: Map[String, List[(String, Int)]] = word_Tuples.groupBy((x) =&gt; &#123;<br>            x._1<br>          &#125;)<br>          val wordCountMap: Map[String, Int] = word_Map.map((x) =&gt; &#123;<br>            val name: String = x._1<br>            val size: Int = x._2.size<br>            name -&gt; size<br>          &#125;)<br><br>          <span class="hljs-comment">//把统计结果反馈给Mainactor,装进WordCount</span><br>          sender ! WordCountResult(wordCountMap)<br>      &#125;<br><br>    &#125;<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountPackage"><a href="#WordCountPackage" class="headerlink" title="WordCountPackage"></a>WordCountPackage</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.定义一个样例类, 描述单词统计信息</span><br><span class="hljs-comment"> * 2.定义一个样例类封装单词统计结果</span><br><span class="hljs-comment"> */</span><br>object WordCountPackage &#123;<br><br>  <span class="hljs-comment">//1.定义一个样例类, 描述单词统计信息</span><br>  <span class="hljs-function"><span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename: String)</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">  <span class="hljs-comment">//2.定义一个样例类封装单词统计结果</span></span><br><span class="hljs-function">  <span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountResult</span><span class="hljs-params">(map: Map[String, Int])</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">&#125;</span><br></code></pre></td></tr></table></figure>

<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><ul>
<li>基本流程</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-number">1.</span>创建上下文对象<br><span class="hljs-number">2.</span>读取文件<br><span class="hljs-number">3.</span>flatMap获取到每个单词<br><span class="hljs-number">4.</span>map将RDD变成 key-value结构<br><span class="hljs-number">5.</span>reduceByKey 求和统计<br><span class="hljs-number">6.</span>打印输出<br><span class="hljs-number">7.</span>关闭上下文对象<br></code></pre></td></tr></table></figure>

<ul>
<li>本地版</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day01<br><br><span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><br><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WordCount</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>).setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    <span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)<br>    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span><br>    <span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.textFile(<span class="hljs-string">&quot;src/main/data/words.txt&quot;</span>)<br>    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span><br>    <span class="hljs-keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//4.继续对每个单词标记为1</span><br>    <span class="hljs-keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))<br>    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span><br>    <span class="hljs-keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)<br>    <span class="hljs-comment">//6.生成最后的RDD, 将结果打印到控制台</span><br>    ouputRDD.foreach(println(_))<br>    <span class="hljs-comment">//7.关闭上下文</span><br>    sc.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li>Linux版</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day01<br><br><span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><br><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WordCount_Linux</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//0.创建输入路径和输出路径</span><br>    <span class="hljs-keyword">val</span> input_path = args(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">val</span> output_path = args(<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>)<br>    <span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)<br>    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span><br>    <span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.textFile(input_path)<br>    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span><br>    <span class="hljs-keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//4.继续对每个单词标记为1</span><br>    <span class="hljs-keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))<br>    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span><br>    <span class="hljs-keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)<br>    <span class="hljs-comment">//6.生成最后的RDD, 将结果上传到HDFS</span><br>    ouputRDD.saveAsTextFile(output_path)<br>    <span class="hljs-comment">//7.关闭上下文</span><br>    sc.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.sparksql<br><br><span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">Row</span>, <span class="hljs-type">SparkSession</span>&#125;<br><br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * @Author: Jface</span><br><span class="hljs-comment"> * @Date: 2021/9/9 23:34</span><br><span class="hljs-comment"> * @Desc: 使用 SparkSQL 读取文本文件做 Wordcount，分别使用 DSL 和 SQL 风格实现</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Wordcount</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//1.构建上下文对象，并导包</span><br>    <span class="hljs-keyword">val</span> spark: <span class="hljs-type">SparkSession</span> = <span class="hljs-type">SparkSession</span>.builder()<br>      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)<br>      .getOrCreate()<br><br>    <span class="hljs-keyword">import</span> spark.implicits._<br>    <span class="hljs-comment">//2.读取文本文件，获取 DataSet</span><br>    <span class="hljs-keyword">val</span> inputDataSet: <span class="hljs-type">Dataset</span>[<span class="hljs-type">String</span>] = spark.read.textFile(<span class="hljs-string">&quot;learnSpark/datas/wordcount.data&quot;</span>)<br>    <span class="hljs-comment">//测试看看</span><br>    <span class="hljs-comment">//inputDataSet.printSchema()</span><br>    <span class="hljs-comment">//inputDataSet.show()</span><br>    <span class="hljs-comment">//3.使用 DSL风格实现，导包</span><br>    <span class="hljs-keyword">import</span> org.apache.spark.sql.functions._<br>    <span class="hljs-comment">//3.1 过滤脏数据</span><br>    <span class="hljs-keyword">val</span> resultDataset01: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = inputDataSet.where($<span class="hljs-string">&quot;value&quot;</span>.isNotNull &amp;&amp; length(trim($<span class="hljs-string">&quot;value&quot;</span>)) &gt; <span class="hljs-number">0</span>)<br>      <span class="hljs-comment">//3.2 切割并把 value 行转成列</span><br>      .select(explode(split(trim($<span class="hljs-string">&quot;value&quot;</span>), <span class="hljs-string">&quot;\\s+&quot;</span>)).as(<span class="hljs-string">&quot;word&quot;</span>))<br>      <span class="hljs-comment">//3.3 分组并聚合</span><br>      .groupBy($<span class="hljs-string">&quot;word&quot;</span>)<br>      .agg(count($<span class="hljs-string">&quot;word&quot;</span>).as(<span class="hljs-string">&quot;total&quot;</span>))<br>      <span class="hljs-comment">//3.4 倒序并只求前5条信息~</span><br>      .orderBy($<span class="hljs-string">&quot;total&quot;</span>.desc)<br>      .limit(<span class="hljs-number">5</span>)<br><br>    <span class="hljs-comment">//resultDataset01.printSchema()</span><br>    <span class="hljs-comment">//resultDataset01.show(10, truncate = false)</span><br><br>    <span class="hljs-comment">//4.使用 SQL 风格实现</span><br>    <span class="hljs-comment">//4.1 注册临时视图</span><br>    <span class="hljs-comment">//4.2 编写 SQL 并执行</span><br>    inputDataSet.createOrReplaceTempView(<span class="hljs-string">&quot;tmp_view_lines&quot;</span>)<br>    <span class="hljs-keyword">val</span> resultDataSet02: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = spark.sql(<br>      <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        |with tmp as</span><br><span class="hljs-string">        | (select explode(split(trim(value), &quot;\\s+&quot;)) as word</span><br><span class="hljs-string">        |from tmp_view_lines</span><br><span class="hljs-string">        |where value is not null and length(trim(value)) &gt; 0 )</span><br><span class="hljs-string">        |select t.word ,count(1) as total</span><br><span class="hljs-string">        |from tmp t</span><br><span class="hljs-string">        |group by t.word</span><br><span class="hljs-string">        |order by total desc</span><br><span class="hljs-string">        |&quot;&quot;&quot;</span>.stripMargin)<br>    resultDataSet02.printSchema()<br>    resultDataSet02.show(<span class="hljs-number">5</span>, truncate = <span class="hljs-literal">false</span>)<br><br>    <span class="hljs-comment">//5.关闭上下文对象</span><br>    spark.stop();<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h3><ul>
<li>前期准备：安装 netcat</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 在Linux上安装 netcat</span><br>yum install nc -y<br>yum install nmap -y<br><span class="hljs-comment">// 向 9999 端口发送数据</span><br>nc -lk <span class="hljs-number">9999</span><br></code></pre></td></tr></table></figure>

<ul>
<li>Wordcount  by UpdateStateByKey</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day06.streaming<br><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">ReceiverInputDStream</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * @Desc: wordcount 案例，通过 UpdateStateByKey 实现宕机后状态恢复</span><br><span class="hljs-comment"> * 需要利用ncat 发数据， </span><br><span class="hljs-comment"> */</span><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S4ocketWordcountUpdateStateByKeyRecovery</span> </span>&#123;<br>    <span class="hljs-comment">//设置路径</span><br>    <span class="hljs-keyword">val</span> <span class="hljs-type">CKP</span> =<span class="hljs-string">&quot;src/main/data/ckp/&quot;</span>+<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>)<br>    <span class="hljs-comment">//1.创建上下文对象, 指定批处理时间间隔为5秒</span><br>    <span class="hljs-keyword">val</span> creatingFunc =()=&gt;<br>    &#123;<br>      <span class="hljs-keyword">val</span> conf: <span class="hljs-type">SparkConf</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()<br>        .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>        .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)<br>      <span class="hljs-comment">//2. 创建一个接收文本数据流的流对象</span><br>      <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br><br>      <span class="hljs-comment">//3.设置checkpoint位置</span><br>      ssc.checkpoint(<span class="hljs-type">CKP</span>)<br>      <span class="hljs-comment">//4.接收socket数据</span><br>      <span class="hljs-keyword">val</span> inputDStream: <span class="hljs-type">ReceiverInputDStream</span>[<span class="hljs-type">String</span>] = ssc.socketTextStream(<span class="hljs-string">&quot;node1&quot;</span>, <span class="hljs-number">9999</span>)<br>      <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 5.wordcount, 并做累计统计</span><br>      <span class="hljs-comment">//自定义一个函数, 实现保存State状态和数据聚合</span><br>      <span class="hljs-comment">//seq里面是value的数组,[1,1,], state是上次的状态, 累计值</span><br>      <span class="hljs-keyword">val</span> updateFunc = (seq: <span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>], state: <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>]) =&gt; &#123;<br>        <span class="hljs-keyword">if</span> (!seq.isEmpty) &#123;<br>          <span class="hljs-keyword">val</span> this_value: <span class="hljs-type">Int</span> = seq.sum<br>          <span class="hljs-keyword">val</span> last_value: <span class="hljs-type">Int</span> = state.getOrElse(<span class="hljs-number">0</span>)<br>          <span class="hljs-keyword">val</span> new_state: <span class="hljs-type">Int</span> = this_value + last_value<br>          <span class="hljs-type">Some</span>(new_state)<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>          state<br>        &#125;<br>      &#125;<br>      <span class="hljs-comment">//开始做wordcount,并打印输出</span><br>      <span class="hljs-keyword">val</span> wordDStream: <span class="hljs-type">DStream</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] = inputDStream.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>        .map((_, <span class="hljs-number">1</span>))<br>        .updateStateByKey(updateFunc)<br>      wordDStream.print()<br>    ssc<br>    &#125;<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> ssc: <span class="hljs-type">StreamingContext</span> = <span class="hljs-type">StreamingContext</span>.getOrCreate(<span class="hljs-type">CKP</span>, creatingFunc)<br><br>    <span class="hljs-comment">//启动流式应用</span><br>    ssc.start()<br>    <span class="hljs-comment">//让应用一直处于监听状态</span><br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理关闭流式应用</span><br>    ssc.stop(<span class="hljs-literal">true</span>, <span class="hljs-literal">true</span>)<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="SparkStreaming-amp-Kafka"><a href="#SparkStreaming-amp-Kafka" class="headerlink" title="SparkStreaming &amp; Kafka"></a>SparkStreaming &amp; Kafka</h3><ul>
<li>自动提交 Offset</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">InputDStream</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="hljs-type">ConsumerStrategies</span>, <span class="hljs-type">KafkaUtils</span>, <span class="hljs-type">LocationStrategies</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<br><span class="hljs-keyword">import</span> scala.collection.mutable.<span class="hljs-type">Set</span><br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * @Desc: Spark  Kafka自动提交offset</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S1KafkaAutoCommit</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)<br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">true</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br><br>    <span class="hljs-keyword">val</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<br>      ssc,<br>      <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,<br>      <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)<br>    )<br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    <span class="hljs-keyword">val</span> resutDStream: <span class="hljs-type">DStream</span>[<span class="hljs-type">Unit</span>] = kafkaDStream.map(x =&gt; &#123;<br>      println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//打印数据</span><br>    resutDStream.print()<br>    <span class="hljs-comment">//启动并停留</span><br>    ssc.start()<br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理化关闭</span><br>    ssc.stop(stopSparkContext = <span class="hljs-literal">true</span>, stopGracefully = <span class="hljs-literal">true</span>)<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li>手动提交 Offset</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">InputDStream</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="hljs-type">CanCommitOffsets</span>, <span class="hljs-type">ConsumerStrategies</span>, <span class="hljs-type">HasOffsetRanges</span>, <span class="hljs-type">KafkaUtils</span>, <span class="hljs-type">LocationStrategies</span>, <span class="hljs-type">OffsetRange</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 topic</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S2KafkaCommit</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)<br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">false</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br><br>    <span class="hljs-keyword">val</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<br>      ssc,<br>      <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,<br>      <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)<br>    )<br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    kafkaDStream.foreachRDD(rdd =&gt; &#123;<br>      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;<br>        <span class="hljs-comment">//对每个批次进行处理</span><br>        <span class="hljs-comment">//提取并打印偏移量范围信息</span><br>        <span class="hljs-keyword">val</span> hasOffsetRanges: <span class="hljs-type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>]<br>        <span class="hljs-keyword">val</span> offsetRanges: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>] = hasOffsetRanges.offsetRanges<br>        println(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)<br>        offsetRanges.foreach(println(_))<br>        <span class="hljs-comment">//打印每个批次的具体信息</span><br>        rdd.foreach(x =&gt; &#123;<br>          println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)<br>        &#125;)<br>        <span class="hljs-comment">//手动将偏移量访问信息提交到默认主题</span><br>        kafkaDStream.asInstanceOf[<span class="hljs-type">CanCommitOffsets</span>].commitAsync(offsetRanges)<br>        println(<span class="hljs-string">&quot;成功提交了偏移量信息&quot;</span>)<br>      &#125;<br>    &#125;)<br><br>    <span class="hljs-comment">//启动并停留</span><br>    ssc.start()<br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理化关闭</span><br>    ssc.stop(<span class="hljs-literal">true</span>,   <span class="hljs-literal">true</span>)<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li><p>手动提交 Offset 到 MySQL</p>
<ul>
<li>S3KafkaOffsetToMysql</li>
</ul>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><br><span class="hljs-keyword">import</span> org.apache.kafka.common.<span class="hljs-type">TopicPartition</span><br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.<span class="hljs-type">InputDStream</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010._<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<br><br><span class="hljs-keyword">import</span> scala.collection.mutable<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 MySQL</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S3KafkaOffsetToMysql</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)<br>    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">false</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br>    <span class="hljs-comment">//去MySQL查询上次消费的位置</span><br>    <span class="hljs-keyword">val</span> offsetMap: mutable.<span class="hljs-type">Map</span>[<span class="hljs-type">TopicPartition</span>, <span class="hljs-type">Long</span>] = <span class="hljs-type">OffsetUtil</span>.getOffsetMap(<span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-string">&quot;spark_kafka&quot;</span>)<br><br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    <span class="hljs-keyword">var</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-literal">null</span><br>    <span class="hljs-comment">//第一次查询, MySQL没有 offset 数据</span><br>    <span class="hljs-keyword">if</span> (offsetMap.isEmpty) &#123;<br>      kafkaDStream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<br>        ssc,<br>        <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,<br>        <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams) <span class="hljs-comment">//第一次就看 Kafka 发啥</span><br>      )<br>    &#125;<br>    <span class="hljs-comment">//第二次查询, MySQL中有 offset 数据</span><br>    <span class="hljs-keyword">else</span> &#123;<br>      kafkaDStream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<br>        ssc,<br>        <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,<br>        <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams, offsetMap) <span class="hljs-comment">//第二次开始就从 MySQL 获取</span><br>      )<br>    &#125;<br><br>    <span class="hljs-comment">//对每个批次进行处理</span><br>    kafkaDStream.foreachRDD(rdd =&gt; &#123;<br>      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;<br>        <span class="hljs-comment">//提取并打印偏移量范围信息</span><br>        <span class="hljs-keyword">val</span> hasOffsetRanges: <span class="hljs-type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>]<br>        <span class="hljs-keyword">val</span> offsetRanges: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>] = hasOffsetRanges.offsetRanges<br>        println(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)<br>        offsetRanges.foreach(println(_))<br>        <span class="hljs-comment">//打印每个批次的具体信息</span><br>        rdd.foreach(x =&gt; &#123;<br>          println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)<br>        &#125;)<br>        <span class="hljs-comment">//手动将偏移量访问信息提交到MySQL</span><br>        <span class="hljs-type">OffsetUtil</span>.saveOffsetRanges(<span class="hljs-string">&quot;spark&quot;</span>, offsetRanges)<br>        println(<span class="hljs-string">&quot;成功提交了偏移量到MySQL&quot;</span>)<br>      &#125;<br>    &#125;)<br><br>    <span class="hljs-comment">//启动并停留</span><br>    ssc.start()<br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理化关闭</span><br>    ssc.stop(stopSparkContext = <span class="hljs-literal">true</span>, stopGracefully = <span class="hljs-literal">true</span>)<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li>OffsetUtil</li>
</ul>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.common.<span class="hljs-type">TopicPartition</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.<span class="hljs-type">OffsetRange</span><br><span class="hljs-keyword">import</span> scala.collection.mutable.<span class="hljs-type">Map</span><br><br><span class="hljs-keyword">import</span> java.sql.&#123;<span class="hljs-type">DriverManager</span>, <span class="hljs-type">ResultSet</span>&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * @Desc: 定义一个单例对象, 定义 2 个方法</span><br><span class="hljs-comment"> *        方法1: 从 MySQL 读取行偏移量</span><br><span class="hljs-comment"> *        方法2: 将行偏移量保存的 MySQL</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">OffsetUtil</span> </span>&#123;<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * 定义一个单例方法, 将偏移量保存到MySQL数据库</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param groupid     消费者组id</span><br><span class="hljs-comment">   * @param offsetRange 行偏移量对象</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">saveOffsetRanges</span></span>(groupid: <span class="hljs-type">String</span>, offsetRange: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>]) = &#123;<br>    <span class="hljs-keyword">val</span> connection = <span class="hljs-type">DriverManager</span>.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>)<br>    <span class="hljs-comment">//replace into表示之前有就替换,没有就插入</span><br>    <span class="hljs-keyword">val</span> ps = connection.prepareStatement(<span class="hljs-string">&quot;replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)&quot;</span>)<br>    <span class="hljs-keyword">for</span> (o &lt;- offsetRange) &#123;<br>      ps.setString(<span class="hljs-number">1</span>, o.topic)<br>      ps.setInt(<span class="hljs-number">2</span>, o.partition)<br>      ps.setString(<span class="hljs-number">3</span>, groupid)<br>      ps.setLong(<span class="hljs-number">4</span>, o.untilOffset)<br>      ps.executeUpdate()<br>    &#125;<br>    ps.close()<br>    connection.close()<br>  &#125;<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * 定义一个方法, 用于从 MySQL 中读取行偏移位置</span><br><span class="hljs-comment">   * @param groupid 消费者组id</span><br><span class="hljs-comment">   * @param topic   想要消费的数据主题</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getOffsetMap</span></span>(groupid: <span class="hljs-type">String</span>, topic: <span class="hljs-type">String</span>) = &#123;<br><br>    <span class="hljs-comment">//1.从数据库查询对应数据</span><br>    <span class="hljs-keyword">val</span> connection = <span class="hljs-type">DriverManager</span>.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>)<br><br>    <span class="hljs-keyword">val</span> ps = connection.prepareStatement(<span class="hljs-string">&quot;select * from t_offset where groupid=?  and topic=?&quot;</span>)<br>    ps.setString(<span class="hljs-number">1</span>, groupid)<br>    ps.setString(<span class="hljs-number">2</span>, topic)<br>    <span class="hljs-keyword">val</span> rs: <span class="hljs-type">ResultSet</span> = ps.executeQuery()<br>    <span class="hljs-comment">//解析数据, 返回</span><br>    <span class="hljs-keyword">var</span> offsetMap = <span class="hljs-type">Map</span>[<span class="hljs-type">TopicPartition</span>, <span class="hljs-type">Long</span>]()<br>    <span class="hljs-keyword">while</span> (rs.next()) &#123;<br>      <span class="hljs-keyword">val</span> topicPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">TopicPartition</span>(rs.getString(<span class="hljs-string">&quot;topic&quot;</span>), rs.getInt(<span class="hljs-string">&quot;partition&quot;</span>))<br><br>      offsetMap.put(topicPartition, (rs.getLong(<span class="hljs-string">&quot;offset&quot;</span>)))<br>    &#125;<br>    rs.close()<br>    rs.close()<br>    connection.close()<br>    offsetMap<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="StructuredStreaming"><a href="#StructuredStreaming" class="headerlink" title="StructuredStreaming"></a>StructuredStreaming</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.structuredStreaming<br><br><span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">Row</span>, <span class="hljs-type">SparkSession</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.sql.streaming.<span class="hljs-type">StreamingQuery</span><br><span class="hljs-keyword">import</span> org.apache.spark.sql.types.&#123;<span class="hljs-type">IntegerType</span>, <span class="hljs-type">StringType</span>, <span class="hljs-type">StructField</span>, <span class="hljs-type">StructType</span>&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * @Desc: wordcount 案例 之 读取文件</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S2StructuredStreamingTextFile</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    <span class="hljs-keyword">val</span> spark: <span class="hljs-type">SparkSession</span> = <span class="hljs-type">SparkSession</span>.builder()<br>      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)<br>      .getOrCreate()<br>    <span class="hljs-comment">//2.读取csv, 得到流式DataFrame, 每行就是每批次的行数据</span><br>    <span class="hljs-comment">//自定义 Schema 信息</span><br>    <span class="hljs-keyword">val</span> schema = <span class="hljs-keyword">new</span> <span class="hljs-type">StructType</span>(<span class="hljs-type">Array</span>(<br>      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-type">StringType</span>),<br>      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;age&quot;</span>, <span class="hljs-type">IntegerType</span>),<br>      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;hobby&quot;</span>, <span class="hljs-type">StringType</span>))<br>    )<br>    <span class="hljs-keyword">val</span> inputDF: <span class="hljs-type">DataFrame</span> = spark.readStream<br>      .format(<span class="hljs-string">&quot;csv&quot;</span>)<br>      .option(<span class="hljs-string">&quot;sep&quot;</span>, <span class="hljs-string">&quot;;&quot;</span>)<br>      .schema(schema)<br>      .load(<span class="hljs-string">&quot;src/main/data/input/persons&quot;</span>)<br>    <span class="hljs-comment">//3.进行wordcount, DSL风格</span><br>    inputDF.printSchema()<br>    <span class="hljs-comment">//用 DSL 风格实现</span><br>    <span class="hljs-keyword">import</span> spark.implicits._<br>    <span class="hljs-keyword">val</span> <span class="hljs-type">DF</span>: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = inputDF.where(<span class="hljs-string">&quot;age&lt;25&quot;</span>)<br>      .groupBy(<span class="hljs-string">&quot;hobby&quot;</span>)<br>      .count()<br>      .orderBy($<span class="hljs-string">&quot;count&quot;</span>.desc)<br><br>    <span class="hljs-comment">// 用 SQL 风格实现</span><br>    inputDF.createOrReplaceTempView(<span class="hljs-string">&quot;t_spark&quot;</span>)<br>    <span class="hljs-keyword">val</span> <span class="hljs-type">DF2</span>: <span class="hljs-type">DataFrame</span> = spark.sql(<br>      <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        |select</span><br><span class="hljs-string">        |hobby,</span><br><span class="hljs-string">        |count(1) as cnt</span><br><span class="hljs-string">        |from t_spark</span><br><span class="hljs-string">        |where age&lt;25</span><br><span class="hljs-string">        |group by hobby</span><br><span class="hljs-string">        |order by cnt desc</span><br><span class="hljs-string">        |&quot;&quot;&quot;</span>.stripMargin)<br><br>    <span class="hljs-keyword">val</span> query: <span class="hljs-type">StreamingQuery</span> = <span class="hljs-type">DF</span>.writeStream<br>      <span class="hljs-comment">//append 默认追加 输出新的数据, 只支持简单查询, 有聚合就不能使用</span><br>      <span class="hljs-comment">//complete:完整模式, 输出完整数据, 支持集合和排序</span><br>      <span class="hljs-comment">//update: 更新模式, 输出有更新的数据,  支持聚合但是不支持排序</span><br>      .outputMode(<span class="hljs-string">&quot;complete&quot;</span>)<br>      .format(<span class="hljs-string">&quot;console&quot;</span>)<br>      .option(<span class="hljs-string">&quot;rowNumber&quot;</span>, <span class="hljs-number">10</span>)<br>      .option(<span class="hljs-string">&quot;truncate&quot;</span>, <span class="hljs-literal">false</span>)<br>      <span class="hljs-comment">//4.启动流式查询</span><br>      .start()<br>    <span class="hljs-comment">//5.驻留监听</span><br>    query.awaitTermination()<br>    <span class="hljs-comment">//6.关闭流式查询</span><br>    query.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="批处理-DataSet"><a href="#批处理-DataSet" class="headerlink" title="批处理 DataSet"></a>批处理 DataSet</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.flink.start;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.operators.Order;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.operators.*;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span>: Jface</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Date</span>: 2021/9/5 12:37</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 基于Flink引擎实现批处理词频统计WordCount：过滤filter、排序sort等操作</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_01WordCount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.准备环境-env</span><br>        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();<br>        <span class="hljs-comment">//2.准备数据-source</span><br>        DataSource&lt;String&gt; inputDataSet = env.readTextFile(<span class="hljs-string">&quot;datas/wc.input&quot;</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.1 过滤脏数据</span><br>        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataSet.filter(<span class="hljs-keyword">new</span> FilterFunction&lt;String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">filter</span><span class="hljs-params">(String line)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span> != line &amp;&amp; line.trim().length() &gt; <span class="hljs-number">0</span>;<br>            &#125;<br>        &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.2 切割</span><br>                .flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>                    <span class="hljs-meta">@Override</span><br>                    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String line, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                        <span class="hljs-keyword">for</span> (String s : line.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>)) &#123;<br>                            out.collect(s);<br>                        &#125;<br>                    &#125;<br>                &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.3 转换二元组</span><br>                .map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>                    <span class="hljs-meta">@Override</span><br>                    <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String word)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                        <span class="hljs-keyword">return</span> Tuple2.of(word, <span class="hljs-number">1</span>);<br>                    &#125;<br>                &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.4 分组求和</span><br>                .groupBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataSet.printToErr();<br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> sort 排序，全局排序需要设置分区数 1</span><br>        SortPartitionOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sortDataSet = resultDataSet.sortPartition(<span class="hljs-string">&quot;f1&quot;</span>, Order.DESCENDING)<br>                .setParallelism(<span class="hljs-number">1</span>);<br>        sortDataSet.printToErr();<br>        <span class="hljs-comment">//只选择前3的数据</span><br>        GroupReduceOperator&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; resultDataSet2 = sortDataSet.first(<span class="hljs-number">3</span>);<br>        resultDataSet2.print();<br><br>        <span class="hljs-comment">//5.触发执行-execute，没有写出不需要触发执行</span><br><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="流处理-DataStream"><a href="#流处理-DataStream" class="headerlink" title="流处理 DataStream"></a>流处理 DataStream</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.stream;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现实时流式数据处理，监听端口并做 wordcount</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StreamWordcount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>         <span class="hljs-comment">//1.准备环境-env</span><br>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();<br>         <span class="hljs-comment">//2.准备数据-source</span><br>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span><br>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);<br>                <span class="hljs-keyword">for</span> (String s : arr) &#123;<br>                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span><br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span><br>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> Tuple2.of(value,<span class="hljs-number">1</span>);<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span><br>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataSet.print();<br>        <span class="hljs-comment">//5.触发执行-execute</span><br>        env.execute(StreamWordcount.class.getSimpleName());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>流处理 Flink On Yarn</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.submit;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现流式数据处理，从socket 接收数据并做 wordcount</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Wordcount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//0.使用工具类，解析程序传递参数</span><br>        ParameterTool parameterTool = ParameterTool.fromArgs(args);<br>        <span class="hljs-keyword">if</span> (parameterTool.getNumberOfParameters() != <span class="hljs-number">2</span>) &#123;<br>            System.out.println(<span class="hljs-string">&quot;Usage: WordCount --host &lt;host&gt; --port &lt;port&gt; ............&quot;</span>);<br>            System.exit(-<span class="hljs-number">1</span>);<br>        &#125;<br>        String host = parameterTool.get(<span class="hljs-string">&quot;host&quot;</span>);<br>        parameterTool.getInt(<span class="hljs-string">&quot;port&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//1.准备环境-env</span><br>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();<br>        <span class="hljs-comment">//2.准备数据-source</span><br>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span><br>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataStream = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);<br>                <span class="hljs-keyword">for</span> (String s : arr) &#123;<br>                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span><br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span><br>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> Tuple2.of(value, <span class="hljs-number">1</span>);<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span><br>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataStream.print();<br>        <span class="hljs-comment">//5.触发执行-execute</span><br>        env.execute(Wordcount.class.getSimpleName());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>


                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C/" class="category-chain-item">日常工作</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Spark/">#Spark</a>
      
        <a href="/tags/Hadoop/">#Hadoop</a>
      
        <a href="/tags/Scala/">#Scala</a>
      
        <a href="/tags/Flink/">#Flink</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>WordCount案例汇总</div>
      <div>https://jface001.github.io/2020/11/06/WordCount案例汇总/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Beitragsautor</div>
          <div>惊羽</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Veröffentlicht am</div>
          <div>November 6, 2020</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Urheberrechtshinweis</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/" title="Redis常见面试题">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Redis常见面试题</span>
                        <span class="visible-mobile">Vorheriger</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/10/20/%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%B7%A5%E5%85%B7CommonsConfiguration/" title="管理配置文件的工具：Commons Configuration">
                        <span class="hidden-mobile">管理配置文件的工具：Commons Configuration</span>
                        <span class="visible-mobile">Nächster</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Inhaltsverzeichnis</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Suchen</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Stichwort</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog funktioniert am besten mit aktiviertem JavaScript</div>
  </noscript>
</body>
</html>
