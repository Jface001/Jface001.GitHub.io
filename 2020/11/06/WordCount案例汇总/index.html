

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.svg">
  <link rel="icon" href="/img/icon.svg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="惊羽">
  <meta name="keywords" content="">
  
  <title>WordCount案例汇总 - </title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>惊羽的博客</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/post/wordcount.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="WordCount案例汇总">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-11-06 21:10" pubdate>
        2020年11月6日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7.2k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      110
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">WordCount案例汇总</h1>
            
            <div class="markdown-body">
              <h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount</p>
<p>经典永不过时，使用的很多函数和方法也是常用的。</p>
<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Map规则, 用来实现把: k1, v1 -&gt; k2, v2, 需要 继承Mapper类, 重写map方法.</span><br><span class="hljs-comment"> * 各个数据解释:</span><br><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><br><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountMapTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 重写map方法,用来将K1 V2 转换成 K2 V2</span><br><span class="hljs-comment">     *</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k1</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> value   v1</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写出K2,V2</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;<br>        <span class="hljs-comment">//1.获取行偏移量,没有什么用处,我们用于测试看看的</span><br>        <span class="hljs-keyword">long</span> index = key.get();<br>        System.out.println(<span class="hljs-string">&quot;行偏移量是: &quot;</span> + index);<br>        <span class="hljs-comment">//2.获取整行数据</span><br>        String line = value.toString();<br>        <span class="hljs-comment">//3.读取并做非空校验,判断值是否相等,也判断地址值是否相等</span><br>        <span class="hljs-keyword">if</span> (line != <span class="hljs-keyword">null</span> &amp;&amp; !<span class="hljs-string">&quot;&quot;</span>.equals(line)) &#123;<br>            <span class="hljs-comment">//4.切割获取K2,V2</span><br>            String[] str = line.split(<span class="hljs-string">&quot; &quot;</span>);<br>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; str.length; i++) &#123;<br>                String s = str[i];<br>                context.write(<span class="hljs-keyword">new</span> Text(s), <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>));<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Reduce规则, 用来实现把: k2, v2的集合 -&gt; k3, v3, 需要 继承Reducer类, 重写reduce方法.</span><br><span class="hljs-comment"> * 各个数据解释:</span><br><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><br><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * shuffle阶段: 分区, 排序, 规约, 分组之后, 数据如下:</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2(的集合): 每个单词的所有次数的集合, 例如: &#123;1, 1&#125;,  &#123;1, 1, 1&#125;, &#123;1, 1&#125;</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * k3: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v3: 每个单词的总次数, 例如: 2, 3, 2</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountReduceTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;<br><br>    <span class="hljs-comment">//重写reduce方法</span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 重写reduce方法,用于把k2,v2 转换成k3,v3</span><br><span class="hljs-comment">     *</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k2</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> values  v2的集合(已经经过了分组)</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写k3,v3</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;<br>        <span class="hljs-comment">//1.获取k3,就是每个单词</span><br>        String word = key.toString();<br>        <span class="hljs-comment">//2.获取v3,就是单词出现的次数</span><br>        <span class="hljs-comment">//2.1先对v2集合求和</span><br>        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (IntWritable value : values) &#123;<br>            count += value.get();<br>        &#125;<br>        <span class="hljs-comment">//2.2写出v3</span><br>        <span class="hljs-comment">//context.write(new Text(word),new IntWritable(count));</span><br>        <span class="hljs-comment">//因为v2和v3是一样的,我们可以优化一下</span><br>        context.write(key, <span class="hljs-keyword">new</span> IntWritable(count));<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountMain简写版"><a href="#WordCountMain简写版" class="headerlink" title="WordCountMain简写版"></a>WordCountMain简写版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 这里写的是驱动类, 即: 封装MR程序的核心8步的. 它有两种写法:</span><br><span class="hljs-comment"> * 1. 官方示例版, 即: 完整版.   理解即可, 因为稍显复杂, 用的人较少.</span><br><span class="hljs-comment"> * 2. 简化版.  推荐掌握.</span><br><span class="hljs-comment"> * 这里是简化版写法</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span><br>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);<br>        <span class="hljs-comment">//2.封装MR程序核心8步</span><br>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span><br>        job.setInputFormatClass(TextInputFormat.class);<br>        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/input/wordcount.txt&quot;</span>));<br>        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span><br>        job.setMapperClass(WordCountMapTask.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.3 分区,用默认的</span><br>        <span class="hljs-comment">//2.4 排序,用默认的</span><br>        <span class="hljs-comment">//2.5 规约,用默认的</span><br>        <span class="hljs-comment">//2.6 分组,用默认的</span><br>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span><br>        job.setReducerClass(WorkCountReduceTask.class);<br>        job.setOutputValueClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span><br>        job.setOutputFormatClass(TextOutputFormat.class);<br>        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/output&quot;</span>));<br>        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span><br>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);<br>        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span><br>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountMain-jar包版"><a href="#WordCountMain-jar包版" class="headerlink" title="WordCountMain jar包版"></a>WordCountMain jar包版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 这个代码一会儿是要打包成jar包, 然后放到Yarn集群中运行的, 需要做如下的几件事儿:</span><br><span class="hljs-comment"> * 1. 在驱动类中设置 jar包的启动类.</span><br><span class="hljs-comment"> * job.setJarByClass(WordCountMain3.class);</span><br><span class="hljs-comment"> * 2. 修改数据源文件 和 目的地文件的路径, 改为: 外部传入.</span><br><span class="hljs-comment"> * TextInputFormat.addInputPath(job, new Path(args[0]));</span><br><span class="hljs-comment"> * TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="hljs-comment"> * 3. 对我们当前的工程进行打包动作, 打包成: 胖jar, 具体操作为: 取消pom.xml文件中最后一个插件的注释, 然后打包即可.</span><br><span class="hljs-comment"> * 细节: 修改jar包名字为: wordcount.jar, 方便我们操作.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 4. 在HDFS集群中创建:   /wordcount/input/ 目录</span><br><span class="hljs-comment"> * 5. 把wordcount.txt 上传到该目录下.</span><br><span class="hljs-comment"> * 6. 把之前打好的 jar包也上传到 Linux系统中.</span><br><span class="hljs-comment"> * 7. 运行该jar包即可, 记得: 传入 数据源文件路径, 目的地目录路径.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 名词解释:</span><br><span class="hljs-comment"> * 胖jar: 指的是一个jar包中还包含有其他的jar包, 这样的jar包就称之为: 胖jar.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 问题1: 为什么需要打包成 胖jar?</span><br><span class="hljs-comment"> * 答案:</span><br><span class="hljs-comment"> * 因为目前我们的工程需要依赖 Hadoop环境, 而我们已经在pom.xml文件中配置了,</span><br><span class="hljs-comment"> * 如果运行的环境中(例如: Linux系统等)没有hadoop环境, 并且我们打包时也没有把hadoop环境打包进去,</span><br><span class="hljs-comment"> * 将来运行jar包的时候就会出错.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 问题2: 当前工程一定要打包成 胖jar吗?</span><br><span class="hljs-comment"> * 答案: 不用, 因为我们的 jar包一会儿是放到 Yarn集群中运行的, 它已经自带Hadoop环境, 所以这里可以不打包 胖jar, 只打包我们自己的代码.</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain3</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span><br>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);<br>        <span class="hljs-comment">//细节1: 在驱动类中设置 jar包的启动类.</span><br>        job.setJarByClass(WorkCountMain3.class);<br><br>        <span class="hljs-comment">//2.封装MR程序核心8步</span><br>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span><br>        job.setInputFormatClass(TextInputFormat.class);<br>        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">0</span>]));<br>        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span><br>        job.setMapperClass(WordCountMapTask.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.3 分区,用默认的</span><br>        <span class="hljs-comment">//2.4 排序,用默认的</span><br>        <span class="hljs-comment">//2.5 规约,用默认的</span><br>        <span class="hljs-comment">//2.6 分组,用默认的</span><br>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span><br>        job.setReducerClass(WorkCountReduceTask.class);<br>        job.setOutputValueClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span><br>        job.setOutputFormatClass(TextOutputFormat.class);<br>        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">1</span>]));<br>        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span><br>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);<br>        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span><br>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>基本流程</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java">基本流程<br><span class="hljs-number">1.</span>传输文件路径到自定义的Actor类,并接收返回值,解析返回值得出统计结果<br><span class="hljs-number">2.</span>自定义Actor类, 接收文件路径并做解析统计单词再返回给发送者<br><br>需要分别定义<span class="hljs-number">3</span>个类<br>Main入口<br><span class="hljs-number">1.</span>用于发送文件路径,封装在自定义的单例类里面<br><span class="hljs-number">2.</span>接收返回值,并做判断是否完成传输, 如果完成就开始解析<br><span class="hljs-number">3.</span>通过apply方法解析结果,合并结果得出最后结果<br><br>自定义的Actor类<br><span class="hljs-number">1.</span>接收文件路径信息,做分析统计<br><span class="hljs-number">2.</span>把结果封装在单例类中,返回给发送者<br><br>自定义的单例类<br><span class="hljs-number">1.</span>用于封装发送信息的单例类<br><span class="hljs-number">2.</span>用于返回统计的单例类<br><br></code></pre></td></tr></table></figure>

<h3 id="MainActor"><a href="#MainActor" class="headerlink" title="MainActor"></a>MainActor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs java">`<span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<br><br><span class="hljs-keyword">import</span> java.io.File<br><span class="hljs-keyword">import</span> scala.actors.Future<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.发送文件名给WordCountActor</span><br><span class="hljs-comment"> * 2.接收WordCountActor返回结果并合并</span><br><span class="hljs-comment"> */</span><br>object MainActor &#123;<br><br>  <span class="hljs-comment">//发送文件名给WordCountActor</span><br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//1.获取文件名</span><br>    val fileDir = <span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;./data&quot;</span>)<br>    val files: Array[File] = fileDir.listFiles()<br>    <span class="hljs-comment">// 测试是成功获取文件名</span><br>    <span class="hljs-comment">// files.foreach(println(_))</span><br><br>    <span class="hljs-comment">//2.发送给wordcountactor</span><br>    val future_Array: Array[Future[Any]] = files.map(f = file =&gt; &#123;<br>      val name = file.toString<br>      <span class="hljs-comment">//每一个文件名新建对应的线程</span><br>      val actor = <span class="hljs-keyword">new</span> WordCountActor<br>      <span class="hljs-comment">//开启线程并发送给我认定任务</span><br>      actor.start()<br>      <span class="hljs-comment">//发送的消息封装在这里面并获取结果</span><br>      val future: Future[Any] = actor !! WordCountTask(name)<br>      future<br>    &#125;)<br><br>    <span class="hljs-comment">//接收WordCountActor返回结果并合并</span><br>    <span class="hljs-comment">//先判断是否全部文件都处理完毕都有结果,是再处理</span><br>    <span class="hljs-keyword">while</span> (!(future_Array.filter((x) =&gt; &#123;<br>      !x.isSet<br>    &#125;)).isEmpty) &#123;&#125;<br>    <span class="hljs-comment">//走到这里, 证明我们可以处理,使用apply获取数据</span><br>    <span class="hljs-comment">//里面的键值对就是多个文件统计结果, 我们还需要合并去重</span><br>    val wordCount: Array[Map[String, Int]] = future_Array.map((x) =&gt; &#123;<br>      val results: Any = x.apply()<br>      val result = results.asInstanceOf[WordCountResult]<br>      val map: Map[String, Int] = result.map<br>      map<br>    &#125;)<br>    <span class="hljs-comment">//wordCount.foreach(println(_))</span><br>    <span class="hljs-comment">//测试结果</span><br>    <span class="hljs-comment">// Map(e -&gt; 2, f -&gt; 1, a -&gt; 1, b -&gt; 1, c -&gt; 1)</span><br>    <span class="hljs-comment">// Map(e -&gt; 1, a -&gt; 2, b -&gt; 1, c -&gt; 2, d -&gt; 3)</span><br><br>    <span class="hljs-comment">//合并结果, 先合并成一个Array</span><br>    val flatten: Array[(String, Int)] = wordCount.flatten<br>    <span class="hljs-comment">//根据Map的key值分组</span><br>    val wordGroup: Map[String, Array[(String, Int)]] = flatten.groupBy((x) =&gt; &#123;<br>      x._1<br>    &#125;)<br>    val finalResult: Map[String, Int] = wordGroup.map((x) =&gt; &#123;<br>      val name = x._1<br>      val size = x._2.size<br>      name -&gt; size<br>    &#125;)<br><br>    finalResult.foreach(println(_))<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountActor"><a href="#WordCountActor" class="headerlink" title="WordCountActor"></a>WordCountActor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<br><br><span class="hljs-keyword">import</span> scala.actors.Actor<br><span class="hljs-keyword">import</span> scala.io.Source<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.接收MainActor的文件名称并进行单词统计</span><br><span class="hljs-comment"> * 2.将单词统计结果返回给MainActor</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountActor</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Actor</span> </span>&#123;<br>  <span class="hljs-function">override def <span class="hljs-title">act</span><span class="hljs-params">()</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//接收消息</span><br>    loop &#123;<br>      react &#123;<br>        <span class="hljs-function"><span class="hljs-keyword">case</span> <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename)</span> </span>=&gt;<br>          println(<span class="hljs-string">&quot;收到了文件名: &quot;</span> + filename)<br>          <span class="hljs-comment">//解析消息, 通过Source解析消息, 定义文件来源再转化成列表</span><br>          <span class="hljs-comment">//一个元素就是一个一行数据</span><br>          val words: List[String] = Source.fromFile(filename).getLines().toList<br>          <span class="hljs-comment">//切割获取每一条数据并合并成一个list集合</span><br>          val word_List: List[String] = words.flatMap((x) =&gt; &#123;<br>            x.split(<span class="hljs-string">&quot; &quot;</span>)<br>          &#125;)<br>          <span class="hljs-comment">//按照单词进行分组, 然后聚合统计</span><br>          val word_Tuples: List[(String, Int)] = word_List.map((x) =&gt; &#123;<br>            (x, <span class="hljs-number">1</span>)<br>          &#125;)<br>          val word_Map: Map[String, List[(String, Int)]] = word_Tuples.groupBy((x) =&gt; &#123;<br>            x._1<br>          &#125;)<br>          val wordCountMap: Map[String, Int] = word_Map.map((x) =&gt; &#123;<br>            val name: String = x._1<br>            val size: Int = x._2.size<br>            name -&gt; size<br>          &#125;)<br><br>          <span class="hljs-comment">//把统计结果反馈给Mainactor,装进WordCount</span><br>          sender ! WordCountResult(wordCountMap)<br>      &#125;<br><br>    &#125;<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="WordCountPackage"><a href="#WordCountPackage" class="headerlink" title="WordCountPackage"></a>WordCountPackage</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.定义一个样例类, 描述单词统计信息</span><br><span class="hljs-comment"> * 2.定义一个样例类封装单词统计结果</span><br><span class="hljs-comment"> */</span><br>object WordCountPackage &#123;<br><br>  <span class="hljs-comment">//1.定义一个样例类, 描述单词统计信息</span><br>  <span class="hljs-function"><span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename: String)</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">  <span class="hljs-comment">//2.定义一个样例类封装单词统计结果</span></span><br><span class="hljs-function">  <span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountResult</span><span class="hljs-params">(map: Map[String, Int])</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">&#125;</span><br></code></pre></td></tr></table></figure>

<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><ul>
<li>基本流程</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-number">1.</span>创建上下文对象<br><span class="hljs-number">2.</span>读取文件<br><span class="hljs-number">3.</span>flatMap获取到每个单词<br><span class="hljs-number">4.</span>map将RDD变成 key-value结构<br><span class="hljs-number">5.</span>reduceByKey 求和统计<br><span class="hljs-number">6.</span>打印输出<br><span class="hljs-number">7.</span>关闭上下文对象<br></code></pre></td></tr></table></figure>

<ul>
<li>本地版</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day01<br><br><span class="hljs-keyword">import</span> org.apache.spark.rdd.RDD<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br>object WordCount &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>).setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc: SparkContext = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span><br>    val inputRDD: RDD[String] = sc.textFile(<span class="hljs-string">&quot;src/main/data/words.txt&quot;</span>)<br>    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span><br>    val flatRDD = inputRDD.flatMap((x) =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//4.继续对每个单词标记为1</span><br>    val wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))<br>    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span><br>    val ouputRDD = wordOneRDD.reduceByKey(_ + _)<br>    <span class="hljs-comment">//6.生成最后的RDD, 将结果打印到控制台</span><br>    ouputRDD.foreach(println(_))<br>    <span class="hljs-comment">//7.关闭上下文</span><br>    sc.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li>Linux版</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day01<br><br><span class="hljs-keyword">import</span> org.apache.spark.rdd.RDD<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br>object WordCount_Linux &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//0.创建输入路径和输出路径</span><br>    val input_path = args(<span class="hljs-number">0</span>)<br>    val output_path = args(<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>)<br>    val sc: SparkContext = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span><br>    val inputRDD: RDD[String] = sc.textFile(input_path)<br>    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span><br>    val flatRDD = inputRDD.flatMap((x) =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//4.继续对每个单词标记为1</span><br>    val wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))<br>    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span><br>    val ouputRDD = wordOneRDD.reduceByKey(_ + _)<br>    <span class="hljs-comment">//6.生成最后的RDD, 将结果上传到HDFS</span><br>    ouputRDD.saveAsTextFile(output_path)<br>    <span class="hljs-comment">//7.关闭上下文</span><br>    sc.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day05.sparkSQL<br><br><span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;Dataset, SparkSession&#125;<br><br>object S1WordcountSQL &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val spark: SparkSession = SparkSession.builder()<br>      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      .getOrCreate()<br>    <span class="hljs-comment">//读取文件生成DataSet, 只有一列时 列名自动为 value</span><br>    val inputDS: Dataset[String] = spark.read.textFile(<span class="hljs-string">&quot;src/main/data/words.txt&quot;</span>)<br>    <span class="hljs-comment">//扁平化单词, 得到新的DataSet</span><br>    <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 转换数据类型都需要转换???</span><br>    <span class="hljs-keyword">import</span> spark.implicits._<br>    val wordDS: Dataset[String] = inputDS.flatMap(x =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    wordDS.printSchema()<br>    wordDS.show()<br>    <span class="hljs-comment">//对新的DataSet进行SQL风格 Wordcount</span><br>    println(<span class="hljs-string">&quot;QL风格 Wordcount&quot;</span>)<br>    println()<br><br>    wordDS.createOrReplaceTempView(<span class="hljs-string">&quot;t_words&quot;</span>)<br>    spark.sql(<br>      <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">        |select value,count(1) cnt</span><br><span class="hljs-string">        |from t_words</span><br><span class="hljs-string">        |group by value</span><br><span class="hljs-string">        |order by cnt desc</span><br><span class="hljs-string">        |&quot;</span><span class="hljs-string">&quot;&quot;</span>.stripMargin).show()<br><br>    <span class="hljs-comment">//对新的DataSet进行DSL风格 Wordcount</span><br>    wordDS.groupBy(<span class="hljs-string">&quot;value&quot;</span>)<br>      .count() <span class="hljs-comment">//自动拼接一列count名</span><br>      .orderBy($<span class="hljs-string">&quot;count&quot;</span>.desc) <span class="hljs-comment">//转成column对象</span><br>      .show()<br><br>    <span class="hljs-comment">//关闭上下文对象</span><br>    spark.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h3><ul>
<li>前期准备：安装 netcat</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 在Linux上安装 netcat</span><br>yum install nc -y<br>yum install nmap -y<br><span class="hljs-comment">// 向 9999 端口发送数据</span><br>nc -lk <span class="hljs-number">9999</span><br></code></pre></td></tr></table></figure>

<ul>
<li>Wordcount  by UpdateStateByKey</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day06.streaming<br><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: wordcount 案例，通过 UpdateStateByKey 实现宕机后状态恢复</span><br><span class="hljs-comment"> * 需要利用ncat 发数据， </span><br><span class="hljs-comment"> */</span><br><br>object S4ocketWordcountUpdateStateByKeyRecovery &#123;<br>    <span class="hljs-comment">//设置路径</span><br>    val CKP =<span class="hljs-string">&quot;src/main/data/ckp/&quot;</span>+<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>)<br>    <span class="hljs-comment">//1.创建上下文对象, 指定批处理时间间隔为5秒</span><br>    val creatingFunc =()=&gt;<br>    &#123;<br>      val conf: SparkConf = <span class="hljs-keyword">new</span> SparkConf()<br>        .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>        .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>      <span class="hljs-comment">//2. 创建一个接收文本数据流的流对象</span><br>      val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br><br>      <span class="hljs-comment">//3.设置checkpoint位置</span><br>      ssc.checkpoint(CKP)<br>      <span class="hljs-comment">//4.接收socket数据</span><br>      val inputDStream: ReceiverInputDStream[String] = ssc.socketTextStream(<span class="hljs-string">&quot;node1&quot;</span>, <span class="hljs-number">9999</span>)<br>      <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 5.wordcount, 并做累计统计</span><br>      <span class="hljs-comment">//自定义一个函数, 实现保存State状态和数据聚合</span><br>      <span class="hljs-comment">//seq里面是value的数组,[1,1,], state是上次的状态, 累计值</span><br>      val updateFunc = (seq: Seq[Int], state: Option[Int]) =&gt; &#123;<br>        <span class="hljs-keyword">if</span> (!seq.isEmpty) &#123;<br>          val this_value: Int = seq.sum<br>          val last_value: Int = state.getOrElse(<span class="hljs-number">0</span>)<br>          val new_state: Int = this_value + <span class="hljs-function">last_value</span><br><span class="hljs-function">          <span class="hljs-title">Some</span><span class="hljs-params">(new_state)</span></span><br><span class="hljs-function">        &#125;</span><br><span class="hljs-function">        <span class="hljs-keyword">else</span> </span>&#123;<br>          state<br>        &#125;<br>      &#125;<br>      <span class="hljs-comment">//开始做wordcount,并打印输出</span><br>      val wordDStream: DStream[(String, Int)] = inputDStream.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>        .map((_, <span class="hljs-number">1</span>))<br>        .updateStateByKey(updateFunc)<br>      wordDStream.print()<br>    ssc<br>    &#125;<br><br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    val ssc: StreamingContext = StreamingContext.getOrCreate(CKP, creatingFunc)<br><br>    <span class="hljs-comment">//启动流式应用</span><br>    ssc.start()<br>    <span class="hljs-comment">//让应用一直处于监听状态</span><br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理关闭流式应用</span><br>    ssc.stop(<span class="hljs-keyword">true</span>, <span class="hljs-keyword">true</span>)<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="SparkStreaming-amp-Kafka"><a href="#SparkStreaming-amp-Kafka" class="headerlink" title="SparkStreaming &amp; Kafka"></a>SparkStreaming &amp; Kafka</h3><ul>
<li>自动提交 Offset</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord<br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><span class="hljs-keyword">import</span> scala.collection.mutable.Set<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: Spark  Kafka自动提交offset</span><br><span class="hljs-comment"> */</span><br>object S1KafkaAutoCommit &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    val kafkaParams = Map(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-keyword">true</span>: java.lang.Boolean) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br><br>    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](<br>      ssc,<br>      LocationStrategies.PreferConsistent,<br>      ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)<br>    )<br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    val resutDStream: DStream[Unit] = kafkaDStream.map(x =&gt; &#123;<br>      println(s<span class="hljs-string">&quot;topic=$&#123;x.topic()&#125;,partition=$&#123;x.partition()&#125;,offset=$&#123;x.offset()&#125;,key=$&#123;x.key()&#125;,value=$&#123;x.value()&#125;&quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//打印数据</span><br>    resutDStream.print()<br>    <span class="hljs-comment">//启动并停留</span><br>    ssc.start()<br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理化关闭</span><br>    ssc.stop(stopSparkContext = <span class="hljs-keyword">true</span>, stopGracefully = <span class="hljs-keyword">true</span>)<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li>手动提交 Offset</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord<br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;CanCommitOffsets, ConsumerStrategies, HasOffsetRanges, KafkaUtils, LocationStrategies, OffsetRange&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: Spark  Kafka 手动提交 offset 到默认 topic</span><br><span class="hljs-comment"> */</span><br>object S2KafkaCommit &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    val kafkaParams = Map(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-keyword">false</span>: java.lang.Boolean) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br><br>    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](<br>      ssc,<br>      LocationStrategies.PreferConsistent,<br>      ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)<br>    )<br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    kafkaDStream.foreachRDD(rdd =&gt; &#123;<br>      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;<br>        <span class="hljs-comment">//对每个批次进行处理</span><br>        <span class="hljs-comment">//提取并打印偏移量范围信息</span><br>        val hasOffsetRanges: HasOffsetRanges = rdd.asInstanceOf[HasOffsetRanges]<br>        val offsetRanges: Array[OffsetRange] = hasOffsetRanges.<span class="hljs-function">offsetRanges</span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)</span></span><br><span class="hljs-function">        offsetRanges.<span class="hljs-title">foreach</span><span class="hljs-params">(println(_)</span>)</span><br><span class="hljs-function">        <span class="hljs-comment">//打印每个批次的具体信息</span></span><br><span class="hljs-function">        rdd.<span class="hljs-title">foreach</span><span class="hljs-params">(x =&gt; &#123;</span></span><br><span class="hljs-params"><span class="hljs-function">          println(s<span class="hljs-string">&quot;topic=$&#123;x.topic()&#125;,partition=$&#123;x.partition()&#125;,offset=$&#123;x.offset()&#125;,key=$&#123;x.key()&#125;,value=$&#123;x.value()&#125;&quot;</span>)</span></span><br><span class="hljs-function">        &#125;)</span><br><span class="hljs-function">        <span class="hljs-comment">//手动将偏移量访问信息提交到默认主题</span></span><br><span class="hljs-function">        kafkaDStream.asInstanceOf[CanCommitOffsets].<span class="hljs-title">commitAsync</span><span class="hljs-params">(offsetRanges)</span></span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;成功提交了偏移量信息&quot;</span>)</span></span><br><span class="hljs-function">      &#125;</span><br><span class="hljs-function">    &#125;)</span><br><span class="hljs-function"></span><br><span class="hljs-function">    <span class="hljs-comment">//启动并停留</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">start</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">awaitTermination</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    <span class="hljs-comment">//合理化关闭</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">stop</span><span class="hljs-params">(<span class="hljs-keyword">true</span>,   <span class="hljs-keyword">true</span>)</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">  &#125;</span><br><span class="hljs-function"></span><br><span class="hljs-function">&#125;</span><br></code></pre></td></tr></table></figure>

<ul>
<li><p>手动提交 Offset 到 MySQL</p>
<ul>
<li>S3KafkaOffsetToMysql</li>
</ul>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord<br><span class="hljs-keyword">import</span> org.apache.kafka.common.TopicPartition<br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.InputDStream<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010._<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br><span class="hljs-keyword">import</span> scala.collection.mutable<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: Spark  Kafka 手动提交 offset 到默认 MySQL</span><br><span class="hljs-comment"> */</span><br>object S3KafkaOffsetToMysql &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    val kafkaParams = Map(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-keyword">false</span>: java.lang.Boolean) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br>    <span class="hljs-comment">//去MySQL查询上次消费的位置</span><br>    val offsetMap: mutable.Map[TopicPartition, Long] = OffsetUtil.getOffsetMap(<span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-string">&quot;spark_kafka&quot;</span>)<br><br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    <span class="hljs-keyword">var</span> kafkaDStream: InputDStream[ConsumerRecord[String, String]] = <span class="hljs-keyword">null</span><br>    <span class="hljs-comment">//第一次查询, MySQL没有 offset 数据</span><br>    <span class="hljs-keyword">if</span> (offsetMap.isEmpty) &#123;<br>      kafkaDStream = KafkaUtils.createDirectStream[String, String](<br>        ssc,<br>        LocationStrategies.PreferConsistent,<br>        ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams) <span class="hljs-comment">//第一次就看 Kafka 发啥</span><br>      )<br>    &#125;<br>    <span class="hljs-comment">//第二次查询, MySQL中有 offset 数据</span><br>    <span class="hljs-keyword">else</span> &#123;<br>      kafkaDStream = KafkaUtils.createDirectStream[String, String](<br>        ssc,<br>        LocationStrategies.PreferConsistent,<br>        ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams, offsetMap) <span class="hljs-comment">//第二次开始就从 MySQL 获取</span><br>      )<br>    &#125;<br><br>    <span class="hljs-comment">//对每个批次进行处理</span><br>    kafkaDStream.foreachRDD(rdd =&gt; &#123;<br>      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;<br>        <span class="hljs-comment">//提取并打印偏移量范围信息</span><br>        val hasOffsetRanges: HasOffsetRanges = rdd.asInstanceOf[HasOffsetRanges]<br>        val offsetRanges: Array[OffsetRange] = hasOffsetRanges.<span class="hljs-function">offsetRanges</span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)</span></span><br><span class="hljs-function">        offsetRanges.<span class="hljs-title">foreach</span><span class="hljs-params">(println(_)</span>)</span><br><span class="hljs-function">        <span class="hljs-comment">//打印每个批次的具体信息</span></span><br><span class="hljs-function">        rdd.<span class="hljs-title">foreach</span><span class="hljs-params">(x =&gt; &#123;</span></span><br><span class="hljs-params"><span class="hljs-function">          println(s<span class="hljs-string">&quot;topic=$&#123;x.topic()&#125;,partition=$&#123;x.partition()&#125;,offset=$&#123;x.offset()&#125;,key=$&#123;x.key()&#125;,value=$&#123;x.value()&#125;&quot;</span>)</span></span><br><span class="hljs-function">        &#125;)</span><br><span class="hljs-function">        <span class="hljs-comment">//手动将偏移量访问信息提交到MySQL</span></span><br><span class="hljs-function">        OffsetUtil.<span class="hljs-title">saveOffsetRanges</span><span class="hljs-params">(<span class="hljs-string">&quot;spark&quot;</span>, offsetRanges)</span></span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;成功提交了偏移量到MySQL&quot;</span>)</span></span><br><span class="hljs-function">      &#125;</span><br><span class="hljs-function">    &#125;)</span><br><span class="hljs-function"></span><br><span class="hljs-function">    <span class="hljs-comment">//启动并停留</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">start</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">awaitTermination</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    <span class="hljs-comment">//合理化关闭</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">stop</span><span class="hljs-params">(stopSparkContext = <span class="hljs-keyword">true</span>, stopGracefully = <span class="hljs-keyword">true</span>)</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">  &#125;</span><br><span class="hljs-function"></span><br><span class="hljs-function">&#125;</span><br></code></pre></td></tr></table></figure>

<ul>
<li>OffsetUtil</li>
</ul>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.common.TopicPartition<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.OffsetRange<br><span class="hljs-keyword">import</span> scala.collection.mutable.Map<br><br><span class="hljs-keyword">import</span> java.sql.&#123;DriverManager, ResultSet&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 定义一个单例对象, 定义 2 个方法</span><br><span class="hljs-comment"> *        方法1: 从 MySQL 读取行偏移量</span><br><span class="hljs-comment"> *        方法2: 将行偏移量保存的 MySQL</span><br><span class="hljs-comment"> */</span><br>object OffsetUtil &#123;<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * 定义一个单例方法, 将偏移量保存到MySQL数据库</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> groupid     消费者组id</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> offsetRange 行偏移量对象</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function">def <span class="hljs-title">saveOffsetRanges</span><span class="hljs-params">(groupid: String, offsetRange: Array[OffsetRange])</span> </span>= &#123;<br>    val connection = DriverManager.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>)<br>    <span class="hljs-comment">//replace into表示之前有就替换,没有就插入</span><br>    val ps = connection.prepareStatement(<span class="hljs-string">&quot;replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)&quot;</span>)<br>    <span class="hljs-keyword">for</span> (o &lt;- offsetRange) &#123;<br>      ps.setString(<span class="hljs-number">1</span>, o.topic)<br>      ps.setInt(<span class="hljs-number">2</span>, o.partition)<br>      ps.setString(<span class="hljs-number">3</span>, groupid)<br>      ps.setLong(<span class="hljs-number">4</span>, o.untilOffset)<br>      ps.executeUpdate()<br>    &#125;<br>    ps.close()<br>    connection.close()<br>  &#125;<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * 定义一个方法, 用于从 MySQL 中读取行偏移位置</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> groupid 消费者组id</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> topic   想要消费的数据主题</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function">def <span class="hljs-title">getOffsetMap</span><span class="hljs-params">(groupid: String, topic: String)</span> </span>= &#123;<br><br>    <span class="hljs-comment">//1.从数据库查询对应数据</span><br>    val connection = DriverManager.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>)<br><br>    val ps = connection.prepareStatement(<span class="hljs-string">&quot;select * from t_offset where groupid=?  and topic=?&quot;</span>)<br>    ps.setString(<span class="hljs-number">1</span>, groupid)<br>    ps.setString(<span class="hljs-number">2</span>, topic)<br>    val rs: ResultSet = ps.executeQuery()<br>    <span class="hljs-comment">//解析数据, 返回</span><br>    <span class="hljs-keyword">var</span> offsetMap = Map[TopicPartition, Long]()<br>    <span class="hljs-keyword">while</span> (rs.next()) &#123;<br>      val topicPartition = <span class="hljs-keyword">new</span> TopicPartition(rs.getString(<span class="hljs-string">&quot;topic&quot;</span>), rs.getInt(<span class="hljs-string">&quot;partition&quot;</span>))<br><br>      offsetMap.put(topicPartition, (rs.getLong(<span class="hljs-string">&quot;offset&quot;</span>)))<br>    &#125;<br>    rs.close()<br>    rs.close()<br>    connection.close()<br>    offsetMap<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="StructuredStreaming"><a href="#StructuredStreaming" class="headerlink" title="StructuredStreaming"></a>StructuredStreaming</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.structuredStreaming<br><br><span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;DataFrame, Dataset, Row, SparkSession&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.sql.streaming.StreamingQuery<br><span class="hljs-keyword">import</span> org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: wordcount 案例 之 读取文件</span><br><span class="hljs-comment"> */</span><br>object S2StructuredStreamingTextFile &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    val spark: SparkSession = SparkSession.builder()<br>      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)<br>      .getOrCreate()<br>    <span class="hljs-comment">//2.读取csv, 得到流式DataFrame, 每行就是每批次的行数据</span><br>    <span class="hljs-comment">//自定义 Schema 信息</span><br>    val schema = <span class="hljs-keyword">new</span> StructType(Array(<br>      StructField(<span class="hljs-string">&quot;name&quot;</span>, StringType),<br>      StructField(<span class="hljs-string">&quot;age&quot;</span>, IntegerType),<br>      StructField(<span class="hljs-string">&quot;hobby&quot;</span>, StringType))<br>    )<br>    val inputDF: DataFrame = spark.readStream<br>      .format(<span class="hljs-string">&quot;csv&quot;</span>)<br>      .option(<span class="hljs-string">&quot;sep&quot;</span>, <span class="hljs-string">&quot;;&quot;</span>)<br>      .schema(schema)<br>      .load(<span class="hljs-string">&quot;src/main/data/input/persons&quot;</span>)<br>    <span class="hljs-comment">//3.进行wordcount, DSL风格</span><br>    inputDF.printSchema()<br>    <span class="hljs-comment">//用 DSL 风格实现</span><br>    <span class="hljs-keyword">import</span> spark.implicits._<br>    val DF: Dataset[Row] = inputDF.where(<span class="hljs-string">&quot;age&lt;25&quot;</span>)<br>      .groupBy(<span class="hljs-string">&quot;hobby&quot;</span>)<br>      .count()<br>      .orderBy($<span class="hljs-string">&quot;count&quot;</span>.desc)<br><br>    <span class="hljs-comment">// 用 SQL 风格实现</span><br>    inputDF.createOrReplaceTempView(<span class="hljs-string">&quot;t_spark&quot;</span>)<br>    val DF2: DataFrame = spark.sql(<br>      <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">        |select</span><br><span class="hljs-string">        |hobby,</span><br><span class="hljs-string">        |count(1) as cnt</span><br><span class="hljs-string">        |from t_spark</span><br><span class="hljs-string">        |where age&lt;25</span><br><span class="hljs-string">        |group by hobby</span><br><span class="hljs-string">        |order by cnt desc</span><br><span class="hljs-string">        |&quot;</span><span class="hljs-string">&quot;&quot;</span>.stripMargin)<br><br>    val query: StreamingQuery = DF.writeStream<br>      <span class="hljs-comment">//append 默认追加 输出新的数据, 只支持简单查询, 有聚合就不能使用</span><br>      <span class="hljs-comment">//complete:完整模式, 输出完整数据, 支持集合和排序</span><br>      <span class="hljs-comment">//update: 更新模式, 输出有更新的数据,  支持聚合但是不支持排序</span><br>      .outputMode(<span class="hljs-string">&quot;complete&quot;</span>)<br>      .format(<span class="hljs-string">&quot;console&quot;</span>)<br>      .option(<span class="hljs-string">&quot;rowNumber&quot;</span>, <span class="hljs-number">10</span>)<br>      .option(<span class="hljs-string">&quot;truncate&quot;</span>, <span class="hljs-keyword">false</span>)<br>      <span class="hljs-comment">//4.启动流式查询</span><br>      .start()<br>    <span class="hljs-comment">//5.驻留监听</span><br>    query.awaitTermination()<br>    <span class="hljs-comment">//6.关闭流式查询</span><br>    query.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="批处理-DataSet"><a href="#批处理-DataSet" class="headerlink" title="批处理 DataSet"></a>批处理 DataSet</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.flink.start;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.operators.Order;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.operators.*;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span>: Jface</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Date</span>: 2021/9/5 12:37</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 基于Flink引擎实现批处理词频统计WordCount：过滤filter、排序sort等操作</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_01WordCount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.准备环境-env</span><br>        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();<br>        <span class="hljs-comment">//2.准备数据-source</span><br>        DataSource&lt;String&gt; inputDataSet = env.readTextFile(<span class="hljs-string">&quot;datas/wc.input&quot;</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.1 过滤脏数据</span><br>        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataSet.filter(<span class="hljs-keyword">new</span> FilterFunction&lt;String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">filter</span><span class="hljs-params">(String line)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span> != line &amp;&amp; line.trim().length() &gt; <span class="hljs-number">0</span>;<br>            &#125;<br>        &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.2 切割</span><br>                .flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>                    <span class="hljs-meta">@Override</span><br>                    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String line, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                        <span class="hljs-keyword">for</span> (String s : line.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>)) &#123;<br>                            out.collect(s);<br>                        &#125;<br>                    &#125;<br>                &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.3 转换二元组</span><br>                .map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>                    <span class="hljs-meta">@Override</span><br>                    <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String word)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                        <span class="hljs-keyword">return</span> Tuple2.of(word, <span class="hljs-number">1</span>);<br>                    &#125;<br>                &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.4 分组求和</span><br>                .groupBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataSet.printToErr();<br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> sort 排序，全局排序需要设置分区数 1</span><br>        SortPartitionOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sortDataSet = resultDataSet.sortPartition(<span class="hljs-string">&quot;f1&quot;</span>, Order.DESCENDING)<br>                .setParallelism(<span class="hljs-number">1</span>);<br>        sortDataSet.printToErr();<br>        <span class="hljs-comment">//只选择前3的数据</span><br>        GroupReduceOperator&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; resultDataSet2 = sortDataSet.first(<span class="hljs-number">3</span>);<br>        resultDataSet2.print();<br><br>        <span class="hljs-comment">//5.触发执行-execute，没有写出不需要触发执行</span><br><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="流处理-DataStream"><a href="#流处理-DataStream" class="headerlink" title="流处理 DataStream"></a>流处理 DataStream</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.stream;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现实时流式数据处理，监听端口并做 wordcount</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StreamWordcount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>         <span class="hljs-comment">//1.准备环境-env</span><br>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();<br>         <span class="hljs-comment">//2.准备数据-source</span><br>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span><br>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);<br>                <span class="hljs-keyword">for</span> (String s : arr) &#123;<br>                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span><br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span><br>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> Tuple2.of(value,<span class="hljs-number">1</span>);<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span><br>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataSet.print();<br>        <span class="hljs-comment">//5.触发执行-execute</span><br>        env.execute(StreamWordcount.class.getSimpleName());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>流处理 Flink On Yarn</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.submit;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现流式数据处理，从socket 接收数据并做 wordcount</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Wordcount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//0.使用工具类，解析程序传递参数</span><br>        ParameterTool parameterTool = ParameterTool.fromArgs(args);<br>        <span class="hljs-keyword">if</span> (parameterTool.getNumberOfParameters() != <span class="hljs-number">2</span>) &#123;<br>            System.out.println(<span class="hljs-string">&quot;Usage: WordCount --host &lt;host&gt; --port &lt;port&gt; ............&quot;</span>);<br>            System.exit(-<span class="hljs-number">1</span>);<br>        &#125;<br>        String host = parameterTool.get(<span class="hljs-string">&quot;host&quot;</span>);<br>        parameterTool.getInt(<span class="hljs-string">&quot;port&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//1.准备环境-env</span><br>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();<br>        <span class="hljs-comment">//2.准备数据-source</span><br>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span><br>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataStream = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);<br>                <span class="hljs-keyword">for</span> (String s : arr) &#123;<br>                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span><br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span><br>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> Tuple2.of(value, <span class="hljs-number">1</span>);<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span><br>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataStream.print();<br>        <span class="hljs-comment">//5.触发执行-execute</span><br>        env.execute(Wordcount.class.getSimpleName());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>


            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%BC%80%E5%8F%91%E6%A0%B7%E4%BE%8B/">开发样例</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Spark/">Spark</a>
                    
                      <a class="hover-with-bg" href="/tags/Hadoop/">Hadoop</a>
                    
                      <a class="hover-with-bg" href="/tags/Scala/">Scala</a>
                    
                      <a class="hover-with-bg" href="/tags/Flink/">Flink</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Redis常见面试题</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/09/28/Spark-Review-Day02/">
                        <span class="hidden-mobile">Apache Spark：分布式并行计算框架（二）</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"fd4471bb76ba37363dfe","clientSecret":"945ad45c695de45f4f9ee79068049e1f6ccf5f04","repo":"GitalkRepo","owner":"Jface001","admin":"Jface001","language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: 'c454aedb86401d5285e1cb0902fa6e39'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
