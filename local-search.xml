<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Vol.12 Kafka 核心工作原理小记</title>
    <link href="/2023/02/26/vol12-Kafka%E6%A0%B8%E5%BF%83%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%B0%8F%E8%AE%B0/"/>
    <url>/2023/02/26/vol12-Kafka%E6%A0%B8%E5%BF%83%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%B0%8F%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="消息队列简介"><a href="#消息队列简介" class="headerlink" title="消息队列简介"></a>消息队列简介</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>消息队列MQ用于实现两个系统或模块之间传递消息数据时, 实现数据缓存</p><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><p>基于队列方式, 实现传递消息的数据缓存</p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ul><li>实时高性能高吞吐量高可靠的消息传递架构</li><li>大数据应用: 作为唯一的实时数据存储平台</li><li>实时数据采集: 生产写入Kafka</li><li>数据数据处理: 消费读取Kafka</li></ul><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>解耦</li><li>异步保证最终一致性, 提高传输性能</li><li>限流削峰</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>运行更复杂, 必须保证消费队列是可靠的</li><li>数据安全保障更复杂, 必须保证生产和消费都是安全的</li></ul><h4 id="同步和异步"><a href="#同步和异步" class="headerlink" title="同步和异步"></a>同步和异步</h4><ul><li><p>同步</p></li><li><ul><li>概念</li></ul></li><li><ul><li><ul><li>提交和处理是同步操作，立即就能看到结果，立即一致性</li></ul></li></ul></li><li><ul><li>优缺点</li></ul></li><li><ul><li><ul><li>安全但性能较低</li></ul></li></ul></li><li><p>异步</p></li><li><ul><li>概念</li></ul></li><li><ul><li><ul><li>提交和处理是异步操作，最终得到一个处理的结果，最终一致性</li></ul></li></ul></li><li><ul><li>优缺点</li></ul></li><li><ul><li><ul><li>性能更高但结果可能有误差</li></ul></li></ul></li></ul><h4 id="点对点模式"><a href="#点对点模式" class="headerlink" title="点对点模式"></a>点对点模式</h4><p><img src="https://s2.loli.net/2023/02/26/Ln3SwmOIzRjulxk.png"></p><ul><li><p>特点</p></li><li><ul><li>数据只能被一个消费者使用</li><li>消费成功以后数据就会被删除，无法实现消费数据的共享</li></ul></li></ul><h4 id="订阅发布模式"><a href="#订阅发布模式" class="headerlink" title="订阅发布模式"></a>订阅发布模式</h4><p><img src="https://s2.loli.net/2023/02/26/9FXpOSfPZLYWiGb.png"></p><ul><li>特点</li></ul><p>1.一个Topic可以被多个消费者订阅</p><p>2.一个消费者可以订阅多个Topic</p><p>3.Topic中的数据可以实现不同消费者共享</p><h3 id="Kafka基本介绍"><a href="#Kafka基本介绍" class="headerlink" title="Kafka基本介绍"></a>Kafka基本介绍</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul><li>分布式的基于订阅发布模式</li><li>高性能高吞吐量高可用高灵活性高安全性的实时消息队列系统</li></ul><h4 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h4><ul><li><p>分布式流式数据实时存储</p></li><li><ul><li>分布式存储</li><li>实时消息队列存储,工作常用</li></ul></li><li><p>分布式流式计算</p></li><li><ul><li>分布式计算KaflaStream</li><li>基本不用</li></ul></li></ul><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul><li><p>对比其它特点</p></li><li><ul><li>适用于大数据量的临时性存储(安全+数据量较大)</li><li>缓存时间相对较长</li></ul></li><li><p>高性能</p></li><li><ul><li>基于分布式内存+分布式磁盘存储系统</li><li>顺序写入和顺序读取,索引</li></ul></li><li><p>高并发</p></li><li><ul><li>分布式并行读写</li></ul></li><li><p>高吞吐量</p></li><li><ul><li>分布式磁盘存储,没有使用HDFS</li></ul></li><li><p>高可靠</p></li><li><ul><li>分布式主从架构</li><li>节点备份机制</li></ul></li><li><p>高安全性</p></li><li><ul><li>数据安全保障机制</li><li>内存是操作系统级别</li><li>副本机制</li></ul></li><li><p>高灵活性</p></li><li><ul><li>根据需求添加生产者和消费者</li></ul></li></ul><h4 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h4><p>实时场景图示<img src="https://s2.loli.net/2023/02/26/o32Clt9jkAyERFp.png"></p><ul><li><p>实时大数据,必用Kafka</p></li><li><ul><li>离线数仓Hive</li><li>实时数仓Kafka</li><li>Kafka生产者数据采集工具: Flume Logstash</li><li>Kafka消费者实时计算程序: SparkStreaming Flink</li></ul></li></ul><h3 id="结构概念"><a href="#结构概念" class="headerlink" title="结构概念"></a>结构概念</h3><p><img src="https://s2.loli.net/2023/02/26/cgDyoJvqnU9hV23.png"></p><h4 id="Producer生产者"><a href="#Producer生产者" class="headerlink" title="Producer生产者"></a>Producer生产者</h4><ul><li>负责将数据写入Kafka</li><li>生产环境一般是数据采集工具</li></ul><h4 id="Broker节点"><a href="#Broker节点" class="headerlink" title="Broker节点"></a>Broker节点</h4><h4 id="Consumer消费者"><a href="#Consumer消费者" class="headerlink" title="Consumer消费者"></a>Consumer消费者</h4><ul><li>负责从Kafka中消费数据</li><li>主要消费的数据是value</li></ul><h4 id="Consumer-Group消费者组"><a href="#Consumer-Group消费者组" class="headerlink" title="Consumer Group消费者组"></a>Consumer Group消费者组</h4><ul><li>必须以消费者组的形式从Kafka中消费数据</li><li>任何一个消费者必须属于某一个消费者组</li><li>消费者组中的多个消费者消费的数据是不一样的,合起来是完整数据</li></ul><h4 id="Topic数据主题"><a href="#Topic数据主题" class="headerlink" title="Topic数据主题"></a>Topic数据主题</h4><ul><li>区分不同数据, 对数据进行分类</li><li>一个Topic可以划分多个Partition分区</li></ul><h4 id="Partition数据分区"><a href="#Partition数据分区" class="headerlink" title="Partition数据分区"></a>Partition数据分区</h4><ul><li>用于实现Topic数据的分布式存储</li><li>一个Topic可以划分多个Partition分区</li><li>不同分区存储在不同的Broker上</li><li>根据一定的规则决定写入哪个分区</li></ul><h4 id="Segment分区内数据文件"><a href="#Segment分区内数据文件" class="headerlink" title="Segment分区内数据文件"></a>Segment分区内数据文件</h4><ul><li><p>概念</p></li><li><ul><li>分区数据更细的划分</li></ul></li><li><p>作用</p></li><li><ul><li>可以加快数据检索效率</li><li>实现数据的删除处理</li></ul></li><li><p>创建流程</p></li><li><ul><li>每个Segment对应2个Segment文件</li></ul></li><li><ul><li><ul><li>XXXX.log真正存储的数据</li><li>XXXX.index索引文件</li></ul></li></ul></li><li><ul><li>数据写入Segment达到阈值会创建新的Segment文件</li></ul></li></ul><h4 id="Offset每条数据在自己分区的偏移量"><a href="#Offset每条数据在自己分区的偏移量" class="headerlink" title="Offset每条数据在自己分区的偏移量"></a>Offset每条数据在自己分区的偏移量</h4><ul><li><p>产生</p></li><li><ul><li>数据写入Topic分区时,分区内部自动给每条数据进行Offset编号</li></ul></li><li><p>级别</p></li><li><ul><li>Offset是分区级别,每个分区独立管理,从0开始</li></ul></li><li><p>功能</p></li><li><ul><li>消费者读取按照Offset来读取数据</li><li>保证消费者每次按照Offset顺序消费保证消息不重复不丢失</li></ul></li></ul><h4 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h4><ul><li><p>概述</p></li><li><ul><li>Kafka通过副本机制来保证数据安全性</li><li>一个分区有多个副本,存储在不同Broker上</li><li>副本个数小于等于Broker个数</li></ul></li><li><p>角色</p></li><li><ul><li>Leader副本</li></ul></li><li><ul><li><ul><li>提高读写</li></ul></li></ul></li><li><ul><li>Follower副本</li></ul></li><li><ul><li><ul><li>与Leader同步数据</li><li>如果Leader故障,选举新的Leader</li></ul></li></ul></li></ul><h3 id="Kafka存储机制"><a href="#Kafka存储机制" class="headerlink" title="Kafka存储机制"></a>Kafka存储机制</h3><h4 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h4><p><img src="https://s2.loli.net/2023/02/26/m9hnAaFgleX4uIc.png"></p><ul><li><p>Broker存储节点</p></li><li><p>Producer生产者</p></li><li><p>Topic数据主题</p></li><li><ul><li>用于区分不同的数据</li></ul></li><li><p>Partition数据分区</p></li><li><ul><li>分布式存储单元</li><li>名称构成：Topic名称+分区编号</li></ul></li><li><p>Segment</p></li><li><ul><li>分区段, 每个分区的数据存储在1个或多个Segment</li><li>每个Segment由一对文件构成</li><li>.log存储的数据</li><li>.index基于offset检索的稀疏索引</li><li>.timeindex基于数据时间检索的索引</li></ul></li></ul><h4 id="写入过程"><a href="#写入过程" class="headerlink" title="写入过程"></a>写入过程</h4><ol><li>生产者生产每一条数据,放入batch批次中, 达到时间或大小条件提交写入请求</li><li>根据分区规则,计算要写入的数据, 连接Kafka获取ZK地址, 用于获取元数据, 获取当前分区的Leader副本所有的Broker地址</li><li>请求对应的Broker写入数据, 先写入PageCache内存</li><li>后台实现同步或者异步写入顺序写磁盘,写入当前最新的Segment文件</li></ol><h4 id="Segment设计"><a href="#Segment设计" class="headerlink" title="Segment设计"></a>Segment设计</h4><ul><li><p>设计思想</p></li><li><ul><li>加快查询效率</li><li>减少删除数据的磁盘IO</li></ul></li><li><p>基本实现</p></li><li><ul><li>.log存储真正的数据</li><li>.index存储对应的.log的索引</li></ul></li><li><p>划分规则</p></li><li><ul><li>按照时间周期生成</li><li>按照文件大小生成</li></ul></li><li><p>命名规则</p></li><li><ul><li>以当前文件存储的最小offset来命名的</li></ul></li></ul><h4 id="读取过程"><a href="#读取过程" class="headerlink" title="读取过程"></a>读取过程</h4><ol><li>消费者根据TopicPartitionOffset提交Kafka请求读取数据</li><li>Kafka根据元数据信息, 找到分区对应的Leader副本所有的Broker</li><li>请求Leader副本所在Broker, 先读取PageCache, 通过零拷贝机制读取</li><li>如果PageCache没有, 就读取Segment文件端,先根据Offset找到对应的的Segment</li><li>将.log文件对应的.index文件加载到内存中, 根据index中的索引信息找到Offset在.log文件中最近位置</li><li>读取.log文件, 根据索引读取对应的Offset的数据</li></ol><h4 id="index索引设计"><a href="#index索引设计" class="headerlink" title="index索引设计"></a>index索引设计</h4><ul><li><p>索引分类</p></li><li><ul><li>全量索引：每一条数据都有索引</li><li>稀疏索引：只有部分数据有索引, Kafka使用</li></ul></li><li><p>生成规则</p></li><li><ul><li>log.index.interval.bytes=4096</li></ul></li><li><p>索引内容</p></li></ul><p><img src="https://s2.loli.net/2023/02/26/qpGY92oTPbI846O.png"></p><ul><li><ul><li>第1列：这条数据在这个文件中的位置</li><li>第2列：这条数据在文件中的物理偏移量</li></ul></li><li><p>检索数据流程</p></li></ul><ol><li>先根据offset计算这条offset是这个文件中的第几条</li><li>读取.index索引，根据二分检索，从索引中找到离这条数据最近偏小的位置</li><li>读取.log文件从最近位置读取到要查找的数据</li></ol><h4 id="Kafka写入很快原因"><a href="#Kafka写入很快原因" class="headerlink" title="Kafka写入很快原因"></a>Kafka写入很快原因</h4><ol><li>先写PageCache操作系统级内存</li><li>顺序写入磁盘</li></ol><h4 id="Kafka读取很快原因"><a href="#Kafka读取很快原因" class="headerlink" title="Kafka读取很快原因"></a>Kafka读取很快原因</h4><ol><li>先读PageCache内存</li><li>通过索引读取磁盘</li><li>零拷贝机制,减少IO</li><li>MMAP设计</li></ol><h3 id="生产分区"><a href="#生产分区" class="headerlink" title="生产分区"></a>生产分区</h3><h4 id="分区规则"><a href="#分区规则" class="headerlink" title="分区规则"></a>分区规则</h4><ul><li><p>指定分区</p></li><li><ul><li>写入指定分区</li></ul></li><li><p>指定key</p></li><li><ul><li>按照key的Hash取余 </li><li>Hash分区: 只要Hash值一样就会进入相同分区</li></ul></li><li><p>没有指定key</p></li><li><ul><li>轮询分区: 每一条数据轮流放入不同分区(相同的数据会进入不同分区)</li><li>粘性分区StickyPartitioner: 全部数据随机选中一个分区</li></ul></li></ul><h4 id="自定义开发生产分区器"><a href="#自定义开发生产分区器" class="headerlink" title="自定义开发生产分区器"></a>自定义开发生产分区器</h4><ol><li>开发一个类实现Partitioner接口</li><li>实现partition方法</li><li>生产者加载分区器</li></ol><h3 id="消费负载均衡"><a href="#消费负载均衡" class="headerlink" title="消费负载均衡"></a>消费负载均衡</h3><h4 id="基本规则"><a href="#基本规则" class="headerlink" title="基本规则"></a>基本规则</h4><ul><li><ul><li><ul><li>一个分区只能被一个消费者消费</li><li>一个消费者可以消费多个分区数据</li></ul></li></ul></li></ul><h4 id="属性配置"><a href="#属性配置" class="headerlink" title="属性配置"></a>属性配置</h4><h4 id="partition-assignment-strategy-org-apache-kafka-clients-consumer-RangeAssignor"><a href="#partition-assignment-strategy-org-apache-kafka-clients-consumer-RangeAssignor" class="headerlink" title="partition.assignment.strategy = org.apache.kafka.clients.consumer.RangeAssignor"></a>partition.assignment.strategy = org.apache.kafka.clients.consumer.RangeAssignor</h4><h4 id="分配策略"><a href="#分配策略" class="headerlink" title="分配策略"></a>分配策略</h4><ul><li><p>RangeAssignor范围分配</p></li><li><ul><li>说明</li></ul></li><li><ul><li><ul><li>默认分配策略</li><li>每个消费者消费一定范围分区,尽量均分</li><li>如果不能均分，优先将分区分配给编号小的消费者</li><li>针对消费者的每个Topic进行范围分配</li></ul></li></ul></li><li><ul><li>优点</li></ul></li><li><ul><li><ul><li>Topic较少会分配相对均衡</li></ul></li></ul></li><li><ul><li>缺点</li></ul></li><li><ul><li><ul><li>Topic个数较多, 而且不能均分,负载失衡</li></ul></li></ul></li><li><ul><li>应用</li></ul></li><li><ul><li><ul><li>适用于Topic个数少,或者Topic能均分场景</li></ul></li></ul></li><li><p>RoundRobinAssignor轮询分配</p></li><li><ul><li>说明</li></ul></li><li><ul><li><ul><li>2.0版本前常用</li><li>按照Topic的名称和分区编号,轮询分配每个消费者</li></ul></li></ul></li><li><ul><li>优点</li></ul></li><li><ul><li><ul><li>多个消费者订阅一样Topic的场景, 能实现负载均衡</li></ul></li></ul></li><li><ul><li>缺点</li></ul></li><li><ul><li><ul><li>消费者订阅不同的Topic,会导致整体负载不均衡</li></ul></li></ul></li><li><ul><li>应用</li></ul></li><li><ul><li><ul><li>所有消费者订阅相同的Topic, 订阅关系都一样的场景</li></ul></li></ul></li><li><p>StickyAssignor粘性分配</p></li><li><ul><li>说明</li></ul></li><li><ul><li><ul><li>2.0版本后常用</li><li>相对的保证的分配的均衡</li><li>如果消费者故障, 尽量避免网络传输</li><li>尽量保证原理消费的分区不变,多余分区再均衡</li></ul></li></ul></li><li><ul><li>优点</li></ul></li><li><ul><li><ul><li>分配更均衡</li><li>消费者故障也可以避免负载失衡</li></ul></li></ul></li><li><ul><li>应用</li></ul></li><li><ul><li><ul><li>2.0版本之后建议使用</li></ul></li></ul></li></ul><h3 id="消费者消费过程"><a href="#消费者消费过程" class="headerlink" title="消费者消费过程"></a>消费者消费过程</h3><h4 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h4><ul><li>根据Offset进行消费，每次从上一次的位置继续消费</li><li>Topic + Partition + Offset</li></ul><h4 id="第1次消费"><a href="#第1次消费" class="headerlink" title="第1次消费"></a>第1次消费</h4><ul><li>由属性决定auto.offset.reset = latest | earliest | none</li><li>latest从最新位置开始</li><li>earliest从最早位置开始,从offset 0 开始</li><li>none 抛出异常</li></ul><h4 id="第2次开始"><a href="#第2次开始" class="headerlink" title="第2次开始"></a>第2次开始</h4><p><img src="https://s2.loli.net/2023/02/26/425c1OEqsBfaYkw.png"></p><ul><li>根据上一次消费的Offset位置+1继续进行消费</li><li>消费者将consumer offset记录在内存</li><li>下次消费consumer offset+1得到commit offset</li></ul><h4 id="Offset偏移量管理"><a href="#Offset偏移量管理" class="headerlink" title="Offset偏移量管理"></a>Offset偏移量管理</h4><ul><li><p>说明</p></li><li><ul><li>Kafka将每个消费者消费的位置主动记录在一个Topic中__consumer_offsets</li><li>如果下次消费者没有给定请求offset，kafka就根据自己记录的offset来提供消费的位置</li></ul></li><li><p>提交规则</p></li><li><ul><li>根据时间自动提交</li><li>props.setProperty(“enable.auto.commit”, “true”);</li><li>props.setProperty(“auto.commit.interval.ms”, “1000”)</li></ul></li><li><p>自动提交</p></li><li><ul><li>消费丢失: 还没有消费就提交offset</li><li>消费重复: 消费了offset还没有提交</li></ul></li></ul><h4 id="手动提交Topic-Offset"><a href="#手动提交Topic-Offset" class="headerlink" title="手动提交Topic Offset"></a>手动提交Topic Offset</h4><ul><li><p>关闭自动提交</p></li><li><ul><li>props.setProperty(“enable.auto.commit”, “false”);</li></ul></li><li><p>消费完成后手动提交</p></li><li><ul><li>consumer.commitSync();</li></ul></li><li><p>存在问题</p></li><li><ul><li>offset是分区级别，提交时topic级别，只要有一个分区失败，整个提交失败</li><li>通过手动提交分区offset实现</li></ul></li></ul><h4 id="手动提交分区-Offset"><a href="#手动提交分区-Offset" class="headerlink" title="手动提交分区 Offset"></a>手动提交分区 Offset</h4><ul><li><p>消费每个分区数据</p></li><li><p>处理每个分区的数据</p></li><li><p>手动提交每个分区的offset</p></li><li><ul><li> consumer.commitSync(offsets);</li></ul></li></ul><h4 id="自行管理offset"><a href="#自行管理offset" class="headerlink" title="自行管理offset"></a>自行管理offset</h4><ul><li><p>保存在外部系统</p></li><li><ul><li>MySQL Redis checkpoint zookeeper</li></ul></li><li><p>保存offset</p></li><li><ul><li>JDBCreplace</li></ul></li><li><p>读取offset</p></li><li><ul><li>JDBC</li></ul></li></ul><h4 id="指定消费"><a href="#指定消费" class="headerlink" title="指定消费"></a>指定消费</h4><ul><li>指定Topic消费</li><li>consumer.subscribe(Arrays.asList(“bigdata01”));</li><li>指定分区消费</li><li>consumer.assign(Arrays.asList(part1,part2));</li><li>指定offset消费</li><li>consumer.seek(TopicPartition,offset);</li></ul><h3 id="ACK机制和重试机制"><a href="#ACK机制和重试机制" class="headerlink" title="ACK机制和重试机制"></a>ACK机制和重试机制</h3><h4 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h4><p>反向应答确认机制</p><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>配合重试机制, 保证生产数据不丢失</p><h4 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h4><ul><li><p>参数0</p></li><li><ul><li>生产者写入一条数据到Kafka，不管Kafka有没有收到这条数据，都直接发送下一条</li><li>速度快但数据容易丢失</li></ul></li><li><p>参数1</p></li><li><ul><li>生产者写入一条数据到Kafka，等待Kafka将这条数据写入对应分区的Leader副本</li><li>Kafka返回一个ack，生产者收到ack，发送下一条</li><li> 性能和安全性折中，依旧存在数据丢失的风险</li></ul></li><li><p>参数all或-1</p></li><li><ul><li>生产者写入一条数据到Kafka，等待Kafka将这条数据写入对应分区Leader副本，</li><li>等待ISR副本同步成功以后，Kafka返回一个ack，生产者收到ack，发送下一条</li><li> 安全但性能较差</li></ul></li><li><p>特别说明</p></li><li><ul><li>如果配置ack为1或者all/-1，生产者必须等待收到ack，再发送下一条</li><li>如果没有收到，超过一定时间，生产者重新发送重试机制</li></ul></li></ul><h3 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h3><h4 id="概念-2"><a href="#概念-2" class="headerlink" title="概念"></a>概念</h4><ul><li>Kafka用于实现实时消息队列的数据缓存</li><li>不需要永久性的存储数据</li></ul><h4 id="开启配置"><a href="#开启配置" class="headerlink" title="开启配置"></a>开启配置</h4><ul><li>开启清理</li><li>log.cleaner.enable = true</li><li>清理规则</li><li>log.cleanup.policy=delete | compact</li></ul><h4 id="delete清理规则"><a href="#delete清理规则" class="headerlink" title="delete清理规则"></a>delete清理规则</h4><ul><li>基于存活时间–常用</li><li>基于文件大小</li><li>基于offset消费规则</li></ul><h4 id="compact清理规则"><a href="#compact清理规则" class="headerlink" title="compact清理规则"></a>compact清理规则</h4><p><img src="https://s2.loli.net/2023/02/26/NiyYjScbv2wf7MC.png"></p><ul><li>将重复的更新数据的老版本删除，保留新版本</li><li>要求每条数据必须要有Key，根据Key来判断是否重复</li></ul><h3 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h3><p><img src="https://s2.loli.net/2023/02/26/2yCYw3sSoucQqzI.png"></p><h4 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h4><ul><li><p>Kafka分布式主从架构</p></li><li><ul><li>Kafka Controller主节点</li></ul></li><li><ul><li><ul><li>特殊Broker, 启动时ZK选举得出, 负责普通Broker工作</li><li>管理所有从节点,Topic,分区,副本</li><li>决定分区的Leader和Follower</li></ul></li></ul></li><li><ul><li>Kafka Broker从节点</li></ul></li><li><ul><li><ul><li>对外提供读写</li><li>监听Controller,如果故障重写选举</li></ul></li></ul></li><li><p>Zookeeper</p></li><li><ul><li>存储Kafka元数据</li><li>辅助选举Kafka的主节点, 抢注模式</li></ul></li></ul><h4 id="搭建部署"><a href="#搭建部署" class="headerlink" title="搭建部署"></a>搭建部署</h4><ul><li>解压安装</li><li>修改配置文件</li><li>分发文件</li><li>添加环境变量</li><li>启动和停止脚本</li></ul><h3 id="Topic管理"><a href="#Topic管理" class="headerlink" title="Topic管理"></a>Topic管理</h3><div class="code-wrapper"><pre><code class="hljs bash">bin/kafka-topics.sh --create --topic bigdata01 --partitions 3 --replication-factor 2 --bootstrap-server node1:9092,node2:9092,node3:9092    ■ --create创建    ■ --topic指定名称    ■ --partitions分区个数    ■ --replication-factor分区的副本个数    ■ --bootstrap-server指定Kafka服务端地址  ○ 列举--list  ○ 查看--describe  ○ 删除--delete</code></pre></div><h3 id="Kafka-API-应用"><a href="#Kafka-API-应用" class="headerlink" title="Kafka API 应用"></a>Kafka API 应用</h3><h4 id="大数据应用"><a href="#大数据应用" class="headerlink" title="大数据应用"></a>大数据应用</h4><ul><li><p>命令行</p></li><li><ul><li>一般只用于topic的管理：创建、删除</li></ul></li><li><p>生产者</p></li><li><ul><li>数据采集工具</li></ul></li><li><p>消费者</p></li><li><ul><li>实时采集工具</li></ul></li></ul><h4 id="生产者API"><a href="#生产者API" class="headerlink" title="生产者API"></a>生产者API</h4><ul><li><p>KafkaProducer生产者连接对象</p></li><li><p>send生产者用于发送数据到Kafka中的方法</p></li><li><ul><li>send（ProducerRecord）</li><li>ProducerRecord(Topic、Key、Value)</li><li>ProducerRecord(Topic、Value)</li><li>ProducerRecord(Topic、Partition、Key、Value)</li></ul></li><li><p>ProducerRecord发送的数据, 3中类型</p></li></ul><h4 id="消费者API"><a href="#消费者API" class="headerlink" title="消费者API"></a>消费者API</h4><ul><li><p>KafkaConsumer消费者连接对象</p></li><li><ul><li>subscribe()订阅Topic</li><li>poll()消费Topic</li></ul></li><li><p>ConsumerRecords返回多条数据集合</p></li><li><p>ConsumerRecord每一条数据</p></li><li><ul><li>.topic</li><li>.partition</li><li>.offset</li><li>.key</li><li>.value</li></ul></li></ul><h3 id="一次性语义"><a href="#一次性语义" class="headerlink" title="一次性语义"></a>一次性语义</h3><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><ul><li><p>at-most-once至多1次</p></li><li><ul><li>数据丢失</li></ul></li><li><p>at-least-once至少1次</p></li><li><ul><li>数据重复</li></ul></li><li><p>exactly-once有且只有1次</p></li><li><ul><li>只消费处理成功1次</li><li>消息队列目标</li></ul></li></ul><h4 id="Kafka如何实现一次性语言"><a href="#Kafka如何实现一次性语言" class="headerlink" title="Kafka如何实现一次性语言"></a>Kafka如何实现一次性语言</h4><ul><li><p>生产不丢失</p></li><li><ul><li>ACK机制+retries重试机制</li></ul></li><li><p>生产不重复</p></li><li><ul><li>问题根源: ACK丢失</li></ul></li></ul><p><img src="https://s2.loli.net/2023/02/26/nVlMfph23Lmd4b1.png"></p><ul><li><ul><li>幂等性机制f(x) = f(f(x))</li></ul></li><li><ul><li><ul><li>在每条数据中增加一个数据id，下一条数据会比上一条数据id多1</li><li>Kafka会根据id进行判断是否写入过了</li></ul></li></ul></li><li><p>消费不丢失不重复</p></li><li><ul><li>所有消费者按照offset顺序消费即可</li><li>保证任何场景下消费者都能知道上一次的Offset</li><li>实现</li></ul></li><li><ul><li><ul><li>Kafka将消费者的offset存储在__consumer-offsets</li><li>offset存储在一种可靠外部存储中，手动管理offset</li></ul></li></ul></li></ul><h3 id="面试知识"><a href="#面试知识" class="headerlink" title="面试知识"></a>面试知识</h3><h4 id="分区副本名词"><a href="#分区副本名词" class="headerlink" title="分区副本名词"></a>分区副本名词</h4><ul><li>AR所有副本</li><li>ISR可用副本</li><li>OSR不可用副本</li></ul><h4 id="数据同步名词"><a href="#数据同步名词" class="headerlink" title="数据同步名词"></a>数据同步名词</h4><p><img src="https://s2.loli.net/2023/02/26/lvxDSVOdqZjsk2f.png"></p><ul><li>HW当前这个分区所有副本同步的最低位置 + 1，消费者能消费到的最大位置</li><li>LEO当前每个副本已经写入数据的最新位置 + 1</li></ul><h4 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h4><ul><li><p>Controller选举</p></li><li><ul><li>Kafka主从节点选举</li><li>ZK辅助实现</li></ul></li><li><p>Leader选举</p></li><li><ul><li>分区副本的角色选举</li><li>Controller根据负载均衡选举</li></ul></li></ul><h4 id="数据限流"><a href="#数据限流" class="headerlink" title="数据限流"></a>数据限流</h4><ul><li><p>问题</p></li><li><ul><li>生产太快,消费跟不上</li><li>生产太慢, 消费速度太快</li></ul></li><li><p>限制生产</p></li><li><p>限制消费</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.11 Hadoop核心工作原理小记</title>
    <link href="/2023/02/19/vol11-Hadoop%E6%A0%B8%E5%BF%83%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%B0%8F%E8%AE%B0/"/>
    <url>/2023/02/19/vol11-Hadoop%E6%A0%B8%E5%BF%83%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%B0%8F%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/02/19/M26ChZ8XJWpxbNv.png">记录一下曾经学习 Hadoop 的笔记，温故知新，现在 Hadoop 已经到 3.x 版本，但是很多机制跟原理还是一致的。</p><h3 id="HDFS分布式文件系统"><a href="#HDFS分布式文件系统" class="headerlink" title="HDFS分布式文件系统"></a>HDFS分布式文件系统</h3><h4 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h4><p>1、硬件故障是常态</p><p>2、HDFS上的应用与一般的应用不同，它们主要是以流式读取数据,更注重数据访问的高吞吐量</p><p>3、典型的HDFS文件大小是GB到TB的级别</p><p>4、大部分HDFS应用对文件要求的是write-one-read-many访问模型,一次写入,多次读取</p><p>5、移动计算的代价比之移动数据的代价低</p><p>6、在异构的硬件和软件平台上的可移植性</p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>存储非常大的文件: 适合于存储大文件,需要 高吞吐量，对延时没有要求</p><p>一次写入、多次读取.一旦将存储进去后 不需要对数据进行修改, 后期只是查询场景</p><h4 id="三大机制和负载均衡"><a href="#三大机制和负载均衡" class="headerlink" title="三大机制和负载均衡"></a>三大机制和负载均衡</h4><h5 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h5><p>数据块的备份数,默认是3.</p><p>当某个数据块的个数不够3的时候,namenode会自动新增该数据块的备份, 当超过3块的时候,会自动删除多余的备份.</p><p>当不够3,又不能新增的时候,就会强制进入安全模式,只能读,不能写.</p><h5 id="心跳机制"><a href="#心跳机制" class="headerlink" title="心跳机制"></a>心跳机制</h5><p>datanode要定时(3秒)给namenode发送心跳包</p><ul><li>主要目的</li></ul><p>告诉namenode我还活着,如果没有定时发送,namenode就会认为我进入假死状态, 超过10分钟间隔就认为datanode宕机了</p><p>datanode定时(6个小时)向 namenode发送自己的块信息(元数据)</p><p>namenode的元数据是存储在内存中的,当集群启动的时候,datanode需要重新向namenode发送字节的快信息</p><h5 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h5><p>将第一个副本放置在某一个机架上, 剩下两个副本放到另一个机架的不同服务器上</p><p>在存储一个文件的某一个副本的时候, 根据机架感知原理以及网络拓扑关系(寻最近机架),</p><h5 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h5><p>namenode要保证各个datanode的数据块的个数和整体利用率保持一致</p><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><h5 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h5><p>1.namenode保存的所有的元数据信息, 都是存储在内存中.</p><p>2.整个HDFS可存储的文件数受限于NameNode的内存大小</p><p>注意: 实际环境中, namenode需要选择一台性能较高的服务器来当做namenode</p><p>3.namenode, 在保存元数据的时候, 会先将元数据保存到磁盘上, 然后在内存中也保存一份.用于保存元数据的磁盘文件, 主要有二个: fsImage文件 和 edis文件</p><p>fsImage文件, 是namenode元数据的镜像文件: 保存一份较为完整的元数据信息</p><p>edits文件: 是namenode在运行过程中, 会将元数据更新, 新增 操作全部写入到edits文件中</p><p>注意:在达到一定阈值后, snn 会进行对 edits文件和fsImage文件进行合并操作, 以保证在namenode edits文件不至于过大.</p><p>4.namenode不真实负责数据的读写操作, 但是需要经过namenode的询问之后 才可以读取datanode数据</p><h5 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h5><ol><li>NameNode是HDFS的核心。</li><li>NameNode也称为Master。</li><li>NameNode仅存储HDFS的元数据：文件系统中所有文件的目录树，并跟踪整个集群中的文件。</li><li>NameNode不存储实际数据或数据集。数据本身实际存储在DataNodes中。</li><li>NameNode知道HDFS中任何给定文件的块列表及其位置。使用此信息NameNode知道如何从块中构建文件。</li><li>NameNode并不持久化存储每个文件中各个块所在的DataNode的位置信息，这些信息会在系统启动时从数据节点重建。</li><li>NameNode对于HDFS至关重要，当NameNode关闭时，HDFS / Hadoop集群无法访问。</li><li>NameNode是Hadoop集群中的单点故障。(3.0版本之后可以解决)</li><li>NameNode所在机器通常会配置有大量内存（RAM）</li></ol><h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>1.datanode以数据块(block)来存储文件</p><p>2.datanode和namenode保持心跳的机制: 默认每隔3秒, 进行一次心跳, 如果10分钟内, 没有心跳, 认为, 宕机了</p><p>3.datanode每隔6个小时, 会向namenode报告一次完整的块信息</p><p>4.在真实集群中, 宕掉某一台datanode, 并不会整个集群的可用性, 因为namenode会自动的容错, 在其他的节点上构建缺失的block块</p><p>5.如果block块份数少于2分,又无法创建新的block快,HDFS会进入安全模式,只能读,不能写.</p><h4 id="SecondaryNode"><a href="#SecondaryNode" class="headerlink" title="SecondaryNode"></a>SecondaryNode</h4><h5 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h5><p>将edits文件和fsimage文件进行合并操作, 形成一个新的fsimage文件</p><h5 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h5><p>SNN所在的服务器的内存的容量 &gt;= NN 内存容量</p><p>SNN服务器一般和NN服务器部署在不同的中服务器中, 以保证在紧急情况下可以进行元数据的恢复工作</p><h5 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h5><ol><li>SNN在时刻监控的NN中edits的文件, 当这个文件达到一定的阈值(时间/大小)后,SNN会通知namenode, 进行关闭当前Edits文件,然后滚动一个新的Edits文件</li><li>SNN 会基于http请求, 从NN中将edits文件和fsimage文件拷贝到自己的SNN的服务器磁盘上</li><li>SNN 将 edits文件和 fsimage文件统一加载内存中, 进行合并操作, 形成一个完整的fsimage文件</li><li>将这个合并后的fsimage文件发送给NN, 替换掉原有Fsimage文件即可</li></ol><h4 id="HDFS读取数据流程"><a href="#HDFS读取数据流程" class="headerlink" title="HDFS读取数据流程"></a>HDFS读取数据流程</h4><p>1.向namenode发送请求, 连接namenode, 请求读取数据操作</p><p>2.namenode接收到请求后,     首先要判断客户端用户是否具有读取权限, 如果没有, 直接报错, 如果有权限接下来要判断在要写入的路径下是否有这个文件, 如果没有, 直接报错, 如果有, 根据机架感知原理以及网络拓扑关系 和 副本机制,返回给客户端部分或者全部的block对应的datanode的地址信息列表</p><p>3.客户端接收到地址列表后, 开始并行方式连接多个datanode的地址, 进行读取数据</p><p>4.如果上一次返回的部分的block列表, 当读取完成后, 再次请求namenode, 获取剩余的部分或者全部的block对应的datanode信息在接着执行第三步, 进行读取数据操作, 直到将所有的block全部读取完成</p><p>5.在client客户端, 进行对block排序, 然后将每个block拼接在一起, 形成了最终的文件</p><h4 id="HDFS写入数据流程"><a href="#HDFS写入数据流程" class="headerlink" title="HDFS写入数据流程"></a>HDFS写入数据流程</h4><p>1.客户端发送请求, 连接namenode, 请求写入数据操作</p><p>2.namenode接收到请求后, 首先要判断客户端用户是否具有写入权限, 如果没有, 直接报错, 如果有权限接下来要判断在要写入的路径下是否有这个文件, 如果有, 直接报错, 如果没有, 通知客户端可以写入</p><p>3.客户端接收到可以写入的信息后, 开始对文件进行切分, 形成多个block块</p><p>4.客户端请求namenode, 询问第一个block应该存储在那些datanode中</p><p>5.namenode 根据机架感知原理以及网络拓扑关系 和 副本机制, 来寻找到对应的datanode的地址列表, 然后将这些datanode的地址列表返回客户端</p><p>6.客户端从接收地址列表中, 拿出第一个地址与之连接, 形成一个pipeline的管道</p><p>7.当第一个连接成功后, 让第一个连接第二个地址, 然后第二个在连接第三个地址, 形成一个pipeline的管道</p><p>8.客户端开始进行写入数据: 数据以 package 数据包(64kb)的形式来发送数据, 当第一台接收数据后, 然后将请求发送给第二个datanode, 然后第二个接收到以后,然后在发送给第三个datanode</p><p>9.当第一个接收到数据后, 建立好一个应答响应队列, 当每个节点接收数据后, 都向应答队列反向报告, 最终所有都报告完成后, 将应答队列返回客户端</p><p>10.不断的进行发送, 不断的进行应答响应, 即可完成数据的发送, 当第一个block发送完成后, 重新请求namenode第二个block应该存储在那些datanode中, 此时在从流程第5步, 往下走即可 … 循环让所有的block写入成功即可</p><h4 id="HDFS读取数据流程-记忆版"><a href="#HDFS读取数据流程-记忆版" class="headerlink" title="HDFS读取数据流程(记忆版)"></a>HDFS读取数据流程(记忆版)</h4><p>1.发送请求</p><p>2.判断权限并返回地址</p><p>3.并行读取数据</p><p>4.再次请求,直至所有block完成</p><p>5.排序拼接</p><h4 id="HDFS写入数据流程-记忆版"><a href="#HDFS写入数据流程-记忆版" class="headerlink" title="HDFS写入数据流程(记忆版)"></a>HDFS写入数据流程(记忆版)</h4><p>1.发送请求</p><p>2.判断权限</p><p>3.切块</p><p>4.询问地址</p><p>5返回地址</p><p>6.建立管道</p><p>7.建立完整管道</p><p>8.开始传输数据</p><p>9.建立应答机制</p><p>10.完成一个block,重新开始请求地址,直至写入完成</p><h4 id="HDFS的shell命令"><a href="#HDFS的shell命令" class="headerlink" title="HDFS的shell命令"></a>HDFS的shell命令</h4><div class="code-wrapper"><pre><code class="hljs bash">lsr 递归查看目录结构mkdirput 上传文件操作get 下载数据getmerge 合并下载操作mvrm -rcpcatdu 统计目录下个文件大小chmod 修改权限appendToFile 追加数据操作copyFromLocalmoveFromLocalcopyToLocalmoveToLocal 未实现</code></pre></div><h4 id="API操作"><a href="#API操作" class="headerlink" title="API操作"></a>API操作</h4><p>操作思路</p><ol><li>创建客户端对象</li><li>执行具体操作</li><li>释放资源</li></ol><div class="code-wrapper"><pre><code class="hljs bash">1.获取FileSystem方式方式1:默认文件系统Configuration conf = new Configuration()方式2:指定文件系统,当前Windows用户conf2.set(<span class="hljs-string">&quot;fs.defaultFS&quot;</span>,<span class="hljs-string">&quot;hdfs://node1:8020&quot;</span>)方式3:指定文件系统.指定用户FileSystem filesystem03 = FileSystem.*get*(uri, conf3, <span class="hljs-string">&quot;root&quot;</span>);2.遍历HDFS中所有文件 filesystem.listFiles()3.创建文件夹fileSystem.mkdirs()4.下载文件copyToLocalFile()5.上传文件fileSystem.copyFromLocalFile()6.小文件合并filehdfs.create() filelocal.listFiles() filelocal.open()  IOUtils.*copy*(fis,fos) IOUtils.*closeQuietly*(fis)7.删除文件fileSystem.delete()8.权限控制修改vim hdfs-site.xml中的&lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;<span class="hljs-literal">true</span>&lt;/value&gt;</code></pre></div><h4 id="其它操作"><a href="#其它操作" class="headerlink" title="其它操作"></a>其它操作</h4><h5 id="HDFS安全模式"><a href="#HDFS安全模式" class="headerlink" title="HDFS安全模式"></a>HDFS安全模式</h5><ul><li>概念</li></ul><p>集群启动时,副本少于2又不能创建新的block快时</p><p>在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求</p><ul><li>命令</li></ul><p>查看安全模式:hdfs  dfsadmin safemode get</p><p>进入安全模式:hdfs  dfsadmin safemode  enter</p><p>离开安全模式 hdfs  dfsadmin safemode  leave</p><h5 id="HDFS基准测试"><a href="#HDFS基准测试" class="headerlink" title="HDFS基准测试"></a>HDFS基准测试</h5><p>测试写入速度</p><p>测试读取速度</p><h5 id="集群内数据复制-scp"><a href="#集群内数据复制-scp" class="headerlink" title="集群内数据复制 scp"></a>集群内数据复制 scp</h5><h5 id="跨集群文件拷贝distscp"><a href="#跨集群文件拷贝distscp" class="headerlink" title="跨集群文件拷贝distscp"></a>跨集群文件拷贝distscp</h5><h5 id="Archive档案"><a href="#Archive档案" class="headerlink" title="Archive档案"></a>Archive档案</h5><p>对小文件进行合并为大文件的操作, 减少hdfs中小文件的数量</p><h5 id="Snapshot快照：差异化快照"><a href="#Snapshot快照：差异化快照" class="headerlink" title="Snapshot快照：差异化快照"></a>Snapshot快照：差异化快照</h5><h5 id="Trash回收站"><a href="#Trash回收站" class="headerlink" title="Trash回收站"></a>Trash回收站</h5><p>类似Windows的回收站</p><h5 id="联邦机制"><a href="#联邦机制" class="headerlink" title="联邦机制"></a>联邦机制</h5><p>对datanode 实施 命名空间操作. 将datanode划分成多个命名空间(数据库), 然后每个命名空间交给一个namenode来管理</p><p>多个namenode, 共享整个datanode</p><h3 id="MapReduce分布式计算系统"><a href="#MapReduce分布式计算系统" class="headerlink" title="MapReduce分布式计算系统"></a>MapReduce分布式计算系统</h3><h4 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h4><ol><li>MapReduce思想:分而治之,Map负责分,Reduce负责合</li><li>MapReduce就是一个分布式计算的框架</li><li>我们只需要关心, 需要计算什么内容, 如何计算, 逻辑是什么, 至于最终如何运行, 如何拆解多个map和多个reduce,如何申请资源等等事情, 都与我们无关.</li><li>核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序</li><li>MapReduce处理的数据类型是&lt;key,value&gt;键值对</li></ol><h4 id="编程规范-核心八步"><a href="#编程规范-核心八步" class="headerlink" title="编程规范-核心八步"></a>编程规范-核心八步</h4><h5 id="map阶段"><a href="#map阶段" class="headerlink" title="map阶段"></a>map阶段</h5><p>1.读取数据, 将数据转换为 k1 和 v1<br>2.自定义map逻辑, 将k1 和 v1  转换为 k2 和 v2</p><h5 id="shuffle阶段-k2-和-v2-转换成-新-k2-和-v2"><a href="#shuffle阶段-k2-和-v2-转换成-新-k2-和-v2" class="headerlink" title="shuffle阶段:  k2 和 v2 转换成 新 k2 和 v2"></a>shuffle阶段:  k2 和 v2 转换成 新 k2 和 v2</h5><p>3.分区: 将相同的k2的数据发送给同一个reduce程序<br>4.排序: 根据k2的数据 进行排序操作 (默认按照字典的升序来排列)<br>5.规约: 是MapReduce的优化步骤, 可以省略<br>6.分组: 将相同的k2的value的数据进行合并为一个集合操作</p><h5 id="reduce阶段"><a href="#reduce阶段" class="headerlink" title="reduce阶段"></a>reduce阶段</h5><p>7.自定义 reduce的逻辑, 将经过shuffle 的k2 和 v2 进行转换为 k3 和 v3<br>8.输出操作: 将k3 和 v3 输出目的地即可</p><h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><p>如果要想实现一个非常简单的MapReduce程序, 只需要编写其中  二步操作(第二步 和 第七步)</p><h4 id="编程步骤"><a href="#编程步骤" class="headerlink" title="编程步骤"></a>编程步骤</h4><p>用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端)</p><h5 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h5><p>(1)  自定义类继承Mapper类<br>(2)  重写自定义类中的map方法，在该方法中将K1和V1转为K2和V2<br>(3)  将生成的K2和V2写入上下文中</p><h5 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h5><p>(1)  自定义类继承Reducer类<br>(2)  重写Reducer中的reduce方法，在该方法中将K2和[V2]转为K3和V3<br>(3)  将K3和V3写入上下文中</p><h5 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h5><p>整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象<br>（1）定义类，编写main方法<br>（2）在main方法中指定以下内容:<br>1、创建建一个job任务对象<br>2、指定job所在的jar包<br>3、指定源文件的读取方式类和源文件的读取路径<br>4、指定自定义的Mapper类和K2、V2类型<br>5、指定自定义分区类（如果有的话）<br>6、指定自定义分组类（如果有的话）<br>7、指定自定义的Reducer类和K3、V3的数据类型<br>8、指定输出方式类和结果输出路径<br>9、将job提交到yarn集群</p><h4 id="wordcount案例"><a href="#wordcount案例" class="headerlink" title="wordcount案例"></a>wordcount案例</h4><p>新建Maven工程,配置pom.xml<br>编写Map任务,继承Mapper类<br>shuffle阶段,用默认的<br>编写Reduce任务,继承Reducer类<br>编写Driver类,封装核心8步<br>本地运行和集群运行</p><h5 id="本地运行-直接运行即可"><a href="#本地运行-直接运行即可" class="headerlink" title="本地运行:直接运行即可"></a>本地运行:直接运行即可</h5><h5 id="集群运行"><a href="#集群运行" class="headerlink" title="集群运行"></a>集群运行</h5><p>1.配置pom.xml,打成胖jar上传HDFS<br>2.修改输入和输出路径为args[0] &amp; args[1]<br>3.设置jar的启动类 job.setJarByClass(WorkCountMain3.class)<br>4.启动命令格式 yarn  jar  jar包的路径  jar包的主入口类的全类名  [args …]</p><h4 id="Mapper任务流程详解"><a href="#Mapper任务流程详解" class="headerlink" title="Mapper任务流程详解"></a>Mapper任务流程详解</h4><p>特点: Hadoop是一个IO密集型框架, 里边有大量的IO的读写操作</p><h5 id="Map阶段总体流程"><a href="#Map阶段总体流程" class="headerlink" title="Map阶段总体流程:"></a>Map阶段总体流程:</h5><p>内存(环形缓冲区) –&gt; 磁盘(临时小文件) –&gt; 磁盘(合并后的最终MapTask结果文件)</p><h5 id="详细流程"><a href="#详细流程" class="headerlink" title="详细流程"></a>详细流程</h5><ol><li>假设HDFS系统中的某一个文件, 被切分成了 3个block块, 分别为: block-0, block-1, block-2.</li><li>此时就会有三个MapTask任务, 分别去读取这 3个block块 中的数据.</li></ol><p>即: TextInputFormat一行一行读取数据, 获取k1(LongWritable行偏移量) 和 v1(Text, 整行数据)</p><ol start="3"><li>三个MapTask任务要做的事儿都是一致的, 都是要把k1,v1 转成 k2, v2.</li></ol><p>在读取数据过程中, 读取一行就会调用一次 MapTask中map方法,<br>在map方法中, 是将接收到k1 和 v1 转换为 k2 和 v2 过程</p><ol start="4"><li>当map处理一条数据后, 就会往出写数据, 只要数据一写出去, 就会执行分区(partition)操作,分区操作的核心, 是对 这条数据进行 打分区标记 的过程</li></ol><p>分区可以保证: 相同key , 打上分区的编号必然是一样的<br>默认分区方案: hash 取模计算法 % numReduceTask</p><ol start="5"><li>当分好区以后, 数据就会被写入到 环形缓冲区中, 环形缓冲区本质上就是一个内存空间(数组), 大小为100M.</li></ol><p>MapTask一条一条的输出, 分区一条一条处理, 一条条的数据写入到环形缓冲区, 当这个环形缓冲区容量达到0.8系数,就会启动一个溢写的线程, 用来将80%的数据溢写到磁盘上, 在溢写过程中就会执行排序的工作<br>如果此时有规约, 此时就是执行规约的时候了.</p><ol start="6"><li>当MapTask执行完成后, 如果 环形缓冲区 依然还有一些剩余数据, 则一次性全部溢写到磁盘上, 此时在磁盘上就会有多个溢写出来的临时文件, 然后对这些临时文件进行 merge合并 操作, 形成一个最终的: 排好序, 分好区 规约好 的大文件.最终合并成一个大文件的时候, 依然会进行排序和规约操作.</li></ol><h5 id="问题-为什么环形缓冲区写到80-就要进行溢写呢-而不是写满在溢写"><a href="#问题-为什么环形缓冲区写到80-就要进行溢写呢-而不是写满在溢写" class="headerlink" title="问题: 为什么环形缓冲区写到80%, 就要进行溢写呢, 而不是写满在溢写?"></a>问题: 为什么环形缓冲区写到80%, 就要进行溢写呢, 而不是写满在溢写?</h5><p>假设写到100%, 这个时候再溢写, MapTask无法再写入数据了, 需要等待溢写完成<br>如果写到80%, 额外开启一个溢写线程, 负责将80%数据写出到磁盘, 主线程依然可以往内存中写入数据</p><h4 id="Reducer任务流程详解"><a href="#Reducer任务流程详解" class="headerlink" title="Reducer任务流程详解"></a>Reducer任务流程详解</h4><h5 id="Reduce阶段总体流程"><a href="#Reduce阶段总体流程" class="headerlink" title="Reduce阶段总体流程:"></a>Reduce阶段总体流程:</h5><p>磁盘 –&gt; 内存(copy线程) –&gt; 磁盘(临时小文件) –&gt; 磁盘(合并后的最终ReduceTask结果文件)</p><h5 id="详细流程-1"><a href="#详细流程-1" class="headerlink" title="详细流程"></a>详细流程</h5><ol><li>当reduceTask检测到mapTask全部都执行完成了, 开启copy(线程)的机制, 从多个mapTask中拷贝属于自己的分区的数据.</li></ol><p>每个ReduceTask都会创建Copy线程, 只拷贝属于自己的分区的数据.</p><ol start="2"><li>在copy过程中, 会先将数据写入到内存中, 当内存存储不下的时候, 在溢写到磁盘上, 形成临时文件</li><li>当copy结束后, 会将所有溢写出来的临时文件全部合并(merge)为一个大文件, 此时在合并的过程中,会对数据进行重新的排序工作, 如果reduce只有一个, 此时排序就是全局排序, 如果是多个, 依然是局部排序操作.</li><li>对排序好数据, 执行分组操作, 将相同key的value数据合并为一个集合,每分好一组, 调用一次reduceTask中reduce方法.</li><li>reduce方法执行完成后, 将结果数据直接输出即可.</li><li>输出组件就会将reduce的输出内容, 输出到目的地址上</li></ol><h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4><ul><li>概念</li></ul><p>相同类型的数据, 有共性的数据, 送到一起去处理</p><ul><li>作用</li></ul><p>指定分区, 会将同一个分区的数据发送到同一个Reduce当中进行处理。</p><ul><li>大白话理解</li></ul><p>分区就是map阶段中, 对map的数据进行打标记的过程<br>有几个分区,就有几个Reduce,结果就有几个文件,一般结合job.ReduceTasks()配合使用<br>彩票案例:自定义分区组件,继承Partitioner类,重写getPartition方法</p><h4 id="排序和序列化"><a href="#排序和序列化" class="headerlink" title="排序和序列化"></a>排序和序列化</h4><ul><li>序列化概念</li></ul><p>序列化（Serialization）是指把结构化对象转化为字节流。<br>反序列化（Deserialization）是序列化的逆过程。把字节流转为结构化对象。</p><ul><li>Writable接口:序列化和反序列化</li><li>子接口WritableComparable:序列化和排序</li></ul><p>重写compareTo方法<br>前减后升序, 后减前降序</p><ul><li>子类WritableComparator: 序列化和分组</li></ul><h4 id="规约"><a href="#规约" class="headerlink" title="规约"></a>规约</h4><ul><li>概念</li></ul><p>每一个 map 都可能会产生大量的本地输出，Combiner 的作用就是对 map 端的输出先做一次合并，以减少在 map 和 reduce 节点之间的数据传输量，以提高网络IO 性能，是 MapReduce 的一种优化手段之一<br>combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件<br>combiner 组件的父类就是 Reducer</p><ul><li>combiner 和 reducer 的区别:运行的位置不同</li></ul><p>combiner 是在每一个 maptask 所在的节点运行<br>Reducer 是接收全局所有 Mapper 的输出结果</p><ul><li>作用</li></ul><p>combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量</p><ul><li>如何实现自定义规约</li></ul><p>1.创建一个类, 继承Reudcer<br>2.重写reducer提供的reduce的方法. 在reduce的方法中, 实现局部聚合逻辑</p><ol start="3"><li>在MR的驱动类中, 将combinner的类添加到驱动类中job.setConbinnerClass()</li></ol><h4 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h4><ul><li>概念</li></ul><p>将相同的k2的value数据进行合并形成一个集合操作 , 在reduce中对同一个分区下的数据, 进行分组操作<br>在一个分区下, 可以有多个不同的key, 但同一个key只能在一个分区下</p><ul><li>分区和分组的区别</li></ul><p>分组: 将相同的k2的value数据进行合并形成一个集合操作 , 在reduce中对同一个分区下的数据, 进行分组操作<br>分区: 将相同的k2的数据, 发往同一个reduce中 , 在map端执行</p><ul><li>如何实现自定义分区</li></ul><p>1.创建一个类, 继承一个  WritableComparator<br>2.重写空参构造 在空参构造中指定, 当前k2的类型什么, 以及是否需要创建k2对象<br>public MyGroup()  { // 告知 分组组件, k2 是一个什么类型的, 已经需要将其k2对象创建出来<br>super(OrderBean.class,true);  }<br>3.重写其  compare方法: 方法中有两个 writableComparable 对象,本质上其实是两个k2的值<br>3.1: 强转为 k2的类型<br>3.2: 根据需求, 告知 MR比较k2什么字段</p><h4 id="并行机制"><a href="#并行机制" class="headerlink" title="并行机制"></a>并行机制</h4><h5 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h5><ul><li>概念</li></ul><p>MapTask的并行度指的是map阶段有多少个并行的task共同处理任务</p><ul><li>并行度</li></ul><p>一个MapReducejob的map阶段并行度由客户端在提交job时决定，客户端提交job之前会对待处理数据进行逻辑切片。</p><p>逻辑切片机制由FileInputFormat实现类的getSplits()方法完成。</p><h5 id="FileInputFormat切片机制-默认"><a href="#FileInputFormat切片机制-默认" class="headerlink" title="FileInputFormat切片机制 (默认)"></a>FileInputFormat切片机制 (默认)</h5><ol><li>切片大小，默认等于block大小，即128M</li><li>block是HDFS上物理上存储的存储的数据，切片是对数据逻辑上的划分</li><li>在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize))</li></ol><p>注意事项:逻辑切片</p><h5 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h5><ol><li>reducetask并行度同样影响整个job的执行并发度和执行效率,Reducetask数量的决定是可以直接手动设置的, 即:   job.setNumReduceTasks(4)</li><li>如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜。</li></ol><ul><li>并行度</li></ul><p>reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask</p><h4 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h4><h5 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h5><p>合并小文件采用CombineTextInputFormat来作为输入</p><h5 id="Map阶段"><a href="#Map阶段" class="headerlink" title="Map阶段"></a>Map阶段</h5><p>A. 减少溢写（spill）次数<br>B. 减少合并（merge）次数<br>C. 在map之后，不影响业务逻辑前提下，先进行combine处理，减少 I/O。</p><h5 id="Reduce阶段"><a href="#Reduce阶段" class="headerlink" title="Reduce阶段"></a>Reduce阶段</h5><p>A. 合理设置map和reduce数<br>B. 设置map、reduce共存<br>C. 规避使用reduce,通过将MapReduce参数setNumReduceTasks设置为0来创建一个只有map的作业。<br>D. 合理设置reduce端的buffer.</p><h5 id="Shuffle阶段"><a href="#Shuffle阶段" class="headerlink" title="Shuffle阶段"></a>Shuffle阶段</h5><p>Shuffle阶段的调优就是给Shuffle过程尽量多地提供内存空间，以防止出现内存溢出现象<br>可以由参数mapred.child.java.opts来设置，任务节点上的内存大小应尽量大。</p><h5 id="其它属性调优"><a href="#其它属性调优" class="headerlink" title="其它属性调优"></a>其它属性调优</h5><p>MapReduce还有一些基本的资源属性的配置，这些配置的相关参数都位mapred-default.xml</p><h3 id="YARN资源调度器"><a href="#YARN资源调度器" class="headerlink" title="YARN资源调度器"></a>YARN资源调度器</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>统一的资源管理调度平台</p><p>服务器的资源: CPU  内存 磁盘</p><h4 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h4><p><strong>ResourceManager: yarn集群中主节点, 可以部署多个, 一般部署为2个</strong></p><ol><li>接收客户端提交的任务请求</li><li>在nodemanger上为每一个任务启动 applicationMaster程序</li><li>负责资源的调度工作</li><li>负责整个集群管理</li></ol><p><strong>NodeManager: yarn集群中从节点, 可以部署多个, 一般部署的数量和datanode节点数相同</strong></p><p>负责执行的任务,基于容器(Container)的资源</p><p><strong>ApplicationMaster: 每一个任务都会有一个app Master ,负责任务分配</strong></p><ol><li>计算当前这个任务, 需要启动多少个map 和 多少个reduce</li><li>负责向resourceManager申请资源</li><li>通知给各个nodemanager启动运行程序, 并且持续的监控这个程序的运行的状态</li></ol><h4 id="Yarn运行流程"><a href="#Yarn运行流程" class="headerlink" title="Yarn运行流程"></a>Yarn运行流程</h4><ol><li>client向RM提交应用程序，其中包括启动该应用的ApplicationMaster的必须信息，例如ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。</li><li>ResourceManager启动一个container用于运行ApplicationMaster。</li><li>启动中的ApplicationMaster向ResourceManager注册自己，启动成功后与RM保持心跳。</li><li>ApplicationMaster向ResourceManager发送请求，申请相应数目的container。</li><li>ResourceManager返回ApplicationMaster的申请的containers信息。申请成功的container，由ApplicationMaster进行初始化。container的启动信息初始化后，AM与对应的NodeManager通信，要求NM启动container。AM与NM保持心跳，从而对NM上运行的任务进行监控和管理。</li><li>container运行期间，ApplicationMaster对container进行监控。container通过RPC协议向对应的AM汇报自己的进度和状态等信息。</li><li>应用运行期间，client直接与AM通信获取应用的状态、进度更新等信息。</li><li>应用运行结束后，ApplicationMaster向ResourceManager注销自己，并允许属于它的container被收回。</li></ol><h4 id="Yarn-调度器-Scheduler"><a href="#Yarn-调度器-Scheduler" class="headerlink" title="Yarn 调度器 Scheduler"></a>Yarn 调度器 Scheduler</h4><p>FIFO Scheduler : 先进先出的调度器(很少使用)</p><p>capacity scheduler : 容量调度器(Apache Hadoop默认)</p><p>Fair Scheduler : 公平调度器(CDH默认)</p><h4 id="Yarn常用参数"><a href="#Yarn常用参数" class="headerlink" title="Yarn常用参数"></a>Yarn常用参数</h4><p>设置container分配最小内存</p><p>设置container分配最大内存</p><p>设置每个container的最小虚拟内核个数</p><p>设置每个container的最大虚拟内核个数</p><p>设置NodeManager可以分配的内存大小</p><p>定义每台机器的内存使用大小</p><p>定义交换区空间可以使用的大小</p>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.10 Spark核心工作原理小记</title>
    <link href="/2023/02/12/vol10-Spark%E6%A0%B8%E5%BF%83%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%B0%8F%E8%AE%B0/"/>
    <url>/2023/02/12/vol10-Spark%E6%A0%B8%E5%BF%83%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%B0%8F%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/02/12/JhsFBVYuNkorAix.png"></p><p>整理学习 Spark 相关知识的笔记，查缺补漏。不得不说整理的时候重新捡起了很多遗忘的知识，Scala 我也很久很久没有写了, 现在公司用的是 Pyspark ，后面也整理记录下 Pyspark 的相关笔记。</p><h3 id="Spark-组件的数据抽象和上下文对象"><a href="#Spark-组件的数据抽象和上下文对象" class="headerlink" title="Spark 组件的数据抽象和上下文对象"></a><strong>Spark 组件的数据抽象和上下文对象</strong></h3><p><strong>SparkCore</strong></p><ul><li>数据抽象: RDD</li><li>上下文对象: SparkContext</li></ul><p><strong>SparkSQL</strong></p><ul><li>数据抽象: DataFrame DataSet</li><li>上下文对象 SparkSession</li></ul><p><strong>SparkStreaming</strong></p><ul><li>数据抽象:DStream</li><li>上下文对象: StreamingContext</li></ul><p><strong>StructuredStreaming</strong></p><ul><li>数据抽象 :DataFrame, DataSet</li><li>上下文对象: SparkSession</li></ul><h3 id="RDD、DataFrame和Dataset-三者的共同点跟区别"><a href="#RDD、DataFrame和Dataset-三者的共同点跟区别" class="headerlink" title="RDD、DataFrame和Dataset 三者的共同点跟区别"></a>RDD、DataFrame和Dataset 三者的共同点跟区别</h3><h4 id="三者的共同点"><a href="#三者的共同点" class="headerlink" title="三者的共同点"></a>三者的共同点</h4><ol><li>都是【分布式数据集】</li><li>转换操作都是【懒lazy】执行。</li><li>都支持【持久化】,都支持【12】种缓存策略</li><li>都支持【checkpoint】</li></ol><p><img src="https://s2.loli.net/2023/02/12/c6AzxOq3tQwGBuR.png" alt="img"></p><h4 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h4><ol><li>弹性分布式数据集</li></ol><ul><li><ul><li>弹性：他的存储和计算的节点可以【扩展】。数据可以存储在内存，如果【内存】不够，可以溢写到【磁盘】。</li><li>分布式：计算和存储都是分布式的</li><li>数据集：我们可以像使用scala本地集合一样使用RDD分布式数据集，可以很方便使用【函数式】编程。</li></ul></li></ul><ol><li>元素【不可】变，【可】分区，分区间【并】行计算。</li><li>元素的【泛型】要支持可序列化。反例如 Connection</li><li>缺点: 集群间的通信效率低，对象的序列化和反序列化开销大，会频繁的创建和销毁对象。</li></ol><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><ol><li>在RDD的基础上支持【schema】</li><li>DataFrame=RDD -【泛型】+方便的【SQL】操作+【DSL】操作+【catalyst】优化</li><li>将【数据】和【schema】分离存储，减小了序列化和反序列化的开销</li><li>支持catalyst，【RBO】基于规则的优化（【常量折叠】，【谓词下推】，【列裁剪】）和【CBO】（基于代价模型的优化）</li><li>缺点：DataFrame不能【类型安全检查】：比如dataframe.select(“不存在字段”)这句代码编译【通过】，但是运行时【报错】。</li></ol><p><img src="https://s2.loli.net/2023/02/12/M4QpcG23kdbClsw.png"></p><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><ol><li>在DataFrame的基础上支持【泛型】</li><li>Dataset=RDD[泛型]+schema+方便的SQL操作+catalyst优化+Tungsten</li><li>泛型分为【强】类型和【弱】类型，当类型是弱类型时DataFrame=Dataset[ Row].</li><li>自动的将样例类（jvm对象）的【属性】映射成分布式表的【字段】</li><li>Dataset具有【类型安全检查】：比如dataset.map(x=&gt;{x.不存在的属性}) ,编译时就报错。</li><li>直接继承了DataFrame的【catalyst】优化</li><li>独有【Tungsten】优化，支持特殊的编码器，基于内存管理的优化，使对象的存储更节省空间。</li></ol><p><img src="https://s2.loli.net/2023/02/12/lvU9q5Q8gGLScRE.png"></p><h3 id="Spark-跟-Hive-的关系"><a href="#Spark-跟-Hive-的关系" class="headerlink" title="Spark 跟 Hive 的关系"></a>Spark 跟 Hive 的关系</h3><p> <strong>Spark Thriftserver hiveserver2 metastore beeline 的关系</strong></p><p><img src="https://s2.loli.net/2023/02/12/Hwfc3kOP1od5Rel.png"></p><h3 id="Spark-SQL-catalyst-优化器"><a href="#Spark-SQL-catalyst-优化器" class="headerlink" title="Spark SQL catalyst 优化器"></a>Spark SQL catalyst 优化器</h3><p><img src="https://s2.loli.net/2023/02/12/KTw7JjcRzhavEYB.png"></p><ul><li><strong>RBO:基于规则优化</strong></li></ul><p>常量折叠：常量提前计算</p><p>谓词下推：在表关联前做过滤</p><p>列裁剪：只选择需要的列</p><ul><li><strong>CBO: 基于物理消耗的优化</strong></li></ul><h3 id="Spark-的两种运行模式"><a href="#Spark-的两种运行模式" class="headerlink" title="Spark 的两种运行模式"></a>Spark 的两种运行模式</h3><h4 id="Yarn-cluster-方式提交任务"><a href="#Yarn-cluster-方式提交任务" class="headerlink" title="Yarn-cluster 方式提交任务"></a>Yarn-cluster 方式提交任务</h4><p><img src="https://s2.loli.net/2023/02/12/kJHfVuOoAWedmn9.png"></p><h4 id="Yarn-client-方式提交任务"><a href="#Yarn-client-方式提交任务" class="headerlink" title="Yarn-client 方式提交任务"></a>Yarn-client 方式提交任务</h4><p><img src="https://s2.loli.net/2023/02/12/zJvfnq1FCY2MPW5.png"></p><h3 id="Spark-调度任务执行流程"><a href="#Spark-调度任务执行流程" class="headerlink" title="Spark 调度任务执行流程"></a>Spark 调度任务执行流程</h3><p><img src="https://s2.loli.net/2023/02/12/3pSP5QIRweFhbGy.png"></p><h4 id="准备执行"><a href="#准备执行" class="headerlink" title="准备执行"></a>准备执行</h4><ol><li>Yarn集群等等任务提交</li><li>Spark程序执行</li><li>客户端向RS申请启动APP</li><li>在NM启动APP,启动Driver进程,合为一体</li><li>APP申请资源</li><li>RS返回可用NM</li><li>APP连接NM并启动Executor</li><li>按照程序分配资源给Executor</li><li>Executor反向注册给Driver</li><li>Driver确定所有Executor准备完毕</li></ol><h4 id="分配任务"><a href="#分配任务" class="headerlink" title="分配任务"></a>分配任务</h4><ol><li>Spark程序SC创建2个调度器对象</li><li>SC识别Action算子DAG</li><li>DAGScheduler切分Stage,将Stage内的Task打包成TaskSet</li><li>DAGScheduler将一个Stage的TaskSet发给TaskSchuster</li><li>TaskScheduler将TaskSet中Task分发给不同Executor处理</li></ol><h4 id="执行任务返回结果"><a href="#执行任务返回结果" class="headerlink" title="执行任务返回结果"></a>执行任务返回结果</h4><ol><li>Executor将Task放到线程池执行</li><li>Executor将执行结果返回给TaskScheduler</li><li>一个StageTask完成后继续反馈给DAGScheduler</li><li>DAGScheduler继续将剩下的Stage发送给TaskScheduler,重复循环,直至完成</li></ol><h3 id="Spark-Steaming-原理"><a href="#Spark-Steaming-原理" class="headerlink" title="Spark Steaming 原理"></a>Spark Steaming 原理</h3><p><img src="https://s2.loli.net/2023/02/12/T4I6tEy7kNmi1Qg.png"></p><ol><li>StreamingContext对象封装了SparkContext对象</li><li>启动时同时启动Driver Executor</li><li>一个Executor启动Receiver</li><li>每到blockInterval时间间隔，就会生成一个数据块,实时同步备份到其它Executor</li><li>blockInterval默认是200ms，可以手动设置spark.streaming.blockInterval最低支持50ms</li><li>把数据块信息同步给StreamingContext对象, 攒够生成RDD给SparkContext对象</li><li>BatchInterval一般是blockInterval的整数倍，其中每个block是RDD的一个分区</li><li>SparkContext对象分发任务给Executor执行</li></ol><h3 id="Structured-Steaming-原理"><a href="#Structured-Steaming-原理" class="headerlink" title="Structured Steaming 原理"></a>Structured Steaming 原理</h3><h4 id="抽象概念"><a href="#抽象概念" class="headerlink" title="抽象概念"></a>抽象概念</h4><p><img src="https://s2.loli.net/2023/02/12/QH4zl9KWRNaeiT5.png"></p><ol><li>input table unbounded table无边界表,动态的增加数据,用DataFrame表示</li><li>data stream增量数据, 以行形式追加到unbounded table</li><li>query 查询逻辑,有追加更新立即触发查询</li><li>result table查询结果, 是一个DataFrame</li><li>output结果输出, append complete update三种模式</li></ol><h4 id="EventTime-事件时间"><a href="#EventTime-事件时间" class="headerlink" title="EventTime 事件时间"></a>EventTime 事件时间</h4><p><img src="https://s2.loli.net/2023/02/12/nV5TjkyH7BXW3b1.png"></p><p> <strong>时间的分类</strong></p><ul><li>EventTime 事件时间数据产生的时间</li><li>IngestionTime 注入时间数据到达流式系统的时间</li><li>ProcessingTime 处理时间数据被流式系统真正处理的时间</li></ul><p><strong>应用场景：基于事件时间的窗口聚合</strong></p><p><img src="https://s2.loli.net/2023/02/12/J1KU2ngYB45GXSM.png"></p><h4 id="watermark-水位"><a href="#watermark-水位" class="headerlink" title="watermark 水位"></a>watermark 水位</h4><p><strong>公式：Watermark = 【MaxEventTime 】 - 【Threshold】</strong></p><ul><li>MaxEventTime【上】一批次数据中的【最大的eventime值】</li><li>Threshold 预估事件的延迟时间上限</li></ul><p><img src="https://s2.loli.net/2023/02/12/Z18GVxJtjYwRA6C.png"></p><p><img src="https://s2.loli.net/2023/02/12/oJuqfAB3VtzpDmx.png"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.09 M1款 MacBookPro 搭建 JupyterLab 数据分析环境</title>
    <link href="/2023/02/05/Vol09-M1%E6%AC%BEMacBookPro%E6%90%AD%E5%BB%BAJupyterLab%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%8E%AF%E5%A2%83/"/>
    <url>/2023/02/05/Vol09-M1%E6%AC%BEMacBookPro%E6%90%AD%E5%BB%BAJupyterLab%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/02/05/YW5l74HBT6skpcf.png"></p><p>Python 用于数据分析的优势我就不多赘述，虽然当前基本不写 Python，但是我经常需要阅读 Python 代码，看别人写的数据处理逻辑，所以开始进一步学习 Pyspark 相关的知识。Jupyter 应该是学习 Python 数据分析最佳的工具了，趁着刚刚安装完，记录下自己环境配置跟常用的工具。</p><h3 id="miniconda-安装"><a href="#miniconda-安装" class="headerlink" title="miniconda 安装"></a>miniconda 安装</h3><p>提到 Python 数据分析大家一般推荐 Anaconda，miniconda 属于轻量级的包管理工具，我更推荐使用。在官网现在自己匹配的平台，下载安装即可，也可以选择 homebrew 安装。</p><p><a href="https://docs.conda.io/en/latest/miniconda.html">https://docs.conda.io/en/latest/miniconda.html</a></p><h3 id="Jupyter-Lab-安装"><a href="#Jupyter-Lab-安装" class="headerlink" title="Jupyter Lab  安装"></a>Jupyter Lab  安装</h3><p>miniconda 安装完成之后，测试 Python 能正常进入，开始 安装 Jupyter 及其拓展插件。</p><div class="code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 本体</span>pip install jupyterlab<span class="hljs-meta">#</span><span class="bash"> 格式化插件</span>pip install jupyterlab_code_formatter<span class="hljs-meta">#</span><span class="bash"> 格式化检测插件</span>pip install black isort<span class="hljs-meta">#</span><span class="bash"> 目录树功能</span>pip install jupyterlab-unfold<span class="hljs-meta">#</span><span class="bash"> 中文支持</span>pip install jupyterlab-language-pack-zh-CN<span class="hljs-meta">#</span><span class="bash"> GitHub 仓库支持</span>pip install jupyterlab-github</code></pre></div><p>也需要注意下修改登陆密码，跟设置后台启动。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 进入 Python 交互命令行</span><span class="hljs-keyword">from</span> notebook.auth <span class="hljs-keyword">import</span> passwd<span class="hljs-comment"># 设置密码，第二次输入密码之后会有一串秘钥，记下来后面配置文件需要用到</span>passwd()</code></pre></div><p>修改配置文件</p><div class="code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 终端命令行输入</span>jupyter lab --generate-config<span class="hljs-meta"></span><span class="hljs-meta">#</span><span class="bash"> Out</span> /Users/jface/.jupyter/jupyter_lab_config.py<span class="hljs-meta"></span><span class="hljs-meta">#</span><span class="bash"> 上面文件增加以下 3 行配置，秘钥填入上面设置密码获取到秘钥</span>c.ServerApp.allow_root = Truec.ExtensionApp.open_browser = Falsec.ServerApp.password = &#x27;秘钥&#x27;</code></pre></div><p>设置后台启动</p><div class="code-wrapper"><pre><code class="hljs shell">nohup jupyter lab --allow-root &gt; jupyterlab.log 2&gt;&amp;1 &amp;<span class="hljs-meta">#</span><span class="bash"> &amp; 让命令后台运行，并把标准输出 写入jupyterlab.log中。</span><span class="hljs-meta">#</span><span class="bash"> nohup 表示no hang up ，就是不挂起，这个命令执行后即使终端退出，jupyter 也不会停止运行。</span></code></pre></div><h3 id="Spark-安装"><a href="#Spark-安装" class="headerlink" title="Spark 安装"></a>Spark 安装</h3><p>Spark 直接使用 homebrew 安装。</p><div class="code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 安装 spark</span>brew install apache-spark<span class="hljs-meta">#</span><span class="bash"> 设置环境变量，如果是 zsh 编辑文件 .zshrc，写入安装路径</span>export SPARK_HOME=/opt/homebrew/Cellar/apache-spark/3.3.1export PYTHONPATH=$SPARK_HOME/libexec/python:$SPARK_HOME/libexec/python/build:$PYTHONPATH</code></pre></div><p>注意如果以下执行文件入口没有权限，需要给他们给到权限</p><div class="code-wrapper"><pre><code class="hljs shell">chmod 755 -R /opt/homebrew/Cellar/apache-spark/3.3.1/binchomd 755 -R /opt/homebrew/Cellar/apache-spark/3.3.1/libexec/bin</code></pre></div><h3 id="Pyspark-入门测试"><a href="#Pyspark-入门测试" class="headerlink" title="Pyspark 入门测试"></a>Pyspark 入门测试</h3><p>安装必要的 spark 相关第三方包。</p><div class="code-wrapper"><pre><code class="hljs shell">pip install findspark pip install pyspark</code></pre></div><p>实际代码测试</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 载入环境变量，注意 Spark 的执行环境路径，可能不同，下面两个都尝试下</span><span class="hljs-comment">#SPARK_HOME=&quot;/opt/homebrew/Cellar/apache-spark/3.3.1/bin&quot;</span>SPARK_HOME=<span class="hljs-string">&quot;/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/&quot;</span><span class="hljs-keyword">import</span> findsparkfindspark.init(SPARK_HOME)<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;ok&#x27;</span>)<span class="hljs-comment">#SparkSQL的许多功能封装在SparkSession的方法接口中</span><span class="hljs-keyword">import</span> pyspark <span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSessionspark = SparkSession.builder \    .master(<span class="hljs-string">&quot;local&quot;</span>) \    .appName(<span class="hljs-string">&quot;Word Count&quot;</span>) \    .config(<span class="hljs-string">&quot;spark.some.config.option&quot;</span>, <span class="hljs-string">&quot;some-value&quot;</span>) \    .getOrCreate()sc = spark.sparkContext<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;ok&#x27;</span>)<span class="hljs-comment">#将RDD转换成DataFrame</span>rdd = sc.parallelize([(<span class="hljs-string">&quot;LiLei&quot;</span>,<span class="hljs-number">15</span>,<span class="hljs-number">88</span>),(<span class="hljs-string">&quot;HanMeiMei&quot;</span>,<span class="hljs-number">16</span>,<span class="hljs-number">90</span>),(<span class="hljs-string">&quot;DaChui&quot;</span>,<span class="hljs-number">17</span>,<span class="hljs-number">60</span>)])df = rdd.toDF([<span class="hljs-string">&quot;name&quot;</span>,<span class="hljs-string">&quot;age&quot;</span>,<span class="hljs-string">&quot;score&quot;</span>])df.show()df.printSchema()<span class="hljs-comment">#Out 输出</span>+---------+---+-----+|     name|age|score|+---------+---+-----+|    LiLei| <span class="hljs-number">15</span>|   <span class="hljs-number">88</span>||HanMeiMei| <span class="hljs-number">16</span>|   <span class="hljs-number">90</span>||   DaChui| <span class="hljs-number">17</span>|   <span class="hljs-number">60</span>|+---------+---+-----+root |-- name: string (nullable = true) |-- age: long (nullable = true) |-- score: long (nullable = true)</code></pre></div><p>最终效果</p><p><img src="https://s2.loli.net/2023/02/05/MtQNHYAJPCFl2rO.png" alt="最终效果"></p><h3 id="数据分析推荐资料"><a href="#数据分析推荐资料" class="headerlink" title="数据分析推荐资料"></a>数据分析推荐资料</h3><p><a href="https://www.howie6879.cn/post/2021/26_jupyterlabv2_tutorial-/">https://www.howie6879.cn/post/2021/26_jupyterlabv2_tutorial-/</a></p><p><a href="https://www.jianshu.com/p/872648f7cc58">https://www.jianshu.com/p/872648f7cc58</a></p><p><a href="https://github.com/wesm/pydata-book">https://github.com/wesm/pydata-book</a></p><p><a href="https://github.com/iamseancheney/python_for_data_analysis_2nd_chinese_version">https://github.com/iamseancheney/python_for_data_analysis_2nd_chinese_version</a></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>周更挑战</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.08 人性之恶，实难揣测----读《连城诀》</title>
    <link href="/2023/01/29/Vol08-%E4%BA%BA%E6%80%A7%E4%B9%8B%E6%81%B6%E5%AE%9E%E9%9A%BE%E6%8F%A3%E6%B5%8B/"/>
    <url>/2023/01/29/Vol08-%E4%BA%BA%E6%80%A7%E4%B9%8B%E6%81%B6%E5%AE%9E%E9%9A%BE%E6%8F%A3%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/29/kdUmy6Pa5NFoquL.jpg"></p><p>《连城诀》被评价作金庸的十五部武侠小说中最具现实主义、批判主义的一部，里面写尽了人性的阴暗面，金庸先生这部小说里要探讨的就是人性的肮脏罪恶。我是第二遍看这部小说了，读到万震山半夜梦游砌墙的情节，还是觉得瘆得慌。我总觉得金庸先生是以最恶意的角度去塑造书中的人物的，对于书中的各个人物我都是以消极负面地角度去看待他的行为的。鲁迅曾说：</p><blockquote><p>我向来是不惮以最坏的恶意来推测中国人的。</p></blockquote><p>小说里的人物，很好的印证了鲁迅先生的话。</p><h3 id="师徒之间"><a href="#师徒之间" class="headerlink" title="师徒之间"></a>师徒之间</h3><p>梅念笙是万震山、戚长发言达平三人的师傅，梅念笙觉得三个徒弟心术不正，始终没把《连城剑谱》教给三人，没成想徒弟三人竟生歹意，联手要把梅念笙杀死，抢夺《连城剑谱》。为了一己私利，连守业恩师都要杀，行为举止，令人发指！对于梅念笙，教授徒弟武艺却总是藏着掖着，单教剑招却不教授内功心法，所传剑招也是掺杂着需要无用的花招，蓄意要将三个徒弟引上歪路。戚长发教授徒弟也是如此。戚长发原本外号“铁锁横江”，满腹才学，足智多谋，十分功于心计，却在自己徒弟和女儿面前，伪装成一个忠厚老实的乡下农人，在教授自己女儿和徒弟功夫时，故意将原本以唐诗为名的武功招数改得粗俗无比，故意教授狄云次等剑法，传授的剑法当中多数是无用的花招。临敌之时使一招不管用的剑法，那是虚耗了机会，让敌人强到上风，便是将生命都交到敌人手里。梅念笙如此教授徒弟，他们的徒弟戚长发、万震山也以同样的心思去教授自己的徒弟。人世间师徒的情分，荡然无存，有的都是一己私利，互相猜疑。为了剑谱，可以杀死自己师兄弟，可以杀死自己的师傅。</p><h3 id="父女之间"><a href="#父女之间" class="headerlink" title="父女之间"></a>父女之间</h3><p>丁典喜欢上了知府凌退思的女儿凌霜华，凌退思知道丁典身上有《连城剑谱》，是以亲身女儿为由，逼迫丁典交出《连城剑谱》。丁典不肯就范，惨遭凌退思诬陷，被囚在荆州大牢，甘愿每月惨遭一次毒打，长达五年之久。而凌退思为了《连城剑谱》，逼得自己女儿毁容，亲自将自己的亲身女儿活埋，凌霜华临死前用指甲在棺盖上刻下《连城剑谱》的数字，双手直挺挺地竖着。想象到这个画面，就让人气愤不已，世上竟然有如此狠心的父亲，为了《连城剑谱》，自己的亲生女儿都可以活埋。除了自己，好像其他人都是猪狗不如，都可以随意抹杀。</p><h3 id="侠义之间"><a href="#侠义之间" class="headerlink" title="侠义之间"></a>侠义之间</h3><p>花铁干原本是“落花流水”“南四奇”之一，江湖上赫赫有名的大侠客。在雪山谷误杀义弟刘乘风，因而心神大受激荡，平生英雄豪气霎时消失得无影无踪。沮丧、惊吓、受不了血刀老祖的心理战，终使这位武林大侠精神崩溃，跪地求饶，屈膝投降。在被困于雪谷的数月内，他把自己结义兄弟地尸体挖出来，剥皮当成御寒衣物，为了苟活，吃掉了自己结义兄弟的尸体，人性之恶全部显露。出谷之后，他为了遮掩丑行，故意编造谎言污蔑结义兄弟的女儿水笙与狄云有染，人性阴暗丑恶，显露无疑，他由一代大侠沦为自私自利的奸诈小人。当所谓的“侠义”与自身利益相冲突时，我们是否能够守住本心？人总是自私自利的，我们都不想死，面临死亡境地的时候，我们是否会为了活下去而做一个奸诈小人？</p><h3 id="夫妻之间"><a href="#夫妻之间" class="headerlink" title="夫妻之间"></a>夫妻之间</h3><p>被万震山的儿子万圭为了得到戚芳，故意陷害狄云入狱，随后便与戚芳结为夫妻，并生下来一个女儿。但是，后来得知戚芳手里拿着的《唐诗选辑》就是《连城剑谱》之后，夫妻之间的情谊完全烟消云散。不分青红皂白，就冤枉戚芳跟吴坎有染，口口声声将自己的相处多年的妻子骂作“淫妇”，要将其亲手杀死。在狄云将万圭父子丢入墙洞之中后，戚芳念着旧情救出万圭父子，却反遭他们所杀。为了一部《连城剑谱》自己的妻儿也可以抛弃，人性到底肮脏到什么程度，才可以杀妻抛子！</p><h3 id="砌墙的快感"><a href="#砌墙的快感" class="headerlink" title="砌墙的快感"></a>砌墙的快感</h3><p>其实书中最令我感到震惊与害怕的，是万震山。他杀死敌人之后，将敌人的尸体砌在自己房中的墙壁里，晚上梦游还在重复着砌墙的动作。</p><blockquote><p>“戚芳甚是奇怪，本已到了口边的一句“公公”又缩了回去，从窗缝中向房内张去。其时月光斜照，透过窗纸，映进房中，只见万震山仰卧在床，双手缓缓的向空中力推，双眼却紧紧闭着。</p></blockquote><blockquote><p>戚芳心道：“原来公公在练高深内功。练内功之时最忌受到外界惊扰，否则极易走火入魔。这时可不能叫他，等他练完了功夫再说。” 只见万震山双手空推一阵，缓缓坐起身来，伸腿下床，向前走了几步，蹲下身子，凌空便伸手去抓甚么物事。戚芳心想：“公公练的是擒拿手法。”又看得片时，但见万震山的手势越来越怪，双手不住在空中抓下甚么东西，随即整整齐齐的排在一起，倒似是将许多砖块安放堆叠一般，但月光下看得明白，地板上显是空无一物。</p></blockquote><blockquote><p>只见他凌空抓了一会，双手比了一比，似乎认为够大了，于是双手作势在地下捧起一件大物，向前塞了过去。戚芳看得迷惘不已，眼见万震山仍是双目紧闭，一举一动决不像是练功，倒似是个哑巴在做戏一般。”</p></blockquote><p>多么渗人的画面，把人杀死之后砌在墙中，晚上梦游还在重复着将敌人砌入墙中的步骤，心理得变态到什么程度，我们常说的变态杀人狂不过如此，在睡梦当中也在“享受”着这种杀人砌墙的快感。</p><p>常言道人不为己，天诛地灭，人都是有私欲的，这是本能。但是我们是拥有高等智慧的社会性动物，除了本能之外我们还有精神层面的“道德伦理”。但是在某些情形下，我们会因为本能而失去理智，冲破“道德伦理”的束缚。书中的《连城剑谱》就是一个利益事物的代表，书中的万震山、言达平、戚长发、凌退思等人，为了它不顾一切，将世间的道德伦理抛到一旁，连自己最亲的亲人儿女都可以欺骗抛弃。我们为了自身的利益，究竟可以罪恶到什么程度啊。我们经常说的，也许一两百万你不为所动，但是如果是像书中这样富可敌国的大宝藏呢。我们会不会也会被冲昏头脑，做出一些罪恶的事情？我觉得人都是有变态的心理的，我脑子里偶尔也会产生各种邪恶的念头，大都停留在“想”而已，因为心中有更为重要的“道德伦理”在约束着我。但是当诱惑到达一定的程度，我还能这样坚守本心，不为所动吗？我也不知道。</p><p><em>（本文首发于豆瓣：</em><a href="https://book.douban.com/review/9289083/"><em>人性之恶，实难揣测</em></a><em>）</em></p>]]></content>
    
    
    <categories>
      
      <category>阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
      <tag>读书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.07《流浪地球2》值得一看</title>
    <link href="/2023/01/23/Vol07-%E3%80%8A%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%832%E3%80%8B%E5%80%BC%E5%BE%97%E4%B8%80%E7%9C%8B/"/>
    <url>/2023/01/23/Vol07-%E3%80%8A%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%832%E3%80%8B%E5%80%BC%E5%BE%97%E4%B8%80%E7%9C%8B/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/23/uK2TIX9rFdbwHsC.webp" alt="流浪地球2电影海报"></p><p>今晚 8 点去电影院看 《流浪地球2》，看到机核电影场大家的评价，充满了期待。我对翻拍充满一种抵触情绪，即使拍得再好，可能都会破坏原著在我心目中的地位，因为文字的魅力在于你可以充分想象，一旦可视化之后可能会破坏心中美好的画面。因此，我喜欢《流浪地球》这样，基于原著背景重新创作的作品，既能有原著的魅力精髓，又能在此之上融入导演编剧自己的想法跟创意，可以相对不受束缚地讲述新的故事。</p><p><img src="https://s2.loli.net/2023/01/23/EI4zGNuqK9Ao8Te.png"></p><h3 id="是我喜欢的中国科幻的样子"><a href="#是我喜欢的中国科幻的样子" class="headerlink" title="是我喜欢的中国科幻的样子"></a>是我喜欢的中国科幻的样子</h3><p>看完已经 快 23 点，商场大部分店铺已经打烊，我在寻找出口的时候满脑子想着是最后结局的画面，人类的赞歌就是勇气的赞歌，我们是社会性动物，互助跟劳动是我们繁衍不息的基石，说句俗气的话，之所以我们能幸福地生活，是因为有人替我们负罪前行。</p><p>开头的“太空电梯”深深震撼了我，中国科幻对于未来科技的想法跟欧美完全是两条路子，不像好莱坞科幻电影喜欢打造超级英雄超级武器，我们对未来的科技畅想都基于现实，基于集体主义的努力，比如“太空电梯”，比如说 “星行星发动机”。开头灾难来临的压迫感跟无人机对战，是我目前为止看过的最喜欢的科幻对战画面，激烈震撼又富有美感，可能在巨物下更能体验人类的蝼蚁般的渺小。作为不太相关的从业者，电影里面的量子超级计算机脱离现实太远了，并且电影画面所有的编码解码控制界面，都太假了，我看到中文跟进度条以及打印的日志会有点出戏。</p><p>文戏来说，李雪健老师的扮演的周让我感觉非常非常亲戚，像是我记忆里的周总理，也像刚刚离世的江主席，果断又充满智慧。天下兴亡匹夫有责可能是刻在我们骨子里的文化基因，在人类共同的危难面前，我们更具有集体主义精神，更愿意为之牺牲跟风险，由于来刻画人物，更能让我有同感。最后大家自愿选择的画面，真的让我联系到疫情时期的支援各个地区的医生护士们，我更愿意称呼他们为医生跟护士，我想尊重他们的职业。</p><p>总的来说，画面特效跟情节都在线，压迫感很强，满足了我对中国式科幻的期望。不足之处就是刚刚说的人工智能部分，太让我跳戏了，希望下次能请一个专业的相关从业者来认真编排指导电影里面的各种代码界面。</p><p>通常我看电影会一直看完演员表直至屏幕关闭，我觉得这是对于电影工作者的尊重，不仅仅有台面上的人，更多人是幕后工作者。看到 《流浪地球2》的演员表，两个让我意外跟惊喜的地方：居然有木工、电工、铁匠参与制作，果然电影是一个复杂工程工业项目，需要不同工种职业共同参与；机核是深度内容合作媒体，机核牛逼！期待后续的相关节目内容，期待一波不仅仅杨老师能上节目，郭帆导演是不是也能上个节目！</p><h3 id="对于衍生产品的想法"><a href="#对于衍生产品的想法" class="headerlink" title="对于衍生产品的想法"></a>对于衍生产品的想法</h3><p>开头的空战跟 “太空电梯” 让我特别着迷，刚刚看到机核公众号发文说《流浪地球》将会出同名的 SLG 游戏，希望能做成我想象中那种画面跟玩法。对于 “太空电梯”，我想到是建造相关的游戏跟玩法，通过建造 “太空电梯” 应对危机，可以做成 RPG，也可能做成经营模拟，核心就是建造跟应对各种敌人的干扰，解决机器故障等等。无人机对战，我想的也是 RPG，也可以融入 FPS 作为一种玩法，控制无人机群占领高地，破坏设备，击退敌人，这个跟 《黑客帝国》的章鱼机械有异曲同工之妙，都是小型杀伤性机器人，数量庞大，防不胜防。</p>]]></content>
    
    
    <categories>
      
      <category>普通生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.06 工作中使用MySQL遇到的几个小坑</title>
    <link href="/2023/01/15/Vol06-%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BD%BF%E7%94%A8MySQL%E9%81%87%E5%88%B0%E7%9A%84%E5%87%A0%E4%B8%AA%E5%B0%8F%E5%9D%91/"/>
    <url>/2023/01/15/Vol06-%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BD%BF%E7%94%A8MySQL%E9%81%87%E5%88%B0%E7%9A%84%E5%87%A0%E4%B8%AA%E5%B0%8F%E5%9D%91/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/15/FgLoEhyf589UCZN.jpg"></p><p>MySQL 是工作当中经常使用到一个开源数据库，我当前工作主要使用 MySQL 作为报表存储数据库，以及承接数据提供给到下游业务使用。使用过程中遇到很多很多坑，都是小问题但是碰到了处理起来也是比较繁琐，特别记录一下。</p><h3 id="分区问题"><a href="#分区问题" class="headerlink" title="分区问题"></a>分区问题</h3><p>MySQL 数据库使用 InnoDB 引擎的时候是支持分区的，MySQL数据库的分区是局部分区索引，一个分区中既存了数据跟索引。聚集索引和非聚集索引都存放在分区当中。MySQL 分区分为 RANGE分区，LIST分区，HASH分区，KEY分区，生产环境一般使用RANGE分区，这个分区通常会跟 hive 数仓里面的分区字段一致，分区字段相当于索引，通常是 YYMMDD 这样的日期数据，</p><ul><li>如果表设置了索引，那么分区字段必须是索引</li><li>RANGE 分区日期字段，分区只能从小到大创建。这个在批量导入分区数据的时候需要特别特别注意，先创建分区再插入数据，其次是分区需要从小到大创建，我被这个坑了许多次。</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 创建分区表，假设分区字段是日期,to_days() 函数作用是返回参数日期跟年份 0 之间差的天数</span><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> table_name (user_id      <span class="hljs-type">INT</span>, nick_name    <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">50</span>), dayno        <span class="hljs-type">DATE</span>    ) ENGINE<span class="hljs-operator">=</span>InnoDB <span class="hljs-keyword">DEFAULT</span> CHARSET<span class="hljs-operator">=</span>utf8mb4 <span class="hljs-keyword">PARTITION</span> <span class="hljs-keyword">BY</span> <span class="hljs-keyword">RANGE</span>(to_days(dayno)) ( <span class="hljs-keyword">PARTITION</span> p20230111 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738897</span>), <span class="hljs-keyword">PARTITION</span> p20230112 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738898</span>), <span class="hljs-keyword">PARTITION</span> p20230113 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738899</span>), <span class="hljs-keyword">PARTITION</span> p20230114 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738900</span>), <span class="hljs-keyword">PARTITION</span> p20230115 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738901</span>) );<span class="hljs-comment">-- 新增分区</span><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> table_name <span class="hljs-keyword">add</span> <span class="hljs-keyword">PARTITION</span>( <span class="hljs-keyword">PARTITION</span> p20230116 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738902</span>) ENGINE <span class="hljs-operator">=</span> InnoDB,<span class="hljs-comment">-- 删除分区</span> <span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> table_name <span class="hljs-keyword">drop</span> <span class="hljs-keyword">PARTITION</span> p20230116 ;<span class="hljs-comment">-- 查询当前表分区</span><span class="hljs-keyword">show</span> <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span>_name;<span class="hljs-comment">-- 或者通过查询元数据</span><span class="hljs-keyword">select</span> partition_name,table_rows<span class="hljs-keyword">from</span> information_schema.partitions<span class="hljs-keyword">where</span> table_name <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;table_name&#x27;</span>;</code></pre></div><h3 id="大小写敏感问题"><a href="#大小写敏感问题" class="headerlink" title="大小写敏感问题"></a>大小写敏感问题</h3><p>MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写。MySQL 大小写敏感配置相关的两个参数，lower_case_file_system 和 lower_case_table_names。</p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 查询当前 MySQL 大小写敏感情况</span><span class="hljs-keyword">show</span> <span class="hljs-keyword">global</span> variables <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%lower_case%&#x27;</span>;<span class="hljs-operator">+</span><span class="hljs-comment">------------------------+-------+</span><span class="hljs-operator">|</span> Variable_name          <span class="hljs-operator">|</span> <span class="hljs-keyword">Value</span> <span class="hljs-operator">|</span><span class="hljs-operator">+</span><span class="hljs-comment">------------------------+-------+</span><span class="hljs-operator">|</span> lower_case_file_system <span class="hljs-operator">|</span> <span class="hljs-keyword">ON</span>    <span class="hljs-operator">|</span><span class="hljs-operator">|</span> lower_case_table_names <span class="hljs-operator">|</span> <span class="hljs-number">2</span>     <span class="hljs-operator">|</span><span class="hljs-operator">+</span><span class="hljs-comment">------------------------+-------+</span><span class="hljs-number">2</span> <span class="hljs-keyword">rows</span> <span class="hljs-keyword">in</span> <span class="hljs-keyword">set</span> (<span class="hljs-number">0.02</span> sec)<span class="hljs-comment">-- 如何修改</span>修改 MySQL 配置文件 my.cnf 当中的参数即可。</code></pre></div><ul><li>lower_case_file_system</li></ul><p>代表当前系统文件是否大小写敏感，只读参数，无法修改。ON 大小写不敏感，OFF 大小写敏感。</p><ul><li>lower_case_table_names，</li></ul><p>代表表名是否大小写敏感，可以修改，参数有0、1、2三种。</p><ul><li><ul><li>0 大小写敏感。</li><li>1 大小写不敏感。</li><li>2 大小写不敏感。跟 1 参数的区别是创建的库表名称保持原样保存在磁盘上但是执行 SQL 语句的时候会自动转换成小写。 1 参数创建的库表名称会转换成小写保存在磁盘上</li></ul></li></ul><h3 id="字符集问题"><a href="#字符集问题" class="headerlink" title="字符集问题"></a>字符集问题</h3><p>通常我们创建 MySQL 表的时候都会指定字符集为 utf8，这个通用于各种工具产品，并且对中文兼容性好。但是但是，但是这个字符集不支持 emoji 符号，我在处理爬虫跟日志数据的时候，就被坑了。字符集设置为 utf8mb4，能支持更多格式并且兼容 emoji 符号存储。</p><ul><li>MySQL 中字符集相关变量，设置直接 set 即可</li></ul><div class="code-wrapper"><pre><code class="hljs sql">character_set_client：客户端请求数据的字符集character_set_connection：从客户端接收到数据，然后传输的字符集character_set_database：默认数据库的字符集，character_set_results：结果集的字符集character_set_server：数据库服务器的默认字符集character_set_system：存储元数据的字符集,默认 utf8，不需要设置</code></pre></div><h3 id="数据类型问题"><a href="#数据类型问题" class="headerlink" title="数据类型问题"></a>数据类型问题</h3><p>因为不同的数据库会有不同的数据类型，从 hive 数仓导入到 MySQL 数据库，尤其需要注意设置匹配的字段类型，一方面是基于兼容性考虑保证数据正常导入且没有改变，另一方适合的字段类型也能节约成本。</p><ul><li>小数类型，财务精准数据，建议使用 DECIMAL 类型，指定步长避免精度损失。更建议的做法是在 hive 数仓上游就处理好步长。</li><li>字符串类型，建议使用可变字符串类型 VARCHAR 指定字符串长度，任何情况下都不建议使用其它的字符串类型。</li><li>日期类型，不包含时间使用 DATE ，包含时间使用 DATETIME，MySQL 库不建议将时间数据存储为字符串，会影响查询性能。</li><li>数值类型，注意不同的 INT 类型在只取整数无符号的情况能存储的最大数值范围，这个我同事经常被坑。</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 各种 INT 类型的取值范围，在只取整数的情况下</span>TINYINT: (<span class="hljs-number">0</span>，<span class="hljs-number">255</span>)<span class="hljs-type">INT</span>: (<span class="hljs-number">0</span>，<span class="hljs-number">4294967295</span>)<span class="hljs-type">BIGINT</span>: (<span class="hljs-number">0</span>，<span class="hljs-number">18446744073709551615</span>)</code></pre></div><h3 id="存储过程跟函数"><a href="#存储过程跟函数" class="headerlink" title="存储过程跟函数"></a>存储过程跟函数</h3><p>MySQL 支持自定义函数跟存储过程，通常来说存储过程会比函数功能更强大，比如有修改全局数据库状态的权限，可以返回参数，能做为独立程序执行，而函数通常作为查询语句的一部分来使用，有严格的限制。日常工作中我使用并没有经常使用到自定义函数，但是经常使用存储过程，主要用于检测 MySQL 分区状态，创建、清空、删除分区等操作，因为自定义程度很高，基本能和脚本语言编写的判断程序达到一样的效果。</p><ul><li>创建一个存储过程的格式</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span>    [DEFINER <span class="hljs-operator">=</span> &#123; <span class="hljs-keyword">user</span> <span class="hljs-operator">|</span> <span class="hljs-built_in">CURRENT_USER</span> &#125;]　<span class="hljs-keyword">PROCEDURE</span> sp_name ([proc_parameter[,...]])    [characteristic ...] routine_body proc_parameter:    [ <span class="hljs-keyword">IN</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">OUT</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">INOUT</span> ] param_name type characteristic:    COMMENT <span class="hljs-string">&#x27;string&#x27;</span>  <span class="hljs-operator">|</span> <span class="hljs-keyword">LANGUAGE</span> <span class="hljs-keyword">SQL</span>  <span class="hljs-operator">|</span> [<span class="hljs-keyword">NOT</span>] <span class="hljs-keyword">DETERMINISTIC</span>  <span class="hljs-operator">|</span> &#123; <span class="hljs-keyword">CONTAINS</span> <span class="hljs-keyword">SQL</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">NO</span> <span class="hljs-keyword">SQL</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">READS</span> <span class="hljs-keyword">SQL</span> DATA <span class="hljs-operator">|</span> <span class="hljs-keyword">MODIFIES</span> <span class="hljs-keyword">SQL</span> DATA &#125;  <span class="hljs-operator">|</span> <span class="hljs-keyword">SQL</span> SECURITY &#123; DEFINER <span class="hljs-operator">|</span> INVOKER &#125; routine_body:　　Valid <span class="hljs-keyword">SQL</span> routine statement [begin_label:] <span class="hljs-keyword">BEGIN</span>　　[statement_list]　　　　……<span class="hljs-keyword">END</span> [end_label]</code></pre></div><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.cnblogs.com/GrimMjx/p/10526821.html">https://www.cnblogs.com/GrimMjx/p/10526821.html</a></p><p><a href="https://www.jianshu.com/p/f2eabcef6577">https://www.jianshu.com/p/f2eabcef6577</a></p><p><a href="https://blog.csdn.net/qq_43563999/article/details/105820952">https://blog.csdn.net/qq_43563999/article/details/105820952</a></p><p><a href="https://www.runoob.com/w3cnote/mysql-stored-procedure.html">https://www.runoob.com/w3cnote/mysql-stored-procedure.html</a></p>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.05 坚持运动一年让我的生活充满力量</title>
    <link href="/2023/01/08/Vol05-%E5%9D%9A%E6%8C%81%E8%BF%90%E5%8A%A8%E4%B8%80%E5%B9%B4%E8%AE%A9%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB%E5%85%85%E6%BB%A1%E5%8A%9B%E9%87%8F/"/>
    <url>/2023/01/08/Vol05-%E5%9D%9A%E6%8C%81%E8%BF%90%E5%8A%A8%E4%B8%80%E5%B9%B4%E8%AE%A9%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB%E5%85%85%E6%BB%A1%E5%8A%9B%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/08/A67QsXtmySWvi9c.jpg"></p><p>本周超级猩猩出了 2022 年度运动报告，全年除了有一两个月因为疫情跟阳了门店停业，每周都坚持去超级猩猩上团课。全年锻炼 126 天，完成 161 次训练，上了 18 节早课，33 节晚课，上了 89 节 BC ，28 节 BJ 跟 23 节 RPM，全年上课时长 9410 分钟。感谢坚持努力的自己，过去一年平均 3 天就去上一次团课，我还是坚持了下来，爱上了运动，运动也改变了我，让我的生活充满力量。</p><img src="https://s2.loli.net/2023/01/08/g9jTq6cHrNSytAp.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/XLTWCKE9yPt2p1s.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/Oc5utQmihgfN4wM.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/6XlGrjChukMLHvZ.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/2Y3Mhkf7U8JP1nc.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/9Ze6LO7QgMlqY2i.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/BOfUXTZSjduJqH7.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/oJqiXVEF7N9euWH.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/mlvw6W4rBD8AG7d.jpg" alt="" style="zoom: 67%;" /><h3 id="坚持运动让我的生活充满力量"><a href="#坚持运动让我的生活充满力量" class="headerlink" title="坚持运动让我的生活充满力量"></a>坚持运动让我的生活充满力量</h3><h4 id="体能增强，身体协调性增强"><a href="#体能增强，身体协调性增强" class="headerlink" title="体能增强，身体协调性增强"></a>体能增强，身体协调性增强</h4><p>坚持运动这一年，我的体能明显增加，刚刚开始 BC 我第 5 小节之后基本干不动，都在摸鱼，现在轻轻松松可以 hold 住全程，燃脂保持在 600 千卡以上。身体协调性也更强了，跳踢、侧踢、波比跳都不在话下，撩起来腿就能踢出去，核心很稳。体能协调性提升的好处非常明显，免疫力增强，日常生活更有动力跟力量，这次阳了之后，我基本没有啥大的症状，3 天就恢复状态。体能增强之后还有一个好处：跟别人讨论沟通更有底气了，因为如果言语上说服不了，我可以尝试物理说服，没在怕的。</p><img src="https://s2.loli.net/2023/01/08/pz3yH5BLE2dcnDU.jpg" alt="7 天转阴记录" style="zoom:50%;" /><h4 id="更自信，更热情开朗"><a href="#更自信，更热情开朗" class="headerlink" title="更自信，更热情开朗"></a>更自信，更热情开朗</h4><p>身边朋友都说我开始运动之后，整个人精神面貌焕然一新。运动锻炼让我能够发泄情绪，把平时在工作跟生活中积攒的负能量发泄出来，多余的精力释放掉，让我能更好的专注于当前的生活。运动也让我的体态更优美，特别开始上 BJ 舞蹈课之后，身体协调性跟节奏变好了，能跟上各种斯比特。因为精神状态跟身体体态的变化，让我对自己的外貌更有自信，接人待物也更开朗，对于生活充满了热情。同时运动也改变了我穿搭的风格，霍比特人身高的我，平时都是有啥穿啥，开始锻炼之后开始走运动风格，白色运动 T 恤 + 运动短裤 + 运动训练鞋，百搭街头风，也特别能凸显个人积极向上的精神面貌。</p><img src="https://s2.loli.net/2023/01/08/DKecnCdmoGjgpSU.jpg" alt="我现在的穿搭" style="zoom: 50%;" /><h4 id="进入新圈子，认识新伙伴"><a href="#进入新圈子，认识新伙伴" class="headerlink" title="进入新圈子，认识新伙伴"></a>进入新圈子，认识新伙伴</h4><p>运动作为一种爱好，让我进入了一个全新的圈子。我喜欢超级猩猩原因之一就是团课不需要社交，上完即走无需交流，但是我还是认识了一群一起上课的 “好友”。我们一起上课、操房里眼神上鼓励交流、BC 的时候 PK 波比跳，但我确实不晓得他们的名字，也不晓得他们从哪里的，做什么工作的，我们仅仅碰巧经常一起上同一节课而已。我特别喜欢这样的社交方式，互相眼熟，有共同的爱好，也能互相鼓励，但是没有绑定社交关系，没有加微信留电话，弱性的社交关系让我非常放松没有社交压力，同时也能知道我们是一起坚持运动锻炼的 “ 好友 ”。</p><img src="https://s2.loli.net/2023/01/08/vXcT4EHZ1M2DKVI.png" alt="过去一年跟 1306 个猩友相遇" style="zoom:50%;" /><h3 id="新-1-年新的运动-flag"><a href="#新-1-年新的运动-flag" class="headerlink" title="新 1 年新的运动 flag"></a>新 1 年新的运动 flag</h3><p>2022 年运动收获满满，但是小遗憾是 BC 没能完成 100 节成就。新一年要继续保持运动锻炼，先立下新一年的运动 flag。运动锻炼是一生的事情，让运动成为日常生活的一部分。</p><ul><li><strong>完成 BC 100 节跟 BJ 100 节</strong></li><li><strong>体验更多类型课程，燃脂、塑性、HIIT、舞蹈、瑜伽</strong></li><li><strong>早课上满 30 节</strong> </li></ul>]]></content>
    
    
    <categories>
      
      <category>普通生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.04 Hive / Spark 如何避免单节点全局排序？</title>
    <link href="/2023/01/02/Vol04-Spark%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%8D%95%E8%8A%82%E7%82%B9%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F/"/>
    <url>/2023/01/02/Vol04-Spark%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%8D%95%E8%8A%82%E7%82%B9%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/02/D6hZklc1OntfFdJ.jpg"></p><p>最近因为经常对接模型算法，营销模型的一个应用场景是：按照模型打分取 TOPN 用户进行营销投放，由此就会产生一个全局排序的场景：<strong>在用户量过亿的情况下，单点全局排序极其容易出现 OOM。</strong>经历了几次线上事故之后，决心要彻底解决这个问题，跟同事请教了下，可以通过 <strong>“加盐打散”</strong> 来解决这个问题。</p><h3 id="加盐打散"><a href="#加盐打散" class="headerlink" title="加盐打散"></a>加盐打散</h3><p>产生全局排序的原因就是因为所有的 key 都需要互相比较才能产生全局排序序号，加盐打散的思路就是：<strong>通过对 key 加盐之后，拆分成不同的分组，用分组排序代替全局排序，实现分布式全局排序。</strong>就拿我日常接触最多的用户模型打分来说，基本思路如下：</p><div class="code-wrapper"><pre><code class="hljs plain">1、对用户加盐进行分组，并且获取不同组的先后顺序跟用户在组内的排序2、获取每个组的全局初始排序3、每个用户的全局排序 = 分组初始排序 + 在组内的排序</code></pre></div><h3 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h3><p>假设有表 model_score ，字段有 user_id 跟 score ，里面有 10 亿条数据，结构如下，要对表里所有的 user_id 按照 score 大小排序。</p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> `model_score`(   `user_id` <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">100</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,   `score` <span class="hljs-keyword">DOUBLE</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>)ENGINE<span class="hljs-operator">=</span>InnoDB <span class="hljs-keyword">DEFAULT</span> CHARSET<span class="hljs-operator">=</span>utf8;</code></pre></div><ul><li>对用户加盐进行分组，获取用户在组内的排序</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">with</span> t1 <span class="hljs-keyword">as</span> (<span class="hljs-keyword">select</span>   user_id  ,score  ,new_score  <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> new_score <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) <span class="hljs-keyword">as</span> rank1 <span class="hljs-comment">-- 获取用户分组内排序</span>  <span class="hljs-keyword">from</span> (  <span class="hljs-keyword">select</span> user_id,score,<span class="hljs-built_in">ceil</span>(score<span class="hljs-operator">*</span><span class="hljs-number">10000</span>) <span class="hljs-keyword">as</span> new_score  <span class="hljs-keyword">from</span> model_score  ) tmp),</code></pre></div><ul><li>获取每个分组全局初始排序</li></ul><div class="code-wrapper"><pre><code class="hljs sql">t2 <span class="hljs-keyword">as</span> (<span class="hljs-keyword">select</span> new_score,<span class="hljs-built_in">sum</span>(cnt) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> new_score <span class="hljs-keyword">desc</span> <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> unbounded preceding <span class="hljs-keyword">and</span> <span class="hljs-number">1</span> preceding ) <span class="hljs-keyword">as</span> rank2   <span class="hljs-comment">-- 从负无穷行到当前 -1 行的求和，获取到该分组用户最初的全局排序</span><span class="hljs-keyword">from</span> ( <span class="hljs-keyword">select</span> new_score,<span class="hljs-built_in">count</span>(<span class="hljs-number">1</span>) <span class="hljs-keyword">as</span> cnt<span class="hljs-keyword">from</span> t1 <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> new_score) tmp   ),</code></pre></div><ul><li>合并获取到用户的全局排序</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> t1.user_id,t1.score ,(t1.rank1 <span class="hljs-operator">+</span> t2.rank2) <span class="hljs-keyword">as</span> rank   <span class="hljs-comment">-- 每个用户的全局排序 = 分组初始排序 + 在组内的排序 </span><span class="hljs-keyword">from</span> t1 <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t2 <span class="hljs-keyword">on</span> t1.new_score <span class="hljs-operator">=</span> t2.new_score</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>Spark</tag>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.03 数据开发当中如何验证数据结果准确性</title>
    <link href="/2022/12/25/Vol03-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%BD%93%E4%B8%AD%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C%E5%87%86%E7%A1%AE%E6%80%A7/"/>
    <url>/2022/12/25/Vol03-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%BD%93%E4%B8%AD%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C%E5%87%86%E7%A1%AE%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/12/25/O7ZVahpqkvtN2Dw.jpg"></p><h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>数据开发日常工作经常需要跟业务方核验数据，校验数据源、业务逻辑是否准确。这里的数据准确性跟 ETL 中的“精确一次性语义” 保证数据不丢失不重复不一样，说的是数据报表或者用户标签特征是否符合既定业务逻辑。</p><p>以我浅薄的经验来说，验证数据准确性主要从：明细数据逻辑验证、业务逻辑验证、白盒测试这 3 个角度去做。</p><h3 id="通过明细数据从逻辑上做数据验证"><a href="#通过明细数据从逻辑上做数据验证" class="headerlink" title="通过明细数据从逻辑上做数据验证"></a>通过明细数据从逻辑上做数据验证</h3><p>通常一个数据报表逻辑都会相对复杂，多个表关联是一定会有的，因为各种业务逻辑的原因，指不定还有非常多的 case when 跟字段二次处理，逻辑上也会一层套一层，使用的明细层（DWS）最后结果是报表层（RPT）。自己开发的报表验证是否准确第一个想到的从逻辑明细上去做验证。</p><p>这个意思是，拿几条明细数据验证，看看这些用户是否按照既定的业务逻辑进入对应报表统计分类里面，特别需要验证代表性的用户，验证我们的 case when、二次处理是否生效等。</p><p>使用明细数据验证统计逻辑是否准确是我日常工作当中最常用也是最高效的一种方式。</p><h3 id="通过业务逻辑做数据验证"><a href="#通过业务逻辑做数据验证" class="headerlink" title="通过业务逻辑做数据验证"></a>通过业务逻辑做数据验证</h3><p>本身报表就是根据业务需求创建的，那么是否准确也应该由业务方来验证，我们自行验证的话就是通过判断报表指标是否符合基本的业务逻辑。这里说的业务逻辑主要只的报表指标数值是否符合业务方预计范围之内，是否符合普遍的规律跟比例。</p><p>比如说，营销投放当中触达率不可能超过 100 %、获客成本应该在一个大致范围之内，应该跟投放平台反馈的成本接近，如果我们报表计算结果指标跟这些数值相差较大，那么就应该从头开始验证，可能是哪一个环节计算错误，或者存在脏数据。</p><h3 id="通过白盒测试验证数据"><a href="#通过白盒测试验证数据" class="headerlink" title="通过白盒测试验证数据"></a>通过白盒测试验证数据</h3><p>白盒测试是指边看着程序代码逻辑，一遍操作系统查看数据变化情况。在我们数据开发角度，其实就是亲自验证下用户是否按照我们既定的逻辑被统计到对应的指标当中。</p><p>拿个埋点数据举例，就是看看拿实机测试，看看上报数据是否正常，对应操作是否上报了正确的埋点标识并且正常被报表通缉到对应指标当中。</p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.02 推荐下今天发现的几个cheatsheet</title>
    <link href="/2022/12/18/Vol02-%E6%8E%A8%E8%8D%90%E4%B8%8B%E4%BB%8A%E5%A4%A9%E5%8F%91%E7%8E%B0%E7%9A%84%E5%87%A0%E4%B8%AAcheatsheet/"/>
    <url>/2022/12/18/Vol02-%E6%8E%A8%E8%8D%90%E4%B8%8B%E4%BB%8A%E5%A4%A9%E5%8F%91%E7%8E%B0%E7%9A%84%E5%87%A0%E4%B8%AAcheatsheet/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/12/18/h13sx5OYpqvIDFb.png" alt="vim cheat sheet"></p><p>今天发现了一个神奇的东西，名字叫 Cheat Sheet，就是各种语言工具的快捷键列表，这个对于我这样记不住各种东西的菜鸟帮助太大了，平时边用边记。</p><p>老年跟菜鸟的区别可能就是你对各种工具快捷键的熟悉程度。记录下常用的几个，纳入自己工作流当中。</p><ul><li><strong>Python 语言（有中文且也有其他工具语言的）</strong></li></ul><p><a href="https://cheatography.com/simpleapples/cheat-sheets/python-3/">https://cheatography.com/simpleapples/cheat-sheets/python-3/</a></p><ul><li><strong>VIM （也有其他语言工具）</strong></li></ul><p><a href="https://quickref.me/vim">https://quickref.me/vim</a></p><ul><li><strong>另一个 VIM</strong> </li></ul><p><a href="https://vim.rtorr.com/lang/zh_cn">https://vim.rtorr.com/lang/zh_cn</a></p><ul><li><strong>正则表达</strong></li></ul><p><a href="https://ihateregex.io/">https://ihateregex.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Linux</tag>
      
      <tag>周更挑战</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vol.01 什么是新怪谈—从《鼠王》开始说</title>
    <link href="/2022/12/11/Vol01-%E4%BB%80%E4%B9%88%E6%98%AF%E6%96%B0%E6%80%AA%E8%B0%88%E2%80%94%E4%BB%8E%E3%80%8A%E9%BC%A0%E7%8E%8B%E3%80%8B%E5%BC%80%E5%A7%8B%E8%AF%B4/"/>
    <url>/2022/12/11/Vol01-%E4%BB%80%E4%B9%88%E6%98%AF%E6%96%B0%E6%80%AA%E8%B0%88%E2%80%94%E4%BB%8E%E3%80%8A%E9%BC%A0%E7%8E%8B%E3%80%8B%E5%BC%80%E5%A7%8B%E8%AF%B4/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/12/17/Nsp3iElyHnRMoID.jpg" alt="鼠王封面"></p><p>频繁在机核的播客里被安利《鼠王》，忘记哪一期节目，介绍了《美国众神》的作者尼尔盖曼作品，就提到通俗奇幻小说的发展史。目前的这类奇幻类小说主要分三类：旧时代民间传说、克苏鲁世界、新怪谈，当然还有史蒂芬金自己独特的体系，不在讨论范围之内。当时就很好奇，为啥新怪谈叫新怪谈，新在哪里。最近机核上了鼠王有声书版本，听音频书容易分神，趁着热度火速去读了鼠王。 </p><h4 id="“怪”是理所当然"><a href="#“怪”是理所当然" class="headerlink" title="“怪”是理所当然"></a><strong>“怪”是理所当然</strong></h4><p>感觉新怪谈第一个特点就是“怪谈”不怪，或者说故事里的人物不会感觉奇怪，理所当然的接受了各种“超自然事物”。就鼠王来说，鼠王登场的一系列操作，惊艳冷酷，但邵尔立刻就接受了这个事实，其它的登场人物同样如此。不会去思考鼠王如何出现，也不会想着上报国家，上报科学研究，大家似乎全都接受了鼠王出现的事实，而且不在于他为何出现，又有何价值，对人类发展有什么影响，只当成一个新事物看待。 </p><h4 id="融入世界"><a href="#融入世界" class="headerlink" title="融入世界"></a><strong>融入世界</strong></h4><p>第二个特殊之处在于，鼠王鸟王蛛王都生活在都市之中，融入世界。他们是都市的部分，他们不是躲在深山老林，他们和都市中其它物种一样，有自己的住宿，有自己的生活，而且是随着时代一起发展的，他们完美融入了“正常人”的世界之中。老鼠鸟蜘蛛都和人类一起生活着。 </p><h4 id="故事不一定是主角"><a href="#故事不一定是主角" class="headerlink" title="故事不一定是主角"></a><strong>故事不一定是主角</strong></h4><p>第三个是我感觉，新怪谈故事情节不一定是第一位的。比说并没有介绍鼠王等人从何而来，也没有说到最后结局会如何，更没有说舞会现场如何收场。很多正常小说应该去解释和说明的地方鼠王都没有解释。其次是承载的文化元素。鼠王这本书融入的伦敦底下音乐文化我是一无所知的，只能依靠文字描写和歌曲名称去想象那是一种什么的的音乐，鼠王融入的是这么一种元素，其它的新怪谈小说比如《遗落南境》又是容易了SCP基金会的元素？尼尔盖曼的《美国众神》又是另一种文化。可以说新怪谈小说都会惨杂着其它元素的内容。从整个作品来说，故事情节不一定是最重要的，故事也不一定完整，但是从故事中渗透出来的文化符号以及表达的想法欲望一定是第一位的。 </p><p>最近的阅读口味很多都是被机核种草的。读完鼠王特别想想玩《极乐迪斯科》，这是一种新的文化体系，曾经被种草克苏鲁，现在又是新怪谈，感谢机核。</p><p><em>（本文首发于豆瓣：</em><a href="https://book.douban.com/review/13384309/"><em>什么是新怪谈—从《鼠王》开始说</em></a><em>）</em></p>]]></content>
    
    
    <categories>
      
      <category>阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
      <tag>读书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高效年轻人的7个办公习惯</title>
    <link href="/2022/04/10/%E9%AB%98%E6%95%88%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%847%E4%B8%AA%E5%8A%9E%E5%85%AC%E4%B9%A0%E6%83%AF/"/>
    <url>/2022/04/10/%E9%AB%98%E6%95%88%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%847%E4%B8%AA%E5%8A%9E%E5%85%AC%E4%B9%A0%E6%83%AF/</url>
    
    <content type="html"><![CDATA[<p>偶然在即刻上看到飞书的 ZARA 分享了这篇小文章，高效年轻人的 7 个办公习惯，都是我目前所欠缺和需要学习的，至少我现在<br>‘elevator list’ 就没有做到，经常是口头表达，而不是先准备好问题和资料。</p><ol><li>如果 leader 布置了一个任务，做到30%的时候先问老板方向对不对，不要憋大招。定期主动跟需求方同步进展，不要等到被问/被催。周报上写“进展”而不是“动作”。“跟xx开会讨论xx事项”不算进展，讨论出来的结果才算进展。</li><li>意识到 leader 的时问非常宝贵，准备 “elevator list”，这是一份文档/笔记，在工作中如果有想跟leader请教/商量/同步的事情，随时记录进去。最终效果是，如果你突然在电梯遇到 leader,或者你发现跟老板在同一辆出租车里去见客户，就可以马上拿出这个 list，利用这段时间去探讨你日常记录的这些问题。珍惜一切跟leader独处的时间，把它变成学习请教的机会。</li><li>主动跟 leader 约定期的 1:1，并每次提前准备要讨论的话题（建议准备文档）</li><li>如果被拉到一个会上，主动记录会议纪要和todo， 并且会后发到会议群里</li><li>珍惜自己的时间，永远去想怎么提升效率。不要觉得因为自己年轻，自己的时间就是便宜的。</li><li>不要闷头干活，不要单打独斗，主动跟跨部门同事约饭、约咖啡，不要社恐，学会协作。</li><li>如果 leader 布置了一个任务，你不理解这件事情的意义，那就主动问“为什么”。如果leader说“别问问什么，做就是了”，建议尽快换工作。</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>项目管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQLBoy日常工作技巧</title>
    <link href="/2022/02/09/SQLBoy%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    <url>/2022/02/09/SQLBoy%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
    
    <content type="html"><![CDATA[<p>入职新工作三周了，虽然还处理 SQLBoy 阶段，但是学习到了非常多小技巧，有必要记录一下，持续更新，避免遗忘。</p><ul><li>规范需求记录，脚本备份，文档归类，代码片段，数字字典</li><li>封装公共参数和大数据脚本执行参数到脚本当中，执行脚本只需要引入变量</li><li>每一种 SQL 脚本方式封装一个方法，固定脚本执行格式</li><li>SQL 中空值和 null 同时过滤：字段 &gt; ‘’</li><li>历史数据回溯，通过封装脚本传参调用执行脚本进行</li><li>任务统一规范命令，数据丢失任务失败方便排除定位</li><li>等值判断注意非空校验和异常数据处理</li><li>等值判断先确定数值格式匹配，大小匹配，格式兼容</li><li>需求闭环，全链路自动化分析</li><li>代码自查，逻辑明确，自我溯源</li><li>明确需求，避免重复遗忘丢失误判</li><li>先理清思路，确认细节，规划执行路径和测试路径，再具体操作</li><li>整理小知识，定期存档回顾</li><li>验证数据准确性，包括：字段类型、数量、topN、趋势等</li><li>跨部门沟通，主动提供上下文，把相关人当做小白，从最基础的理念给合作部门介绍</li></ul>]]></content>
    
    
    <categories>
      
      <category>工作记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>SQL</tag>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开发环境准备</title>
    <link href="/2022/01/20/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
    <url>/2022/01/20/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<p>最近换了 M1 MBP，新电脑开发环境需要从头部署，汇总一下我开发环境常用的工具吧。</p><h3 id="环境支持"><a href="#环境支持" class="headerlink" title="环境支持"></a>环境支持</h3><ul><li>资源下载</li></ul><p><a href="https://repo.huaweicloud.com/java/jdk/#/">下载 JDK 1.8</a></p><p><a href="https://www.scala-lang.org/download/2.11.12.html#/">下载 Scala 2.11.12</a></p><ul><li>安装说明</li></ul><p><a href="https://www.jianshu.com/p/717a5b92e01a#/">win10下jdk1.8安装和环境变量的配置</a></p><p><a href="https://www.cnblogs.com/mawangwang/p/12250018.html#/">scala安装教程及简单配置</a></p><h3 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h3><ul><li>IDEA</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 插件</span>Cosy Java CodingAtom Material IconsPDF ViewerRainbow BracketsScalaIDE Eval Reset</code></pre></div><ul><li>VScode</li><li>Git </li><li>Maven </li><li>Notepad++</li><li>Navicat</li><li>CRT</li><li>MySQL</li></ul><h3 id="办公工具"><a href="#办公工具" class="headerlink" title="办公工具"></a>办公工具</h3><ul><li>Typora</li><li>幕布</li><li>Xmind</li><li>Snipaste</li><li>uTools</li><li>WPS</li><li>Everything</li><li>火绒</li><li>任务栏透明</li><li>chrome 浏览器</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 插件</span>简悦油猴插件<span class="hljs-operator">-</span>CSDN广告屏蔽</code></pre></div><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p><a href="https://wallhaven.cc/#/">壁纸网站</a></p><p><a href="https://pan.baidu.com/s/1TMU388jytBRRx4y8XCNkAw">百度网盘</a></p><p>提取码: 2n0f </p><p><a href="https://developer.aliyun.com/article/681235#/">国内开源镜像站点</a></p><p><a href="https://developer.aliyun.com/mirror/maven#/">阿里 Maven 镜像</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL中的行转列和列转行</title>
    <link href="/2021/06/22/SQL%E4%B8%AD%E7%9A%84%E8%A1%8C%E8%BD%AC%E5%88%97%E5%92%8C%E5%88%97%E8%BD%AC%E8%A1%8C/"/>
    <url>/2021/06/22/SQL%E4%B8%AD%E7%9A%84%E8%A1%8C%E8%BD%AC%E5%88%97%E5%92%8C%E5%88%97%E8%BD%AC%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<h3 id="MySQL-的行转列"><a href="#MySQL-的行转列" class="headerlink" title="MySQL 的行转列"></a>MySQL 的行转列</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> <span class="hljs-operator">+</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-operator">+</span> max<span class="hljs-operator">/</span>sum 函数</code></pre></div><h3 id="MySQL-的列转行"><a href="#MySQL-的列转行" class="headerlink" title="MySQL 的列转行"></a>MySQL 的列转行</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 指定语句 <span class="hljs-operator">+</span> <span class="hljs-keyword">union</span> 拼接即可<span class="hljs-keyword">union</span> 去重<span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span> 不去重FLink 中 <span class="hljs-keyword">union</span> 不去重，相当于 <span class="hljs-keyword">SQL</span>中的 <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span></code></pre></div><h3 id="Hive-行转列"><a href="#Hive-行转列" class="headerlink" title="Hive 行转列"></a>Hive 行转列</h3><div class="code-wrapper"><pre><code class="hljs sql"># 基本思路：列拼接输出# 涉及函数concat(str1,str2,...) # 字段或字符串拼接concat_ws(sep, str1,str2) # 以分隔符拼接每个字符串collect_set(col) #将某字段的值进行去重汇总，产生<span class="hljs-keyword">array</span>类型字段# 代码实现<span class="hljs-keyword">select</span> collect_set( concat_ws(<span class="hljs-string">&#x27;:&#x27;</span>,s_id,s_name,s_sex) ) <span class="hljs-keyword">from</span> student;</code></pre></div><h3 id="Hive-列转行"><a href="#Hive-列转行" class="headerlink" title="Hive 列转行"></a>Hive 列转行</h3><div class="code-wrapper"><pre><code class="hljs sql"># 涉及函数# explode函数# explode(col) 将hive一列中复杂的<span class="hljs-keyword">array</span>或者map结构拆分成多行# <span class="hljs-keyword">lateral</span> <span class="hljs-keyword">view</span> 侧视图# 用于和split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。# 代码实现<span class="hljs-keyword">select</span> deptno,name <span class="hljs-keyword">from</span> emp <span class="hljs-keyword">lateral</span> <span class="hljs-keyword">view</span> explode(names) tmp_tb <span class="hljs-keyword">as</span> name;</code></pre></div><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>mysql 行转列和列转行 </p><p><a href="https://www.jianshu.com/p/0e6113241979">https://www.jianshu.com/p/0e6113241979</a> </p><p>hive 行转列和列转行</p><p><a href="https://www.jianshu.com/p/26d85daef92c">https://www.jianshu.com/p/26d85daef92c</a> </p><p>Hive笔记之collect_list/collect_set（列转行）</p><p><a href="https://www.cnblogs.com/cc11001100/p/9043946.html">https://www.cnblogs.com/cc11001100/p/9043946.html</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Awk和Shell</title>
    <link href="/2021/06/10/Awk%E5%92%8CShell/"/>
    <url>/2021/06/10/Awk%E5%92%8CShell/</url>
    
    <content type="html"><![CDATA[<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><h3 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h3><ul><li><code>awk [选项参数] &#39;script&#39; var=value file(s)</code></li><li>基本语法<ul><li>$0 代表整个文本行</li><li>$1 代表文本行中的第 1 个数据字段</li><li>printf 打印输出</li><li>默认每行按空格或TAB分割，使用$n来获取段号</li></ul></li></ul><h3 id="段连接符OFS"><a href="#段连接符OFS" class="headerlink" title="段连接符OFS"></a>段连接符OFS</h3><ul><li><code>awk &#39;&#123;OFS=&quot;#&quot;&#125;&#123;print $1,$2,$3&#125;&#39; test_awk.txt</code></li></ul><h3 id="指定分隔符-F"><a href="#指定分隔符-F" class="headerlink" title="指定分隔符 -F"></a>指定分隔符 -F</h3><ul><li><code>awk -F &quot;:&quot; &#39;&#123;print $1&#125;&#39; test_awk2.txt</code></li></ul><h3 id="内容匹配"><a href="#内容匹配" class="headerlink" title="内容匹配"></a>内容匹配</h3><ul><li><code>格式&#39;/这里写具体的正则表达式/&#39;</code></li><li>正则规则<ul><li>1、^linux 以linux开头的行</li><li>2、$php 以php结尾的行</li><li>3、. 匹配任意单字符</li><li>4、.+ 匹配任意多个字符</li><li>5、 .* 匹配0个或多个字符(可有可无)</li><li>6、 [0-9a-z] 匹配中括号内任意一个字符</li><li>7、 (linux)+ 出现多次Linux单词</li><li>8、 (web){2} web出现两次以上</li><li>9、\ 屏蔽转义</li></ul></li><li>匹配到aaa或者ddd,就打印全部内容<ul><li><code>awk -F &#39;:&#39; &#39;/aaa|ddd/ &#123;print $0&#125;&#39; test_awk2.txt</code></li></ul></li></ul><h3 id="段内容判断"><a href="#段内容判断" class="headerlink" title="段内容判断"></a>段内容判断</h3><ul><li>支持赋值,条件表达式,关系运算符等</li></ul><h3 id="段之间比较"><a href="#段之间比较" class="headerlink" title="段之间比较"></a>段之间比较</h3><ul><li><code>awk -F &#39;:&#39; &#39;$3&lt;$4 &#123;print $0&#125;&#39; test_awk2.txt</code></li></ul><h3 id="NR行号和NF段数"><a href="#NR行号和NF段数" class="headerlink" title="NR行号和NF段数"></a>NR行号和NF段数</h3><ul><li>概念<ul><li>NF段数</li><li>NR行号从1开始</li><li>nl命令在linux系统中用来计算文件中行号</li><li><code>nl test_awk2.txt | head -2</code></li></ul></li><li>从test_awk2.txt前3行，把第1段内容替换为test，指定分隔符为|，显示行号</li><li><code>awk -F &#39;:&#39; &#39;&#123;OFS=&quot;|&quot;&#125; NR&lt;=3 &amp;&amp; $1=&quot;test&quot; &#123;print NR, $0&#125;&#39; test_awk2.txt</code></li></ul><h3 id="分段求和"><a href="#分段求和" class="headerlink" title="分段求和"></a>分段求和</h3><ul><li>格式<ul><li>BEGIN{} {} END{}</li><li>BEGIN{}在读取数据之前做的事情, 可以理解为: 前.</li><li>{} 在读取过程中做的事情, 可以理解为: 中.</li><li>END{} 在读取数据之后做的事情, 可以理解为: 后.</li></ul></li><li>对test_awk2.txt中的第2段求和<ul><li><code>awk -F &#39;:&#39; &#39;BEGING&#123;&#125;&#123;total=total+$2&#125;END&#123;print total&#125;&#39; test_awk2.txt</code></li><li><code>awk -F &#39;:&#39; &#39;BEGIN&#123;&#125;&#123;total=total+$2&#125;END&#123;print total&#125;&#39; test_awk2.txt</code></li></ul></li></ul><h3 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h3><ul><li>统计当前目录所有文本文件的大小</li></ul><div class="code-wrapper"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;BEGIN&#123;&#125;&#123;total=total+$5&#125; END&#123;print(total)&#125;&#x27;</span></code></pre></div><ul><li>打印99乘法表</li></ul><div class="code-wrapper"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;BEGIN&#123; for(i=1;i&lt;=9;i++)&#123; for(j=1;j&lt;=i;j++)&#123; printf(&quot;%dx%d=%d%s&quot;, i, j, i*j, &quot;\t&quot; ) &#125; printf(&quot;\n&quot;) &#125; &#125;&#x27;</span></code></pre></div><ul><li>求总成绩</li></ul><p>文本文件</p><div class="code-wrapper"><pre><code class="hljs apache"><span class="hljs-attribute">Marry</span>    <span class="hljs-number">2143</span> <span class="hljs-number">78</span> <span class="hljs-number">84</span> <span class="hljs-number">77</span><span class="hljs-attribute">Jack</span>     <span class="hljs-number">2321</span> <span class="hljs-number">66</span> <span class="hljs-number">78</span> <span class="hljs-number">45</span><span class="hljs-attribute">Tom</span>     <span class="hljs-number">2122</span> <span class="hljs-number">48</span> <span class="hljs-number">77</span> <span class="hljs-number">71</span><span class="hljs-attribute">Mike</span>     <span class="hljs-number">2537</span> <span class="hljs-number">87</span> <span class="hljs-number">97</span> <span class="hljs-number">95</span><span class="hljs-attribute">Bob</span>      <span class="hljs-number">2415</span> <span class="hljs-number">40</span> <span class="hljs-number">57</span> <span class="hljs-number">62</span></code></pre></div><p>脚本文件</p><div class="code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/awk -f</span><span class="hljs-meta">#</span><span class="bash">运行前</span>BEGIN &#123;    math = 0    english = 0    computer = 0     printf &quot;NAME    NO.   MATH  ENGLISH  COMPUTER   TOTAL\n&quot;    printf &quot;---------------------------------------------\n&quot;&#125;<span class="hljs-meta">#</span><span class="bash">运行中</span>&#123;    math+=$3    english+=$4    computer+=$5    printf &quot;%-6s %-6s %4d %8d %8d %8d\n&quot;, $1, $2, $3,$4,$5, $3+$4+$5&#125;<span class="hljs-meta">#</span><span class="bash">运行后</span>END &#123;    printf &quot;---------------------------------------------\n&quot;    printf &quot;  TOTAL:%10d %8d %8d \n&quot;, math, english, computer    printf &quot;AVERAGE:%10.2f %8.2f %8.2f\n&quot;, math/NR, english/NR, computer/NR&#125;</code></pre></div><h2 id="shell编程"><a href="#shell编程" class="headerlink" title="shell编程"></a>shell编程</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><ul><li>shell脚本执行方式Shell 是一个用 C 语言编写的程序，通过 Shell 用户可以访问操作系统内核服务。</li><li>查看系统安装 <code>shellcat /etc/shells</code></li><li>查看Linux系统默认的SHELL解释器的<code>echo $SHELL</code></li></ul><h3 id="格式-1"><a href="#格式-1" class="headerlink" title="格式"></a>格式</h3><ul><li><code>#!/bin/bash</code></li></ul><h3 id="4种运行方式"><a href="#4种运行方式" class="headerlink" title="4种运行方式"></a>4种运行方式</h3><ul><li>sh执行</li><li>工作目录执行</li><li>绝对路径执行</li><li>source hello.sh</li></ul><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><ul><li>用户变量</li><li>环境变量</li><li>特殊变量</li><li>$#命令行参数的个数</li><li>$n 表示第n个参数</li><li>$0 当前程序的名称</li><li>$? 前一个命令或许或函数的返回码</li><li>$*以“参数1 参数2 。。。”形式保存所有参数</li><li>$@ 以“参数1”“参数2”。。。形式保存所有参数</li><li>$$ 本程序的（进程ID号）PID</li><li>$! 上一个命令的PID</li></ul><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ul><li>优先使用双引号</li><li>拼接字符串</li><li><code>wenhou_1=&quot;你好,$yourname .&quot;</code></li><li><code>wenhou_2=&quot;你好,&quot;$yourname&quot; .&quot;</code></li><li><code>wenhou_3=&quot;你好,\&quot;$yourname\&quot; .&quot;</code></li><li>获取字符串长度</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>string=<span class="hljs-string">&quot;jobs&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;string&#125;</span>    <span class="hljs-comment"># 输出结果: jobs</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;#string&#125;</span>   <span class="hljs-comment"># 输出结果: 4</span></code></pre></div><ul><li>提取子字符串</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>string=<span class="hljs-string">&quot;敢于亮剑决不后退&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;string:2:2&#125;</span>    <span class="hljs-comment"># 输出结果为: 亮剑</span></code></pre></div><ul><li>查找字符串（记得有漂号）</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>string=<span class="hljs-string">&quot;i am a boy&quot;</span><span class="hljs-built_in">echo</span> `expr index <span class="hljs-string">&quot;<span class="hljs-variable">$string</span>&quot;</span> am`</code></pre></div><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><ul><li>支持包括：算术、关系、布尔、字符串等运算符</li></ul><h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h3><ul><li>数字语句判断<ul><li>-eq 检测两个数是否相等，相等返回 true。</li><li>-ne检测两个数是否不相等，不相等返回 true。</li><li>gt检测左边的数是否大于右边的，如果是，则返回 true。</li><li>lt检测左边的数是否小于右边的，如果是，则返回 true。</li><li>-ge检测左边的数是否大于等于右边的，如果是，则返回 true。</li><li>-le检测左边的数是否小于等于右边的，如果是，则返回 true。</li></ul></li><li>字符串语句判断<ul><li>n STRING 字符串长度不为零</li><li>z STRING 字符串长度为0</li><li>= 判断两个字符串是否一样</li><li>!=判断两个字符串是否不一样</li></ul></li><li>文件语句判断<ul><li>f 存在且是普通文件</li><li>-d 存在且是目录</li><li>h 存在且是符号链接</li><li>e 文件存在</li><li>–r 文件存在并且可读</li><li>–w 文件存在并且可写</li><li>–x 文件存在并且可执行</li></ul></li><li>test命令：可以代替[]<ul><li>为0表示为真，为1表示为假</li></ul></li><li>let命令：执行一个或多个表达式</li><li>if语句格式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> condition    //条件, 条件要用 [] 包裹.             <span class="hljs-keyword">then</span>                command1     //符合条件后, 就会执行这里的内容                command2                ...                commandN             <span class="hljs-keyword">fi</span></code></pre></div><ul><li>if else 语法格式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> condition    //条件, 用[]包裹        <span class="hljs-keyword">then</span>            command1    //符合条件后, 执行的内容            command2           ...            commandN        <span class="hljs-keyword">else</span>            <span class="hljs-built_in">command</span>        //不符合条件后, 执行的内容.         <span class="hljs-keyword">fi</span></code></pre></div><ul><li>if else-if else</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">if</span>  condition1        //条件1        <span class="hljs-keyword">then</span>            command1        //满足条件1后, 执行的内容        <span class="hljs-keyword">elif</span>  condition2     //条件2        <span class="hljs-keyword">then</span>             command2        //满足条件2后, 执行的内容        <span class="hljs-keyword">else</span>            commandN        //所有条件都不满足, 则执行这里.        <span class="hljs-keyword">fi</span></code></pre></div><ul><li>第一种for循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> 变量 <span class="hljs-keyword">in</span> 值1 值2 值3…            <span class="hljs-keyword">do</span>            程序            <span class="hljs-keyword">done</span></code></pre></div><ul><li>第二种for循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> ((初始值；循环控制条件；变量变化))            <span class="hljs-keyword">do</span>            程序            <span class="hljs-keyword">done</span></code></pre></div><ul><li>while循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">while</span> 条件        <span class="hljs-keyword">do</span>            程序        <span class="hljs-keyword">done</span></code></pre></div><ul><li>死循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 方式1</span><span class="hljs-keyword">while</span> :<span class="hljs-comment"># 方式2</span><span class="hljs-keyword">while</span> <span class="hljs-literal">true</span><span class="hljs-comment"># 方式3</span><span class="hljs-keyword">for</span> ((;;))</code></pre></div><ul><li>case判断语句</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">case</span> 值  <span class="hljs-keyword">in</span>        模式1)            command1            command2            ...            commandN            ;;        模式2）            command1            command2            ...            commandN            ;;        <span class="hljs-keyword">esac</span></code></pre></div><ul><li>跳出循环break和continue操作</li></ul><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><ul><li>语法格式</li></ul><div class="code-wrapper"><pre><code class="hljs bash">[ <span class="hljs-keyword">function</span> ] <span class="hljs-function"><span class="hljs-title">funname</span></span>()&#123;            action;            [<span class="hljs-built_in">return</span> int;]        &#125;</code></pre></div><ul><li>格式解释<ol><li>可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。 </li><li>参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255）</li></ol></li><li>注意事项<ol><li>函数返回值在调用该函数后通过 $? 来获得。 </li><li>所有函数在使用前必须定义。这意味着必须将函数放在脚本开始部分，直至shell解释器首次发现它时，才可以使用。 调用函数仅使用其函数名即可。 </li><li>函数的返回值只能是0-255区间的数据, 否则返回的内容可能不是我们想要的结果.  一般定义一个函数, 是为了实现特殊的功能, 当这个功能执行完成后, 返回一个状态信息. 0成功, 1失败等, 我们根据这个信息再来做其他的操作即可. </li><li>$? 严格意义来讲并不是获取返回值的, 而是获取上一个的执行后状态码信息(0-255之间)默认情况下如果状态码为0 表示成功执行, 如果为其他值, 则表示执行有问题</li></ol></li><li>如何具体接收方法返回值的问题<ul><li>在方法外定义变量</li><li>用输出语句输出,在方法外用变量接收该值</li></ul></li><li>有参数的函数操作<ul><li>$1表示第一个参数,以此类推</li></ul></li></ul><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><ul><li>定义数组  <div class="code-wrapper"><pre><code class="hljs bash">my_array=(A B <span class="hljs-string">&quot;C&quot;</span> D)array_name[0]=value0array_name[1]=value1array_name[2]=value2</code></pre></div></li><li>读取数组  <div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;第一个元素为: <span class="hljs-variable">$&#123;my_array[0]&#125;</span>&quot;</span></code></pre></div></li><li>获取数组所有元素  <div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组的全部元素为: <span class="hljs-variable">$&#123;my_array[*]&#125;</span>&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组的全部元素为: <span class="hljs-variable">$&#123;my_array[@]&#125;</span>&quot;</span></code></pre></div></li><li>获取数组长度  <div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组元素个数为: <span class="hljs-variable">$&#123;#my_array[*]&#125;</span>&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组元素个数为: <span class="hljs-variable">$&#123;#my_array[@]&#125;</span>&quot;</span></code></pre></div></li><li>遍历数组:结合2中for循环</li></ul><h3 id="select语句"><a href="#select语句" class="headerlink" title="select语句"></a>select语句</h3><ul><li>概念<ul><li>擅长于交互式场合。用户可以从一组不同的值中进行选择.</li></ul></li><li><code>格式PS3= //界面提示符select var in ... ; do　commond done  .... now $var can be used ...</code></li><li>注意事项:break 命令退出循环，或exit 命令终止脚本</li></ul><h3 id="加载其它变量"><a href="#加载其它变量" class="headerlink" title="加载其它变量"></a>加载其它变量</h3><ul><li>格式1：<code>. filename</code></li><li>格式2：<code>source filename</code> （推荐使用）</li></ul><h3 id="综合案例-1"><a href="#综合案例-1" class="headerlink" title="综合案例"></a>综合案例</h3><ul><li>1.猜数字小游戏</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment">#生成100以内的随机数 提示用户猜测 猜对为止</span><span class="hljs-comment">#random 系统自带，值为0-32767任意数</span><span class="hljs-comment">#随机数1-100</span>num=$[RANDOM%100+1]<span class="hljs-comment">#read 提示用户猜数字</span><span class="hljs-comment">#if判断</span><span class="hljs-keyword">while</span>  :<span class="hljs-keyword">do</span><span class="hljs-built_in">read</span> -p <span class="hljs-string">&quot;计算机生成了一个 1‐100 的随机数,你猜: &quot;</span> cai    <span class="hljs-keyword">if</span> [ <span class="hljs-variable">$cai</span> -eq <span class="hljs-variable">$num</span> ]    <span class="hljs-keyword">then</span>       <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;恭喜,猜对了&quot;</span>       <span class="hljs-built_in">exit</span>    <span class="hljs-keyword">elif</span> [ <span class="hljs-variable">$cai</span> -gt <span class="hljs-variable">$num</span> ]    <span class="hljs-keyword">then</span>           <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;不巧,猜大了&quot;</span>      <span class="hljs-keyword">else</span>           <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;不巧,猜小了&quot;</span> <span class="hljs-keyword">fi</span><span class="hljs-keyword">done</span></code></pre></div><ul><li>2.数据库定时备份</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment">#完成数据库的定时备份</span><span class="hljs-comment">#备份的路径</span>BACKUP=/<span class="hljs-built_in">export</span>/data/db<span class="hljs-comment">#当前时间作为文件名</span>DATETIME=$(date +%Y_%m_%d_%H%M%S)<span class="hljs-comment">#可以通过输出变量来调试</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$DATETIME</span><span class="hljs-comment">#使用变量的时候，也可以用&#123;&#125;花括号的方式把变量名包起来，如下：</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;DATETIME&#125;</span> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;---------------------开始备份数据库---------------------&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;---------------------备份的路径是<span class="hljs-variable">$BACKUP</span>/<span class="hljs-variable">$DATETIME</span>.tar.gz---------------------&quot;</span><span class="hljs-comment">#主机ip地址</span>HOST=192.168.88.100<span class="hljs-comment">#数据库用户名</span>DB_USER=root<span class="hljs-comment">#数据库密码</span>DB_PWD=123456<span class="hljs-comment">#数据库名</span>DATABASE=test_shop<span class="hljs-comment">#创建备份路径</span><span class="hljs-comment">#如果备份的文件夹路径存在的话，就直接使用，否则就创建</span>[ ! -d <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>&quot;</span> ] &amp;&amp; mkdir -p <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>&quot;</span><span class="hljs-comment">#执行mysql的备份数据库的指令</span>mysqldump -u<span class="hljs-variable">$&#123;DB_USER&#125;</span> -p<span class="hljs-variable">$&#123;DB_PWD&#125;</span> --host=<span class="hljs-variable">$&#123;HOST&#125;</span> <span class="hljs-variable">$&#123;DATABASE&#125;</span> | gzip &gt; <span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>.sql.gz<span class="hljs-comment">#打包备份文件</span><span class="hljs-built_in">cd</span> <span class="hljs-variable">$&#123;BACKUP&#125;</span>tar -czvf <span class="hljs-variable">$&#123;DATETIME&#125;</span>.tar.gz <span class="hljs-variable">$&#123;DATETIME&#125;</span><span class="hljs-comment">#删除临时目录</span>rm -rf <span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span><span class="hljs-comment">#删除10天前的备份文件</span>find <span class="hljs-variable">$&#123;BACKUP&#125;</span> -mtime +10 -name <span class="hljs-string">&quot;*.tar.gz&quot;</span> -<span class="hljs-built_in">exec</span> rm -rf &#123;&#125; \;<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------备份成功-------------------------&quot;</span></code></pre></div><h2 id="小技巧总结"><a href="#小技巧总结" class="headerlink" title="小技巧总结"></a>小技巧总结</h2><h3 id="给变量赋值的特殊写法"><a href="#给变量赋值的特殊写法" class="headerlink" title="给变量赋值的特殊写法"></a>给变量赋值的特殊写法</h3><div class="code-wrapper"><pre><code class="hljs bash">a=`Linux命令`a=$(Linux命令)<span class="hljs-comment"># 这两个命令的执行结果是一样的, 都是把Linux命令的执行结果给变量</span></code></pre></div><h3 id="关于数字的运算"><a href="#关于数字的运算" class="headerlink" title="关于数字的运算"></a>关于数字的运算</h3><div class="code-wrapper"><pre><code class="hljs bash">$((<span class="hljs-number">10</span> + <span class="hljs-number">20</span>))$[10 + 20]`expr 10 + 20 `</code></pre></div><h3 id="关于for-while的条件"><a href="#关于for-while的条件" class="headerlink" title="关于for, while的条件"></a>关于for, while的条件</h3><div class="code-wrapper"><pre><code class="hljs bash">如果是 <span class="hljs-keyword">for</span>(()) 或者 <span class="hljs-keyword">while</span> (()) 这种方式, 不用通过$引入变量, 可以直接用.如果是 <span class="hljs-keyword">for</span>[] 或者 <span class="hljs-keyword">while</span>[]这种方式, 必须通过$引入变量, 才可以继续使用.</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>存档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hive性能优化</title>
    <link href="/2021/05/20/hive%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <url>/2021/05/20/hive%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="基础优化"><a href="#基础优化" class="headerlink" title="基础优化"></a>基础优化</h2><ul><li>Shuffle 阶段压缩</li><li>hive的数据压缩<ul><li>Snappy</li></ul></li><li>hive的数据存储格式<ul><li>ORC</li><li>TextFile</li></ul></li><li>fetch抓取</li><li>本地模式</li><li>join的优化<ul><li>小表在前，小表放入缓冲区</li><li>谓词下推，先过滤再 join</li></ul></li><li>SQL优化的方案<ul><li>列裁剪</li><li>分区裁剪</li><li>group by 优化</li><li>count（distinct）优化</li></ul></li><li>动态分区调整</li><li>并行编译执行</li><li>严格模式<ul><li>拒绝可能影响效率的 SQL 语句</li></ul></li><li>jvm的重用</li><li>推测执行</li><li>小文件合并</li><li>矢量化查询</li><li>读取零拷贝</li><li>关联优化器<ul><li>多个MR共享使用一个shuffle结果</li></ul></li></ul><h2 id="Join-优化"><a href="#Join-优化" class="headerlink" title="Join 优化"></a>Join 优化</h2><h3 id="map-join"><a href="#map-join" class="headerlink" title="map join"></a>map join</h3><ul><li>核心<ul><li>将某个符合条件的表放置在内存中, 然后和另一个在map端进行join工作</li></ul></li><li>解决问题<ul><li>1.数据倾斜问题</li><li>2.提升查询的效率</li></ul></li><li>适用场景<ul><li>小表和大表join场景</li></ul></li><li>使用条件<ul><li>1.必须有一张表为小表<ul><li>set hive.auto.convert.join.noconditionaltask.size=512000000</li><li>设置小表最大数据量,默认值为 20M</li></ul></li><li>2.必须开启map Join优化支持<ul><li>set hive.auto.convert.join=true; 默认就是true</li></ul></li><li>3.不限制任何类型的表</li></ul></li></ul><h3 id="bucket-map-join"><a href="#bucket-map-join" class="headerlink" title="bucket map join"></a>bucket map join</h3><ul><li>适应场景<ul><li>中型表 和 大表 join</li></ul></li><li>使用条件<ul><li>1.开启bucket map join: set hive.optimize.bucketmapjoin = true;</li><li>2.一个表的bucket数是另一个表bucket数的整数倍</li><li><ol start="3"><li>bucket列 == join列</li></ol></li><li>4.必须是应用在map join的场景中</li><li>5.两个表必须是分桶表</li></ul></li></ul><h3 id="SMB-map-join"><a href="#SMB-map-join" class="headerlink" title="SMB map join"></a>SMB map join</h3><ul><li>适应场景<ul><li>大表 和 大表join</li></ul></li><li>使用条件<ul><li>1.开启bucket map join: set hive.optimize.bucketmapjoin = true;</li><li>2.两个表的分桶数量是一致的</li><li>3.bucket列 == join列 == sort列</li><li>4.必须是应用在bucket map join的场景中</li><li>5.两个表必须是分桶表</li></ul></li><li>整理SMB条件<ul><li><ol><li>保证 join的表必须是桶表:</li></ol><ul><li>set hive.enforce.bucketing=true; 写入数据强制分桶</li></ul></li><li><ol start="2"><li>在建表的时候, 必须设置分桶排序字段, 而且需要保证, 分桶字段 == join字段 == 排序字段create table test_smb_2( mid string, age_id string ) CLUSTERED BY(mid) SORTED BY(mid) INTO 500 BUCKETS; set hive.enforce.sorting=true; – 开启强制排序操作</li></ol></li><li><ol start="3"><li>两个分桶表的分桶的数量必须是一致的</li></ol></li><li><ol start="4"><li>必须建立在bucket map join基础上:</li></ol><ul><li>set hive.optimize.bucketmapjoin = true;</li></ul></li><li><ol start="5"><li>开启SMB join支持</li></ol><ul><li>set hive.auto.convert.sortmerge.join=true;</li><li>set hive.auto.convert.sortmerge.join.noconditionaltask=true;</li></ul></li><li><ol start="6"><li>必须开启自动尝试SMB连接</li></ol><ul><li>set hive.optimize.bucketmapjoin.sortedmerge = true;</li></ul></li></ul></li></ul><h2 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><ul><li>减少最终数据的扫描量, 从而提高效率</li></ul><h3 id="原始索引"><a href="#原始索引" class="headerlink" title="原始索引"></a>原始索引</h3><ul><li>特点<ul><li>可以针对hive表中任意字段构建索引</li><li>构建索引后, 在查询数据时候, 根据索引查询, 减少MR读取数据扫描量</li></ul></li><li>弊端<ul><li>索引不能自动更新, 当表中数据发生变更后, 需要手动重建索引</li><li>此性能是一般(数据量越大, 性能越差)</li></ul></li><li>应用场景<ul><li>生产中一般不采用原始索引</li><li>在hive3.0版本后, 不支持hive原始索引</li></ul></li></ul><h3 id="行组索引row-group-index"><a href="#行组索引row-group-index" class="headerlink" title="行组索引row group index"></a>行组索引row group index</h3><ul><li>特点<ul><li>一旦开启后, 会将表的每一个字段的最小 最大值都放置在索引中</li></ul></li><li>使用条件<ul><li>1.必须应用在表结构为ORC类型</li><li>2.在插入数据的时候, 要保证后续需要索引的字段, 是有序插入的</li><li>3.行组索引主要是应用在数值类型的字段上</li><li>4.在创建表的时候, 必须开启 行组索引<ul><li>‘orc.create.index’=’true’</li></ul></li><li><ol start="5"><li>开启hive自动使用索引(CM)</li></ol><ul><li>hive.optimize.index.filter=true</li></ul></li></ul></li><li>应用场景<ul><li>主要是应用 数值类型字段上, 并且执行 &gt; &lt; &gt;= &lt;= = 相关的操作中</li></ul></li><li>使用方案<ul><li>1.在建表的时候, 需要开启行组索引CREATE TABLE lxw1234_orc2 ( 字段 ….)stored AS ORCTBLPROPERTIES( ‘orc.compress’=’SNAPPY’,- 开启行组索引 ‘orc.create.index’=’true’)</li><li>2.在向表中插入数据的时候, 保证对应需要索引的列 进行有序插入</li></ul></li></ul><h3 id="开发过滤索引Bloom-Filter-index"><a href="#开发过滤索引Bloom-Filter-index" class="headerlink" title="开发过滤索引Bloom Filter index"></a>开发过滤索引Bloom Filter index</h3><ul><li>说明<ul><li>将需要构建索引的字段的值, 在索引信息(script)中进行放置</li><li>当查询的时候, 根据索引字段查询, 首先会先查询这个script索引信息中, 是否包含这个值</li><li>如果包含, 直接查询这个script片段, 如果不包含, 直接跳过</li></ul></li><li>使用条件<ul><li>1.必须应用在表结构为ORC类型</li><li>2.在建表的时候, 需要指定为那些字段构建开发过滤索引:<ul><li>‘orc.bloom.filter.columns’=’pcid,字段2, 字段3’</li></ul></li><li>3.必须要应用在 等值连接场景中 (不局限数据类型)</li></ul></li><li>应用场景<ul><li>应用在等值连接场景(不限制数据类型)</li></ul></li><li>使用方案<ul><li>1.需要开启行组索引</li><li>2.指定字段开启BloomFilter索引CREATE TABLE lxw1234_orc2 ( 字段) stored AS ORCTBLPROPERTIES( ‘orc.compress’=’SNAPPY’, ‘orc.create.index’=’true’,- pcid字段开启BloomFilter索引 “orc.bloom.filter.columns”=”pcid”)</li></ul></li></ul><h3 id="总结索引使用方法"><a href="#总结索引使用方法" class="headerlink" title="总结索引使用方法"></a>总结索引使用方法</h3><ul><li>行组索引建议常开<ul><li>在查询过程中, 如果正常插入数据是有序的, 并且根据这个字段查询操作</li><li>自动使用行组索引, 从而提升效率, 而且增加行组索引, 不会导致太多冗余数据出现</li></ul></li><li>开发过滤索引看需求<ul><li>将后续会经常的进行等值连接的字段, 构建为开发过滤的索引</li><li>需要大家有一定的预判能力</li></ul></li></ul><h2 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h2><h3 id="join-的数据倾斜"><a href="#join-的数据倾斜" class="headerlink" title="join 的数据倾斜"></a>join 的数据倾斜</h3><ul><li>方案1: 采用 map join</li><li>方案2: 将数据倾斜的key单独找一个MR处理,最后合并结果<ul><li>运行期解决<ul><li>说明<ul><li>知道一定有数据倾斜, 但是不知道哪一个key会有数据倾斜</li></ul></li><li>处理思路<ul><li>在运行的过程中, 对每个key的数量进行计数, 当发现key对应的value的条数比较大的时候, 认为key出现了数据倾斜问题</li><li>将整个键值从当面MR移出去, 单独找一个MR来处理</li></ul></li><li>配置方式<ul><li>set hive.optimize.skewjoin=true;</li><li>开启运行期数据倾斜的处理,默认为FALSE</li><li>set hive.skewjoin.key=100000;</li><li>当key数据量到达多少的时候, 认为出现数据倾斜</li></ul></li></ul></li><li>编译期解决<ul><li>说明<ul><li>提前知道哪个key会出现数据倾斜</li></ul></li><li>处理思路<ul><li>创建表的时候, 提前指定那个key会出现数据倾斜</li><li>后期执行操作的时候hive会在编译期将key单独移出去, 单独找一个MR运行</li></ul></li><li>配置方式<ul><li>配置参数<ul><li>set hive.optimize.skewjoin.compiletime=true;</li><li>是否开启编译期数据倾斜解决</li></ul></li><li>创建表的参数<ul><li>SKEWED BY (key) ON (1,5,6)</li><li>倾斜的字段和需要拆分的key值</li><li>[STORED AS DIRECTORIES];-</li><li>为倾斜值创建子目录单独存放</li></ul></li></ul></li></ul></li><li>开启MR合并优化<ul><li>说明<ul><li>两个MR执行完成后, 直接将数据写入到最终目录下, 直接作为最终结果</li><li>建议和join倾斜的配置同时开启</li></ul></li><li>设置参数<ul><li>set hive.optimize.union.remove=true;</li><li>开启对union的优化配置</li></ul></li></ul></li><li>生产环境做法<ul><li>两个组合使用, 提前知道提前定义好, 不知道的采用运行期解决, 达到效率最高</li><li>并且可以解决所有的join倾斜问题</li></ul></li></ul></li></ul><h3 id="group-by-的数据倾斜"><a href="#group-by-的数据倾斜" class="headerlink" title="group by 的数据倾斜"></a>group by 的数据倾斜</h3><ul><li>说明<ul><li>当某一个的数据量远远大于其它组的数据量, 这个时候就容易出现数据倾斜</li></ul></li><li>解决方案<ul><li>方案1: 小的combiner操作<ul><li>说明<ul><li>利用规约提前处理k2v2</li></ul></li><li>设置参数<ul><li>hive.map.aggr=true;</li><li>开启map端的combiner操作</li></ul></li></ul></li><li>方案2:负载均衡策略(大的combiner)<ul><li>说明<ul><li>利用2个MR来解决, 第一个MR负责将数据打散,保证每一个reduce都能接收到大致相等的数据量</li><li>得出一个局部结果,第二个MR对局部结果进一步处理,得出最终结果(自定义分区)</li></ul></li><li>设置参数<ul><li>hive.groupby.skewindata=true;</li><li>开启groupby负载均衡 默认为FALSE</li></ul></li></ul></li><li>注意事项<ul><li>在多个列上进行的去重操作与hive环境变量 hive.groupby.skewindata 存在冲突。</li></ul></li></ul></li></ul><h3 id="如何判断执行的SQL存在数据倾斜"><a href="#如何判断执行的SQL存在数据倾斜" class="headerlink" title="如何判断执行的SQL存在数据倾斜"></a>如何判断执行的SQL存在数据倾斜</h3><ul><li>1.查看MR日志,对比reduce执行时间</li><li>2.运行过程中,通过HUE来观察</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>HiveSQL优化方法与实践（全）</p><p><a href="https://mp.weixin.qq.com/s/C8236cW8E4HMwox-UJFrgA">https://mp.weixin.qq.com/s/C8236cW8E4HMwox-UJFrgA</a> </p><p>原来 8 张图，就可以搞懂「零拷贝」了！</p><p><a href="https://mp.weixin.qq.com/s/P0IP6c_qFhuebwdwD8HM7w">https://mp.weixin.qq.com/s/P0IP6c_qFhuebwdwD8HM7w</a></p>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hbase 无法删除表问题及解决办法</title>
    <link href="/2021/03/20/Hbase%20%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E8%A1%A8%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2021/03/20/Hbase%20%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E8%A1%A8%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul><li>正常删除表格的方法</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 禁用表</span><span class="hljs-built_in">disable</span> <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span><span class="hljs-comment"># 删除表</span>drop <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span></code></pre></div><p>但是操作过程中出现如下的问题</p><ul><li>已经禁用表</li></ul><div class="code-wrapper"><pre><code class="hljs bash">hbase(main):005:0&gt; <span class="hljs-built_in">disable</span> <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span>ERROR: Table TRIPDB:trip_sample is disabled!For usage try <span class="hljs-string">&#x27;help &quot;disable&quot;&#x27;</span>Took 0.1485 seconds</code></pre></div><ul><li>但是无法删除表，删除报错，死循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash">hbase(main):004:0&gt; drop <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span>ERROR: Table org.apache.hadoop.hbase.TableNotDisabledException: Not DISABLED; tableName=TRIPDB:trip_sample, state=ENABLING        at org.apache.hadoop.hbase.master.HMaster.checkTableModifiable(HMaster.java:2517)        at org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.prepareDelete(DeleteTableProcedure.java:241)        at org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.executeFromState(DeleteTableProcedure.java:89)        at org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.executeFromState(DeleteTableProcedure.java:56)        at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:189)        at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:850)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1473)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1241)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access<span class="hljs-variable">$800</span>(ProcedureExecutor.java:75)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor<span class="hljs-variable">$WorkerThread</span>.run(ProcedureExecutor.java:1761) should be disabled!For usage try <span class="hljs-string">&#x27;help &quot;drop&quot;&#x27;</span>Took 0.7635 seconds                                                                                                                                                  hbase(main):005:0&gt; <span class="hljs-built_in">disable</span> <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span>ERROR: Table TRIPDB:trip_sample is disabled!For usage try <span class="hljs-string">&#x27;help &quot;disable&quot;&#x27;</span></code></pre></div><h3 id="产生原因"><a href="#产生原因" class="headerlink" title="产生原因"></a>产生原因</h3><p>1、未开启 Zookeeper 就开启 Hbase 创建表，元数据没有同步到 ZK</p><p>2、 disable  tableName 命令执行未完成就删除 Hbase， 以至于表格被锁定</p><p>3、数据输入过大，HMaster 崩溃</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><ul><li>删除 Hbase 对应表格元数据</li></ul><div class="code-wrapper"><pre><code class="hljs bash">scan <span class="hljs-string">&quot;hbase:meta&quot;</span><span class="hljs-comment"># 找到要删除的表格的 rowkey</span>                                                                                                                                              TRIPDB:trip_sample                               column=table:state, timestamp=1632540393079, value=\x08\x03                                                                                     TRIPDB:trip_sample,,1632540392481.c3d71389e53809 column=info:regioninfo, timestamp=1632540396531, value=&#123;ENCODED =&gt; c3d71389e538092c4eda1b01b5a0a441, NAME =&gt; <span class="hljs-string">&#x27;TRIPDB:trip_sample,,1632540392481</span><span class="hljs-string"> 2c4eda1b01b5a0a441.                              .c3d71389e538092c4eda1b01b5a0a441.&#x27;</span>, STARTKEY =&gt; <span class="hljs-string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="hljs-string">&#x27;&#x27;</span>&#125;                                                                              TRIPDB:trip_sample,,1632540392481.c3d71389e53809 column=info:sn, timestamp=1632540396531, value=node1,16020,1632540299845                                                                        2c4eda1b01b5a0a441.                                                                                                                                                                              TRIPDB:trip_sample,,1632540392481.c3d71389e53809 column=info:state, timestamp=1632540396531, value=OPENING                                                                                       2c4eda1b01b5a0a441.<span class="hljs-comment"># 也可以通过报错查看 rowkey</span>scan <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span></code></pre></div><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 根据 rowkey 删除整行数据</span>deleteall <span class="hljs-string">&#x27;hbase:meta&#x27;</span>,<span class="hljs-string">&quot;TRIPDB:trip_sample,,1632540392481.c3d71389e53809&quot;</span></code></pre></div><ul><li>删除 Zookeeper 中的表格元数据，在 table  和 table-lock 中</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 登陆Zookeeper 客户端</span>bin/zkCli.sh<span class="hljs-comment"># 删除对应表格元数据</span>ls /hbase/tablels /hbase/table-lockrmr /hbase/tablermr /hbase/table-lock</code></pre></div><ul><li>删除 HDFS 上对应数据</li></ul><div class="code-wrapper"><pre><code class="hljs bash">hdfs dfs ls /hbase/data/TRIPDB/hdfs dfs rm -r /hbase/data/TRIPDB/trip_sample</code></pre></div><ul><li>修复 Hbase 元数据</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment">#命令行执行</span>hbase hbck -fixMeta</code></pre></div><ul><li>重启 ZK 和 Hbase</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 关闭</span>bin/zkServer.sh stopbin/stop-hbase.sh stop<span class="hljs-comment"># 重启</span>bin/zkServer.sh startbin/stop-hbase.sh start</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Debug记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hbase</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive数仓缓慢渐变维之拉链表</title>
    <link href="/2021/03/05/Hive%E6%95%B0%E4%BB%93%E7%BC%93%E6%85%A2%E6%B8%90%E5%8F%98%E7%BB%B4%E4%B9%8B%E6%8B%89%E9%93%BE%E8%A1%A8/"/>
    <url>/2021/03/05/Hive%E6%95%B0%E4%BB%93%E7%BC%93%E6%85%A2%E6%B8%90%E5%8F%98%E7%BB%B4%E4%B9%8B%E6%8B%89%E9%93%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="缓慢渐变维"><a href="#缓慢渐变维" class="headerlink" title="缓慢渐变维"></a>缓慢渐变维</h3><p>主要是为了解决, 是否需要在数仓中维护历史变化的数据操作</p><p>注意：如果不维护一个数据的历史变化信息, 那么在进行数仓分析的时候, 是有可能对未来分析的结果产生影响</p><p><strong>实现缓慢维的3种方式</strong></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-operator">*</span><span class="hljs-operator">*</span>SCD1<span class="hljs-operator">*</span><span class="hljs-operator">*</span>对于历史变化的数据, 是进行维护操作, 直接进行覆盖即可此种操作仅适合于对于错误数据处理<span class="hljs-operator">*</span><span class="hljs-operator">*</span>SCD2(拉链表)<span class="hljs-comment">--常用**</span>对原有表, 增加两个新的字段, 一个是起始的时间字段,一个是截止时间字段当有数据发生变更后, 只需要对上一次数据进行标记起止范围, 新增一个新的变更后的数据即可, 由此产生一个拉链数据状态   好处: 不会修改原有记录数据操作, 利于维护操作, 而且可以对多个历史版本进行数据存储操作  弊端: 造成数据冗余, 占用更大的磁盘空间<span class="hljs-operator">*</span><span class="hljs-operator">*</span>SCD3<span class="hljs-operator">*</span><span class="hljs-operator">*</span>直接对原有表, 进行新增列的方案, 一旦有数据发生变更, 新增一列字段, 标记当前最新数据即可,以此来维护历史变化数据 适合于磁盘空间不足, 而且只需要维护少量历史变化情况  优点: 尽可能减少数据冗余情况,   弊端: 不利于维护, 仅能维护少量的历史变化版本</code></pre></div><h3 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h3><ul><li>拉链表处理逻辑</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>创建临时表<span class="hljs-number">2.</span>查询新数据，即抽取数据中 end_time<span class="hljs-operator">=</span>&quot;9999-99-99&quot;<span class="hljs-number">3.</span>查询旧数据，就原拉链表的数据，如果主键 id 与抽取数据中的重复，且 end_time<span class="hljs-operator">=</span>&quot;9999-99-99&quot;,需要改成 end_time<span class="hljs-operator">=</span>业务日期 <span class="hljs-operator">-</span> <span class="hljs-number">1</span>天<span class="hljs-number">4.</span>合并新旧数据到临时表<span class="hljs-number">5.</span>临时表插入原表</code></pre></div><ul><li>基本流程图</li><li><img src="https://i.loli.net/2021/09/18/ptV2hILPlW1uZUB.png" alt="CD2"></li></ul>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>牛客网SQL练习总结</title>
    <link href="/2021/02/13/%E7%89%9B%E5%AE%A2%E7%BD%91SQL%E7%BB%83%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <url>/2021/02/13/%E7%89%9B%E5%AE%A2%E7%BD%91SQL%E7%BB%83%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a>补充知识</h2><h3 id="补充知识整理"><a href="#补充知识整理" class="headerlink" title="补充知识整理"></a>补充知识整理</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>一张表可以多次被引用使用<span class="hljs-number">2.</span>筛选条件包含某个值, 这个值可以通过子查询求出, 再通过 <span class="hljs-keyword">where</span> 条件判断<span class="hljs-number">3.</span>子查询没有符合要求的条件会直接返回 <span class="hljs-keyword">null</span><span class="hljs-number">4.</span><span class="hljs-keyword">insert</span> ignore <span class="hljs-keyword">into</span> 相当于 replace<span class="hljs-number">5.</span>创建视图格式:  <span class="hljs-keyword">create</span> <span class="hljs-keyword">view</span> actor_name_view <span class="hljs-keyword">as</span> <span class="hljs-operator">+</span> 字段列表(查询结果等)<span class="hljs-number">6.</span>查询强制走索引: <span class="hljs-keyword">from</span> 表名 <span class="hljs-operator">+</span> force index(索引名)<span class="hljs-number">7.</span>创建表字段后<span class="hljs-operator">+</span> <span class="hljs-keyword">default</span> <span class="hljs-string">&#x27;默认值&#x27;</span>, 即赋予默认值<span class="hljs-number">8.</span>如果需要对数据进行更新操作, 一定是通过update 或者 replace 进行<span class="hljs-number">9.</span>需要排除最大值最小值再进行计算的题目,可以通过子查询排除, 或者通过 first_value last_value 开窗函数进行<span class="hljs-number">10.</span>exsits 用来判断是否存在某种条件的记录，存在就返回 <span class="hljs-literal">true</span> ,不存在则返回 <span class="hljs-literal">false</span><span class="hljs-number">11.</span>第 <span class="hljs-number">1</span> 的隐含条件是 该字段数值大于 它的数量为 <span class="hljs-number">0</span> , 大于等于它的数量为 <span class="hljs-number">1</span> , 以此类推<span class="hljs-number">12.</span>聚合字段的部分值，可以让其它值为<span class="hljs-number">0</span>，为<span class="hljs-keyword">null</span>，聚合的时候不影响结果</code></pre></div><h3 id="非空校验方法"><a href="#非空校验方法" class="headerlink" title="非空校验方法"></a>非空校验方法</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--如果x为null，就返回y，否则返回x</span>nvl(x,y)<span class="hljs-comment">--返回集合中第一个不为null的值，如果全部为null就返回null</span>colease(x,y,e,d,f)<span class="hljs-comment">--如果x为null，就返回y，否则返回x</span>ifnull(x,y)</code></pre></div><h3 id="创建和删除索引"><a href="#创建和删除索引" class="headerlink" title="创建和删除索引"></a>创建和删除索引</h3><p><a href="https://www.nowcoder.com/practice/e1824daa0c49404aa602cf0cb34bdd75">SQL37 对first_name创建唯一索引uniq_idx_firstname</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- MySQL中4种方式给字段添加索引</span><span class="hljs-comment">-- 创建主键索引,索引值必须是唯一的，且不能为NULL。</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> <span class="hljs-keyword">PRIMARY</span> KEY (col_list);<span class="hljs-comment">-- 创建唯一索引,值唯一</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> <span class="hljs-keyword">UNIQUE</span> index_name (col_list);<span class="hljs-comment">-- 创建普通索引,索引值可以重复出现</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> INDEX index_name (col_list);<span class="hljs-comment">-- 创建全文索引, 指定索引</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> FULLTEXT index_name (col_list);<span class="hljs-comment">-- 删除索引</span><span class="hljs-keyword">DROP</span> INDEX index_name <span class="hljs-keyword">ON</span> tbl_name;<span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">DROP</span> INDEX index_name；<span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">DROP</span> <span class="hljs-keyword">PRIMARY</span> KEY;</code></pre></div><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><div class="code-wrapper"><pre><code class="hljs sql">在MySQL中，创建触发器语法如下：<span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TRIGGER</span> trigger_nametrigger_time trigger_event <span class="hljs-keyword">ON</span> tbl_name<span class="hljs-keyword">FOR</span> <span class="hljs-keyword">EACH</span> <span class="hljs-type">ROW</span>trigger_stmt其中：trigger_name：标识触发器名称，用户自行指定；trigger_time：标识触发时机，取值为 BEFORE 或 AFTER；trigger_event：标识触发事件，取值为 <span class="hljs-keyword">INSERT</span>、UPDATE 或 <span class="hljs-keyword">DELETE</span>；tbl_name：标识建立触发器的表名，即在哪张表上建立触发器；trigger_stmt：触发器程序体，可以是一句<span class="hljs-keyword">SQL</span>语句，或者用 <span class="hljs-keyword">BEGIN</span> 和 <span class="hljs-keyword">END</span> 包含的多条语句，每条语句结束要分号结尾。</code></pre></div><p><a href="https://www.notion.so/7e920bb2e1e74c4e83750f5c16033e2e">SQL41 构造一个触发器audit_log</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">trigger</span> audit_logafter <span class="hljs-keyword">insert</span> <span class="hljs-keyword">on</span> employees_test<span class="hljs-keyword">for</span> <span class="hljs-keyword">each</span> <span class="hljs-type">row</span><span class="hljs-keyword">begin</span><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> audit <span class="hljs-keyword">values</span>(new.id,new.name);<span class="hljs-keyword">end</span></code></pre></div><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><ul><li>窗口函数完整模式</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 指定字段求和并排序，窗口范围所有行</span><span class="hljs-built_in">sum</span>(x) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> b  <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> unbounded preceding  <span class="hljs-keyword">and</span> unbounded following )<span class="hljs-comment">-- 指定字段求和并排序，窗口范围是从改值前面的行到当前行</span><span class="hljs-built_in">sum</span>(x) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> b  <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> unbounded preceding  <span class="hljs-keyword">and</span> <span class="hljs-keyword">current</span> <span class="hljs-type">row</span> )<span class="hljs-comment">-- 指定字段求和并做排序，将所有行当前字段数据中，当前行值+2 和 -2范围之内的所有值求和 </span><span class="hljs-built_in">sum</span>(x) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a <span class="hljs-keyword">range</span> <span class="hljs-keyword">between</span> <span class="hljs-number">2</span> preceding  <span class="hljs-keyword">and</span> <span class="hljs-number">2</span> followling )</code></pre></div><ul><li>常用窗口函数</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 排名窗口函数</span><span class="hljs-built_in">row_number</span>()<span class="hljs-built_in">rank</span>()<span class="hljs-built_in">dense_rank</span>()<span class="hljs-comment">-- 聚合窗口函数</span>sum 求和count 总数max 最大值min 最小值avg 平均值<span class="hljs-comment">--其它窗口函数</span>lag 上一行的值lead 下一行的值first_value 该字段第一行的值last_value 该字段最后一行的值</code></pre></div><h3 id="其它常用函数"><a href="#其它常用函数" class="headerlink" title="其它常用函数"></a>其它常用函数</h3><div class="code-wrapper"><pre><code class="hljs sql">concat(字符<span class="hljs-number">1</span>, 连接符,字符<span class="hljs-number">2</span>) 指定连接符连接字符 <span class="hljs-number">1</span> 和字符 <span class="hljs-number">2</span>group_concat（X，Y）X是要连接的字段，Y是连接时用的符号，可省略，默认为逗号。此函数必须与<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> 配合使用。统计字符串长度：<span class="hljs-keyword">char_length</span>(<span class="hljs-string">&#x27;string&#x27;</span>) <span class="hljs-operator">/</span> <span class="hljs-keyword">char_length</span>(column_name)<span class="hljs-number">1</span>、返回值为字符串string或者对应字段长度，长度的单位为字符，一个多字节字符（例如，汉字）算作一个单字符；<span class="hljs-number">2</span>、不管汉字还是数字或者是字母都算是一个字符；<span class="hljs-number">3</span>、任何编码下，多字节字符都算是一个字符；参考资料来源：https:<span class="hljs-operator">/</span><span class="hljs-operator">/</span>blog.csdn.net<span class="hljs-operator">/</span>iris_xuting<span class="hljs-operator">/</span>article<span class="hljs-operator">/</span>details<span class="hljs-operator">/</span><span class="hljs-number">53763894</span>length(<span class="hljs-string">&#x27;string&#x27;</span>)<span class="hljs-operator">/</span>length(column_name)<span class="hljs-number">1</span>、utf8字符集编码下,一个汉字是算三个字符,一个数字或字母算一个字符。<span class="hljs-number">2</span>、其他编码下,一个汉字算两个字符, 一个数字或字母算一个字符。字符串替换：replace(s,s1,s2)，将字符串 s2 替代字符串 s 中的字符串 s1MySQL常用函数：https:<span class="hljs-operator">/</span><span class="hljs-operator">/</span>www.runoob.com<span class="hljs-operator">/</span>mysql<span class="hljs-operator">/</span>mysql<span class="hljs-operator">-</span>functions.html截取字符串函数 substr(X,Y,Z) 或 substr(X,Y) X是要截取的字符串 Y是字符串的起始位置Z是要截取字符串的长度<span class="hljs-keyword">left</span> 和 <span class="hljs-keyword">right</span> 函数切割，后面 <span class="hljs-keyword">left</span>(a,<span class="hljs-number">7</span>) a是要切割的字符串，<span class="hljs-number">7</span>是切割位数date_sub(<span class="hljs-type">date</span>,expr) <span class="hljs-type">date</span><span class="hljs-operator">/</span>datetime 减去expr 值后返回对应 <span class="hljs-type">date</span><span class="hljs-operator">/</span>datetime                   </code></pre></div><h2 id="求排名"><a href="#求排名" class="headerlink" title="求排名"></a>求排名</h2><h3 id="求排名第N个的"><a href="#求排名第N个的" class="headerlink" title="求排名第N个的"></a>求排名第N个的</h3><p><a href="https://www.nowcoder.com/practice/ec1ca44c62c14ceb990c3c40def1ec6c">SQL2 查找入职员工时间排名倒数第三的员工所有信息</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 方法1:先求出排名求第N个, 再倒排</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> employees<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> hire_date <span class="hljs-keyword">desc</span>limit <span class="hljs-number">0</span>,<span class="hljs-number">3</span>) e<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> e.hire_datelimit <span class="hljs-number">1</span>;<span class="hljs-comment">-- 方法2:子查询,排名字段比自己高大于N的即可, 缺点是如果数据少于排名就不能求出</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> employees e1<span class="hljs-keyword">where</span> <span class="hljs-number">2</span><span class="hljs-operator">=</span>(<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">from</span> employees e2 <span class="hljs-keyword">where</span> e1.hire_date <span class="hljs-operator">&lt;</span> e2.hire_date);<span class="hljs-comment">-- 方法3:子查询, 比排名N+1中最高的就是第N个</span><span class="hljs-comment">-- 方法4:开窗函数，不计算重复就用row_number ，统计重复就dense_rank，子查询 &lt;=N 即可。</span></code></pre></div><h3 id="求某个分组里面排名第N个"><a href="#求某个分组里面排名第N个" class="headerlink" title="求某个分组里面排名第N个"></a>求某个分组里面排名第N个</h3><p><a href="https://www.nowcoder.com/practice/4a052e3e1df5435880d4353eb18a91c6">SQL12 获取每个部门中当前员工薪水最高的相关信息</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 1.窗口函数求出每个成员在分组里面的排名</span><span class="hljs-comment">-- 2.子查询让排名等于N</span><span class="hljs-comment">-- 3.可以做个非空校验, 如果没有符合的的rank, 就返回指定值.</span><span class="hljs-keyword">select</span> dept_no,emp_no,salary <span class="hljs-keyword">as</span> maxSalary<span class="hljs-keyword">from</span>(<span class="hljs-keyword">select</span> d.dept_no,d.emp_no,s.salary,<span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> d.dept_no <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> s.salary <span class="hljs-keyword">desc</span>)  <span class="hljs-keyword">as</span> r1<span class="hljs-keyword">from</span> dept_emp d, salaries s<span class="hljs-keyword">where</span> d.emp_no <span class="hljs-operator">=</span>s.emp_no) ss<span class="hljs-keyword">where</span> r1<span class="hljs-operator">=</span><span class="hljs-number">1</span><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> dept_no;</code></pre></div><h2 id="用户登陆相关"><a href="#用户登陆相关" class="headerlink" title="用户登陆相关"></a>用户登陆相关</h2><h3 id="最近登陆日期"><a href="#最近登陆日期" class="headerlink" title="最近登陆日期"></a>最近登陆日期</h3><p><a href="https://www.nowcoder.com/practice/7cc3c814329546e89e71bb45c805c9ad">SQL67 牛客每个人最近的登录日期(二）</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--基本思路就是求出最近登陆的日期和 user_id 作为字典表，里面对应的user_id 就是最近登陆的用户，查询即可。</span><span class="hljs-keyword">select</span> u.name,c.name,l.date<span class="hljs-keyword">from</span> login l, <span class="hljs-keyword">user</span> u , client c<span class="hljs-keyword">where</span> l.user_id<span class="hljs-operator">=</span>u.id <span class="hljs-keyword">and</span> l.client_id<span class="hljs-operator">=</span>c.id<span class="hljs-keyword">and</span> (l.user_id,l.date) <span class="hljs-keyword">in</span>(<span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">max</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> u.name;</code></pre></div><h3 id="次日留存率"><a href="#次日留存率" class="headerlink" title="次日留存率"></a>次日留存率</h3><p><a href="https://www.nowcoder.com/practice/16d41af206cd4066a06a3a0aa585ad3d">SQL68 牛客每个人最近的登录日期(三)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 1.筛选出初次登陆的新用户</span><span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>)<span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id;<span class="hljs-comment">--2.新用户登陆后, 第二天登陆的情况</span><span class="hljs-keyword">select</span> round(<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id) <span class="hljs-operator">/</span> (<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id) <span class="hljs-keyword">from</span> login) ,<span class="hljs-number">3</span>) <span class="hljs-keyword">as</span> p<span class="hljs-keyword">from</span> login<span class="hljs-keyword">where</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,date_add(<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>),<span class="hljs-type">INTERVAL</span> <span class="hljs-number">1</span> <span class="hljs-keyword">day</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)</code></pre></div><h3 id="每日新增用户数"><a href="#每日新增用户数" class="headerlink" title="每日新增用户数"></a>每日新增用户数</h3><p><a href="https://www.nowcoder.com/practice/e524dc7450234395aa21c75303a42b0a">SQL69 牛客每个人最近的登录日期(四)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--1.求出新用户登陆日期</span><span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">as</span> <span class="hljs-type">date</span> <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id;<span class="hljs-comment">--2.通过case when 判断, 如果当天登陆用户中和新用户登陆表相同则记为1,否则是0.</span><span class="hljs-keyword">select</span> tmp.date,<span class="hljs-built_in">sum</span>(tmp.t) <span class="hljs-keyword">as</span> <span class="hljs-keyword">new</span><span class="hljs-keyword">from</span>(<span class="hljs-keyword">select</span> <span class="hljs-type">date</span>,<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">as</span> <span class="hljs-type">date</span> <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id) <span class="hljs-keyword">then</span> <span class="hljs-number">1</span><span class="hljs-keyword">else</span> <span class="hljs-number">0</span><span class="hljs-keyword">end</span> <span class="hljs-keyword">as</span> t<span class="hljs-keyword">from</span> login) tmp<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>;</code></pre></div><h3 id="新用户次日留存率"><a href="#新用户次日留存率" class="headerlink" title="新用户次日留存率"></a>新用户次日留存率</h3><p><a href="https://www.nowcoder.com/practice/ea0c56cd700344b590182aad03cc61b8">SQL70 牛客每个人最近的登录日期(五)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--分子:当前日期+ 1day 中是前一天新用户的用户</span><span class="hljs-comment">--分母 当前日期中是新用户, 即当前日期是该用户所有登陆日期中最小的</span><span class="hljs-comment">--当前日期-1day =date   等于 当前日期= date + 1day</span><span class="hljs-keyword">select</span> <span class="hljs-type">date</span>,ifnull(round (<span class="hljs-built_in">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id, <span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)  <span class="hljs-keyword">and</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id, date_add(<span class="hljs-type">date</span>,<span class="hljs-type">interval</span> <span class="hljs-number">-1</span> <span class="hljs-keyword">day</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id) <span class="hljs-keyword">then</span> <span class="hljs-number">1</span><span class="hljs-keyword">else</span> <span class="hljs-number">0</span><span class="hljs-keyword">end</span>)  <span class="hljs-operator">/</span><span class="hljs-built_in">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id, <span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)<span class="hljs-keyword">then</span> <span class="hljs-number">1</span><span class="hljs-keyword">else</span> <span class="hljs-number">0</span><span class="hljs-keyword">end</span>),  <span class="hljs-number">3</span>),<span class="hljs-number">0</span>)<span class="hljs-keyword">as</span> p<span class="hljs-keyword">from</span> login<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>;</code></pre></div><h3 id="用户登陆并统计核心行为统计"><a href="#用户登陆并统计核心行为统计" class="headerlink" title="用户登陆并统计核心行为统计"></a>用户登陆并统计核心行为统计</h3><p><a href="https://www.nowcoder.com/practice/572a027e52804c058e1f8b0c5e8a65b4">SQL71 牛客每个人最近的登录日期(六)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 1.先求出每个用户登陆时候的刷题信息和姓名</span><span class="hljs-comment">-- 2.根据日期排序, 再根据姓名排序</span><span class="hljs-comment">-- 3.题目是求到某一天累计刷题多少, 我们可以通过子查询筛选符合的</span><span class="hljs-keyword">select</span> p1.user_id,p1.date,<span class="hljs-built_in">sum</span>(p1.number) <span class="hljs-keyword">as</span> number<span class="hljs-keyword">from</span> passing_number p1,passing_number p2<span class="hljs-keyword">where</span>  p1.user_id<span class="hljs-operator">=</span>p2.user_id <span class="hljs-keyword">and</span> p1.date<span class="hljs-operator">&gt;=</span>p2.date<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> p1.user_id,p1.date;<span class="hljs-comment">-- 最终SQL</span><span class="hljs-keyword">select</span>u.name,p.date,p.number<span class="hljs-keyword">from</span> login l,(<span class="hljs-keyword">select</span> p1.user_id,p1.date,<span class="hljs-built_in">sum</span>(p2.number) <span class="hljs-keyword">as</span> number<span class="hljs-keyword">from</span> passing_number p1 <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> passing_number p2<span class="hljs-keyword">on</span>  p1.user_id<span class="hljs-operator">=</span>p2.user_id <span class="hljs-keyword">and</span> p1.date<span class="hljs-operator">&gt;=</span>p2.date<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> p1.user_id,p1.date) p ,<span class="hljs-keyword">user</span> u <span class="hljs-keyword">where</span> l.user_id<span class="hljs-operator">=</span>p.user_id <span class="hljs-keyword">and</span> l.date<span class="hljs-operator">=</span>p.date <span class="hljs-keyword">and</span> l.user_id<span class="hljs-operator">=</span>u.id<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> p.date,u.name;</code></pre></div><h3 id="求连续登陆N天的用户"><a href="#求连续登陆N天的用户" class="headerlink" title="求连续登陆N天的用户"></a>求连续登陆N天的用户</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--求连续三天登录的人数的方法</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">or</span> replace temporary <span class="hljs-keyword">view</span> mytab <span class="hljs-keyword">as</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-01&#x27;</span> logintime <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-02&#x27;</span> logintime <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-03&#x27;</span> logintime <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-04&#x27;</span> logintime;<span class="hljs-comment">--思路一，将having count(1)&gt;=n，就是求连续n天登录的人数</span><span class="hljs-comment">--连续三天登陆等于登陆日期-登陆的次数排序=第一天登陆</span><span class="hljs-keyword">with</span> t1 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> name,logintime <span class="hljs-keyword">from</span> mytab),     t2 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,                    date_sub(logintime,<span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> name <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> logintime)) <span class="hljs-keyword">as</span> temp_date             <span class="hljs-keyword">from</span> t1 )<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> name) cnt<span class="hljs-keyword">from</span> t2<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> name, temp_date<span class="hljs-keyword">having</span> <span class="hljs-built_in">count</span>(<span class="hljs-number">1</span>)<span class="hljs-operator">&gt;=</span><span class="hljs-number">3</span>;<span class="hljs-comment">--思路二，将下面的date_add(logintime,n-1)，lead(logintime,n-1) ，就是求连续n天登录的人数</span><span class="hljs-keyword">with</span> t1 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> name,logintime <span class="hljs-keyword">from</span> mytab),     t2 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,                    date_add(logintime,<span class="hljs-number">2</span>) <span class="hljs-keyword">as</span> expectdate,                    <span class="hljs-built_in">lead</span>(logintime,<span class="hljs-number">2</span>) <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> name <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> logintime) realdate             <span class="hljs-keyword">from</span> t1 )<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> name) cnt  <span class="hljs-keyword">from</span> t2 <span class="hljs-keyword">where</span> expectdate<span class="hljs-operator">=</span>realdate;</code></pre></div><h2 id="中位数相关"><a href="#中位数相关" class="headerlink" title="中位数相关"></a>中位数相关</h2><h3 id="求中位数"><a href="#求中位数" class="headerlink" title="求中位数"></a>求中位数</h3><p><a href="https://www.nowcoder.com/practice/502fb6e2b1ad4e56aa2e0dd90c6edf3c">考试分数（四）</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--需求</span><span class="hljs-comment">--请你写一个sql语句查询各个岗位分数升序排列之后的中位数位置的范围，并且按job升序排序</span><span class="hljs-comment">--补充知识</span><span class="hljs-comment">--中位数的特征：</span><span class="hljs-comment">--当个数为偶数时，中位数的起始位置等于个数/2，结束位置等于个数/2+1</span><span class="hljs-comment">--当个数为奇数时，中位数的起始位置等于向上取整（个数/2），结束位置等于向上取整（个数/2）</span><span class="hljs-comment">--用除以2的余数是否为0来判断奇偶，%2=0</span><span class="hljs-comment">--记得取整数，本题用ceiling函数向上取整（返回不小于该数的最小整数值）或round(数，0)四舍五入取整都可以</span><span class="hljs-comment">--我的写法</span><span class="hljs-keyword">SELECT</span>job,<span class="hljs-built_in">ceil</span>(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span>) <span class="hljs-keyword">as</span> <span class="hljs-keyword">start</span>,<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> <span class="hljs-built_in">count</span>(id)<span class="hljs-operator">%</span><span class="hljs-number">2</span><span class="hljs-operator">=</span><span class="hljs-number">0</span><span class="hljs-keyword">then</span> <span class="hljs-built_in">ceil</span>(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span><span class="hljs-operator">+</span><span class="hljs-number">1</span>)<span class="hljs-keyword">else</span> <span class="hljs-built_in">ceil</span>(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span>)<span class="hljs-keyword">end</span> <span class="hljs-keyword">as</span> <span class="hljs-keyword">end</span><span class="hljs-keyword">from</span> grade<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> job<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> job;# 大神简化版 ,后面的<span class="hljs-operator">+</span><span class="hljs-number">1</span>再除以<span class="hljs-number">2</span>, <span class="hljs-keyword">select</span> job, round(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span>), round((<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">+</span><span class="hljs-number">1</span>)<span class="hljs-operator">/</span><span class="hljs-number">2</span>)<span class="hljs-keyword">from</span> grade <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> job<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> job;</code></pre></div><h3 id="求中位数以上的情况"><a href="#求中位数以上的情况" class="headerlink" title="求中位数以上的情况"></a>求中位数以上的情况</h3><p><a href="https://www.nowcoder.com/practice/b626ff9e2ad04789954c2132c74c0512">考试分数（五）</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--需求</span><span class="hljs-comment">--请你写一个sql语句查询各个岗位分数的中位数位置上的所有grade信息，并且按id升序排序</span><span class="hljs-comment">--1.先求出中位数范围</span><span class="hljs-comment">--2.再通过开窗函数求出每个同学分数的自然排名</span><span class="hljs-comment">--3.按照id排序</span><span class="hljs-keyword">SELECT</span>tmp.id,tmp.job,tmp.score,tmp.t_rank<span class="hljs-keyword">from</span>(<span class="hljs-keyword">SELECT</span> id,job,score,  <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) t_rank,  <span class="hljs-built_in">count</span>(score) <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job) <span class="hljs-keyword">as</span> cnt  <span class="hljs-keyword">from</span> grade) <span class="hljs-keyword">as</span> tmp  <span class="hljs-keyword">where</span> tmp.t_rank<span class="hljs-operator">=</span><span class="hljs-built_in">floor</span>((tmp.cnt<span class="hljs-operator">+</span><span class="hljs-number">1</span>)<span class="hljs-operator">/</span><span class="hljs-number">2</span>) <span class="hljs-keyword">or</span> tmp.t_rank<span class="hljs-operator">=</span><span class="hljs-built_in">floor</span>((tmp.cnt<span class="hljs-operator">+</span><span class="hljs-number">2</span>)<span class="hljs-operator">/</span><span class="hljs-number">2</span>)  <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> tmp.id;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>面试准备</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis常见面试题</title>
    <link href="/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <url>/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>学习和整理 Redis 相关的知识当中，这里汇总了一下经常被问到的 Redis 面试题</p><p>Redis 的八股无外乎这三个：缓存穿透、缓存击穿、缓存雪崩。</p><h1 id="分片集群问题"><a href="#分片集群问题" class="headerlink" title="分片集群问题"></a>分片集群问题</h1><h3 id="1-Redis的多数据机制了解多少"><a href="#1-Redis的多数据机制了解多少" class="headerlink" title="1.Redis的多数据机制了解多少"></a>1.Redis的多数据机制了解多少</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>Redis支持多个数据库,单机模式下有从db0到db15, 数据库之间不能共享<span class="hljs-number">2.</span>分片集群中只有一个数据库空间,不会使用到Redis的多数据库</code></pre></div><h3 id="2-懂Redis的批量操作吗"><a href="#2-懂Redis的批量操作吗" class="headerlink" title="2.懂Redis的批量操作吗"></a>2.懂Redis的批量操作吗</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>有mget,mset,hmget,hmset,hgetall,hvals,<span class="hljs-number">2.</span>分片集群中, 不同的key会分到不同的slot中, 不能直接使用mget,mset如何解决: 加上相同的前缀,用大括号&#123;&#125;包裹</code></pre></div><h3 id="3-Redis的集群机制中-你觉得有什么不足的地方吗"><a href="#3-Redis的集群机制中-你觉得有什么不足的地方吗" class="headerlink" title="3.Redis的集群机制中, 你觉得有什么不足的地方吗?"></a>3.Redis的集群机制中, 你觉得有什么不足的地方吗?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>如果<span class="hljs-keyword">value</span>是hash类型, 对象非常大, 即对应属性非常多, 也只能存入一个集群的节点中<span class="hljs-number">2.</span>批量操作也很麻烦, 属性太多写得很长</code></pre></div><h3 id="4-在Redis集群模式下-如何进行批量操作"><a href="#4-在Redis集群模式下-如何进行批量操作" class="headerlink" title="4.在Redis集群模式下, 如何进行批量操作?"></a>4.在Redis集群模式下, 如何进行批量操作?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>如何执行的key数量很少, 串行<span class="hljs-keyword">get</span>操作<span class="hljs-number">2.</span>如果一定要批量操作, 加上相同的前缀,前缀用&#123;&#125;包裹</code></pre></div><h3 id="4-5-什么是Redis的事务"><a href="#4-5-什么是Redis的事务" class="headerlink" title="4.5 什么是Redis的事务"></a>4.5 什么是Redis的事务</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>Redis事务是一些命令的集合, 执行的时候是串行执行<span class="hljs-number">2.</span>分片集群中,不同的key可能会被分到不同的slot中, Redis的事务不生效<span class="hljs-number">3.</span>Redis的事务不支持回滚操作, 基本用不上.</code></pre></div><h1 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h1><h3 id="5-什么是缓存穿透-如何解决"><a href="#5-什么是缓存穿透-如何解决" class="headerlink" title="5.什么是缓存穿透,如何解决"></a>5.什么是缓存穿透,如何解决</h3><div class="code-wrapper"><pre><code class="hljs sql">现象：客户端高并发不断向Redis请求一个不存在的Key，MySQL中也没有由于Redis没有，导致这个并发全部落在MySQL上解决<span class="hljs-number">1.</span>对于那些每秒访问频次过高的IP进行限制，拒绝访问<span class="hljs-number">2.</span>如果第一次redis中没有，读MYSQL，MySQL也没有，在Redis中设置一个<span class="hljs-operator">=</span><span class="hljs-operator">=</span>临时<span class="hljs-operator">=</span><span class="hljs-operator">=</span>默认值<span class="hljs-number">3.</span>利用BitMap类型构建布隆过滤器<span class="hljs-operator">*</span><span class="hljs-operator">*</span>(只保证MySQL数据库没有这个数据, 不保证一定有)<span class="hljs-operator">*</span><span class="hljs-operator">*</span></code></pre></div><h3 id="6-什么是缓存击穿-如何解决"><a href="#6-什么是缓存击穿-如何解决" class="headerlink" title="6.什么是缓存击穿,如何解决"></a>6.什么是缓存击穿,如何解决</h3><div class="code-wrapper"><pre><code class="hljs sql">现象：有一个Key，经常需要高并发的访问，这个Key有过期时间的，一旦达到过期时间<span class="hljs-operator">=</span><span class="hljs-operator">=</span>，这个Key被删除，所有高并发落到了MySQL中，被击穿了解决<span class="hljs-number">1.</span>资源充足的情况下，设置永不过期<span class="hljs-number">2.</span>对这个Key做一个互斥锁，只允许一个请求去读取，其他的所有请求先阻塞掉第一个请求redis中没有读取到，读了MySQL，再将这个数据放到Redis中释放所有阻塞的请求</code></pre></div><h3 id="7-什么是缓存雪崩-如何解决"><a href="#7-什么是缓存雪崩-如何解决" class="headerlink" title="7.什么是缓存雪崩 ,如何解决"></a>7.什么是缓存雪崩 ,如何解决</h3><div class="code-wrapper"><pre><code class="hljs sql">现象：大量的Key在同一个时间段过期，大量的Key的请求在Redis中都没有，都去请求MySQL，导致MySQL奔溃解决<span class="hljs-number">1.</span>资源充足允许的情况下，设置大部分的Key不过期<span class="hljs-number">2.</span>给所有Key设置过期时间时加上随机值，让Key不再同一时间过期</code></pre></div><h3 id="8-Redis中的Key如何设计"><a href="#8-Redis中的Key如何设计" class="headerlink" title="8.Redis中的Key如何设计?"></a>8.Redis中的Key如何设计?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>使用统一的命名规范<span class="hljs-number">2.</span>一般使用业务名(或数据库名)为前缀，用冒号分隔，例如，业务名:表名:id。例如：shop:usr:msg_code（电商:用户:验证码）<span class="hljs-number">4.</span>控制key名称的长度，不要使用过长的key<span class="hljs-number">5.</span>在保证语义清晰的情况下，尽量减少Key的长度。有些常用单词可使用缩写，例如，<span class="hljs-keyword">user</span>缩写为u，messages缩写为msg<span class="hljs-number">6.</span>名称中不要包含特殊字符、包含空格、单双引号以及其他转义字符</code></pre></div><h3 id="9-为什么Redis是单线程的"><a href="#9-为什么Redis是单线程的" class="headerlink" title="9.为什么Redis是单线程的?"></a>9.为什么Redis是单线程的?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>因为Redis是基于内存的操作，CPU不是Redis的瓶颈<span class="hljs-number">2.</span>Redis的瓶颈最有可能是机器内存的大小或者网络带宽<span class="hljs-number">3.</span>单线程容易实现，而且CPU不会成为瓶颈，所以没必要使用多线程增加复杂度<span class="hljs-number">4.</span>可以使用多Redis压榨CPU，提高性能</code></pre></div><h3 id="10-为什么Redis的性能很高"><a href="#10-为什么Redis的性能很高" class="headerlink" title="10.为什么Redis的性能很高?"></a>10.为什么Redis的性能很高?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>基于内存操作, 快<span class="hljs-number">2.</span>用C语言编写, 数据结构简单, 对数据操作也简单<span class="hljs-number">3.</span>采用单线程, 避免不必要的线程切换和资源抢占<span class="hljs-number">4.</span>IO多路复用模型, 非阻塞IO</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>面试准备</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WordCount案例汇总</title>
    <link href="/2020/11/06/WordCount%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB/"/>
    <url>/2020/11/06/WordCount%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount</p><p>经典永不过时，使用的很多函数和方法也是常用的。</p><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<span class="hljs-keyword">import</span> java.io.IOException;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Map规则, 用来实现把: k1, v1 -&gt; k2, v2, 需要 继承Mapper类, 重写map方法.</span><span class="hljs-comment"> * 各个数据解释:</span><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountMapTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;    <span class="hljs-comment">/**</span><span class="hljs-comment">     * 重写map方法,用来将K1 V2 转换成 K2 V2</span><span class="hljs-comment">     *</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k1</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> value   v1</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写出K2,V2</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><span class="hljs-comment">     */</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;        <span class="hljs-comment">//1.获取行偏移量,没有什么用处,我们用于测试看看的</span>        <span class="hljs-keyword">long</span> index = key.get();        System.out.println(<span class="hljs-string">&quot;行偏移量是: &quot;</span> + index);        <span class="hljs-comment">//2.获取整行数据</span>        String line = value.toString();        <span class="hljs-comment">//3.读取并做非空校验,判断值是否相等,也判断地址值是否相等</span>        <span class="hljs-keyword">if</span> (line != <span class="hljs-keyword">null</span> &amp;&amp; !<span class="hljs-string">&quot;&quot;</span>.equals(line)) &#123;            <span class="hljs-comment">//4.切割获取K2,V2</span>            String[] str = line.split(<span class="hljs-string">&quot; &quot;</span>);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; str.length; i++) &#123;                String s = str[i];                context.write(<span class="hljs-keyword">new</span> Text(s), <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>));            &#125;        &#125;    &#125;&#125;</code></pre></div><h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<span class="hljs-keyword">import</span> java.io.IOException;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Reduce规则, 用来实现把: k2, v2的集合 -&gt; k3, v3, 需要 继承Reducer类, 重写reduce方法.</span><span class="hljs-comment"> * 各个数据解释:</span><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * shuffle阶段: 分区, 排序, 规约, 分组之后, 数据如下:</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v2(的集合): 每个单词的所有次数的集合, 例如: &#123;1, 1&#125;,  &#123;1, 1, 1&#125;, &#123;1, 1&#125;</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * k3: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v3: 每个单词的总次数, 例如: 2, 3, 2</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountReduceTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;    <span class="hljs-comment">//重写reduce方法</span>    <span class="hljs-comment">/**</span><span class="hljs-comment">     * 重写reduce方法,用于把k2,v2 转换成k3,v3</span><span class="hljs-comment">     *</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k2</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> values  v2的集合(已经经过了分组)</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写k3,v3</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><span class="hljs-comment">     */</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;        <span class="hljs-comment">//1.获取k3,就是每个单词</span>        String word = key.toString();        <span class="hljs-comment">//2.获取v3,就是单词出现的次数</span>        <span class="hljs-comment">//2.1先对v2集合求和</span>        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;        <span class="hljs-keyword">for</span> (IntWritable value : values) &#123;            count += value.get();        &#125;        <span class="hljs-comment">//2.2写出v3</span>        <span class="hljs-comment">//context.write(new Text(word),new IntWritable(count));</span>        <span class="hljs-comment">//因为v2和v3是一样的,我们可以优化一下</span>        context.write(key, <span class="hljs-keyword">new</span> IntWritable(count));    &#125;&#125;</code></pre></div><h3 id="WordCountMain简写版"><a href="#WordCountMain简写版" class="headerlink" title="WordCountMain简写版"></a>WordCountMain简写版</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<span class="hljs-keyword">import</span> java.io.IOException;<span class="hljs-comment">/**</span><span class="hljs-comment"> * 这里写的是驱动类, 即: 封装MR程序的核心8步的. 它有两种写法:</span><span class="hljs-comment"> * 1. 官方示例版, 即: 完整版.   理解即可, 因为稍显复杂, 用的人较少.</span><span class="hljs-comment"> * 2. 简化版.  推荐掌握.</span><span class="hljs-comment"> * 这里是简化版写法</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);        <span class="hljs-comment">//2.封装MR程序核心8步</span>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span>        job.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/input/wordcount.txt&quot;</span>));        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span>        job.setMapperClass(WordCountMapTask.class);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.3 分区,用默认的</span>        <span class="hljs-comment">//2.4 排序,用默认的</span>        <span class="hljs-comment">//2.5 规约,用默认的</span>        <span class="hljs-comment">//2.6 分组,用默认的</span>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span>        job.setReducerClass(WorkCountReduceTask.class);        job.setOutputValueClass(Text.class);        job.setOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span>        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/output&quot;</span>));        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);    &#125;&#125;</code></pre></div><h3 id="WordCountMain-jar包版"><a href="#WordCountMain-jar包版" class="headerlink" title="WordCountMain jar包版"></a>WordCountMain jar包版</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<span class="hljs-comment">/**</span><span class="hljs-comment"> * 这个代码一会儿是要打包成jar包, 然后放到Yarn集群中运行的, 需要做如下的几件事儿:</span><span class="hljs-comment"> * 1. 在驱动类中设置 jar包的启动类.</span><span class="hljs-comment"> * job.setJarByClass(WordCountMain3.class);</span><span class="hljs-comment"> * 2. 修改数据源文件 和 目的地文件的路径, 改为: 外部传入.</span><span class="hljs-comment"> * TextInputFormat.addInputPath(job, new Path(args[0]));</span><span class="hljs-comment"> * TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><span class="hljs-comment"> * 3. 对我们当前的工程进行打包动作, 打包成: 胖jar, 具体操作为: 取消pom.xml文件中最后一个插件的注释, 然后打包即可.</span><span class="hljs-comment"> * 细节: 修改jar包名字为: wordcount.jar, 方便我们操作.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 4. 在HDFS集群中创建:   /wordcount/input/ 目录</span><span class="hljs-comment"> * 5. 把wordcount.txt 上传到该目录下.</span><span class="hljs-comment"> * 6. 把之前打好的 jar包也上传到 Linux系统中.</span><span class="hljs-comment"> * 7. 运行该jar包即可, 记得: 传入 数据源文件路径, 目的地目录路径.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 名词解释:</span><span class="hljs-comment"> * 胖jar: 指的是一个jar包中还包含有其他的jar包, 这样的jar包就称之为: 胖jar.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 问题1: 为什么需要打包成 胖jar?</span><span class="hljs-comment"> * 答案:</span><span class="hljs-comment"> * 因为目前我们的工程需要依赖 Hadoop环境, 而我们已经在pom.xml文件中配置了,</span><span class="hljs-comment"> * 如果运行的环境中(例如: Linux系统等)没有hadoop环境, 并且我们打包时也没有把hadoop环境打包进去,</span><span class="hljs-comment"> * 将来运行jar包的时候就会出错.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 问题2: 当前工程一定要打包成 胖jar吗?</span><span class="hljs-comment"> * 答案: 不用, 因为我们的 jar包一会儿是放到 Yarn集群中运行的, 它已经自带Hadoop环境, 所以这里可以不打包 胖jar, 只打包我们自己的代码.</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain3</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);        <span class="hljs-comment">//细节1: 在驱动类中设置 jar包的启动类.</span>        job.setJarByClass(WorkCountMain3.class);        <span class="hljs-comment">//2.封装MR程序核心8步</span>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span>        job.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">0</span>]));        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span>        job.setMapperClass(WordCountMapTask.class);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.3 分区,用默认的</span>        <span class="hljs-comment">//2.4 排序,用默认的</span>        <span class="hljs-comment">//2.5 规约,用默认的</span>        <span class="hljs-comment">//2.6 分组,用默认的</span>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span>        job.setReducerClass(WorkCountReduceTask.class);        job.setOutputValueClass(Text.class);        job.setOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span>        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">1</span>]));        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);    &#125;&#125;</code></pre></div><h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>基本流程</p><div class="code-wrapper"><pre><code class="hljs java">基本流程<span class="hljs-number">1.</span>传输文件路径到自定义的Actor类,并接收返回值,解析返回值得出统计结果<span class="hljs-number">2.</span>自定义Actor类, 接收文件路径并做解析统计单词再返回给发送者需要分别定义<span class="hljs-number">3</span>个类Main入口<span class="hljs-number">1.</span>用于发送文件路径,封装在自定义的单例类里面<span class="hljs-number">2.</span>接收返回值,并做判断是否完成传输, 如果完成就开始解析<span class="hljs-number">3.</span>通过apply方法解析结果,合并结果得出最后结果自定义的Actor类<span class="hljs-number">1.</span>接收文件路径信息,做分析统计<span class="hljs-number">2.</span>把结果封装在单例类中,返回给发送者自定义的单例类<span class="hljs-number">1.</span>用于封装发送信息的单例类<span class="hljs-number">2.</span>用于返回统计的单例类</code></pre></div><h3 id="MainActor"><a href="#MainActor" class="headerlink" title="MainActor"></a>MainActor</h3><div class="code-wrapper"><pre><code class="hljs java">`<span class="hljs-keyword">package</span> com.test.day04.wordcount<span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<span class="hljs-keyword">import</span> java.io.File<span class="hljs-keyword">import</span> scala.actors.Future<span class="hljs-comment">/**</span><span class="hljs-comment"> * 1.发送文件名给WordCountActor</span><span class="hljs-comment"> * 2.接收WordCountActor返回结果并合并</span><span class="hljs-comment"> */</span>object MainActor &#123;  <span class="hljs-comment">//发送文件名给WordCountActor</span>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;    <span class="hljs-comment">//1.获取文件名</span>    val fileDir = <span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;./data&quot;</span>)    val files: Array[File] = fileDir.listFiles()    <span class="hljs-comment">// 测试是成功获取文件名</span>    <span class="hljs-comment">// files.foreach(println(_))</span>    <span class="hljs-comment">//2.发送给wordcountactor</span>    val future_Array: Array[Future[Any]] = files.map(f = file =&gt; &#123;      val name = file.toString      <span class="hljs-comment">//每一个文件名新建对应的线程</span>      val actor = <span class="hljs-keyword">new</span> WordCountActor      <span class="hljs-comment">//开启线程并发送给我认定任务</span>      actor.start()      <span class="hljs-comment">//发送的消息封装在这里面并获取结果</span>      val future: Future[Any] = actor !! WordCountTask(name)      future    &#125;)    <span class="hljs-comment">//接收WordCountActor返回结果并合并</span>    <span class="hljs-comment">//先判断是否全部文件都处理完毕都有结果,是再处理</span>    <span class="hljs-keyword">while</span> (!(future_Array.filter((x) =&gt; &#123;      !x.isSet    &#125;)).isEmpty) &#123;&#125;    <span class="hljs-comment">//走到这里, 证明我们可以处理,使用apply获取数据</span>    <span class="hljs-comment">//里面的键值对就是多个文件统计结果, 我们还需要合并去重</span>    val wordCount: Array[Map[String, Int]] = future_Array.map((x) =&gt; &#123;      val results: Any = x.apply()      val result = results.asInstanceOf[WordCountResult]      val map: Map[String, Int] = result.map      map    &#125;)    <span class="hljs-comment">//wordCount.foreach(println(_))</span>    <span class="hljs-comment">//测试结果</span>    <span class="hljs-comment">// Map(e -&gt; 2, f -&gt; 1, a -&gt; 1, b -&gt; 1, c -&gt; 1)</span>    <span class="hljs-comment">// Map(e -&gt; 1, a -&gt; 2, b -&gt; 1, c -&gt; 2, d -&gt; 3)</span>    <span class="hljs-comment">//合并结果, 先合并成一个Array</span>    val flatten: Array[(String, Int)] = wordCount.flatten    <span class="hljs-comment">//根据Map的key值分组</span>    val wordGroup: Map[String, Array[(String, Int)]] = flatten.groupBy((x) =&gt; &#123;      x._1    &#125;)    val finalResult: Map[String, Int] = wordGroup.map((x) =&gt; &#123;      val name = x._1      val size = x._2.size      name -&gt; size    &#125;)    finalResult.foreach(println(_))  &#125;&#125;</code></pre></div><h3 id="WordCountActor"><a href="#WordCountActor" class="headerlink" title="WordCountActor"></a>WordCountActor</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<span class="hljs-keyword">import</span> scala.actors.Actor<span class="hljs-keyword">import</span> scala.io.Source<span class="hljs-comment">/**</span><span class="hljs-comment"> * 1.接收MainActor的文件名称并进行单词统计</span><span class="hljs-comment"> * 2.将单词统计结果返回给MainActor</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountActor</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Actor</span> </span>&#123;  <span class="hljs-function">override def <span class="hljs-title">act</span><span class="hljs-params">()</span>: Unit </span>= &#123;    <span class="hljs-comment">//接收消息</span>    loop &#123;      react &#123;        <span class="hljs-function"><span class="hljs-keyword">case</span> <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename)</span> </span>=&gt;          println(<span class="hljs-string">&quot;收到了文件名: &quot;</span> + filename)          <span class="hljs-comment">//解析消息, 通过Source解析消息, 定义文件来源再转化成列表</span>          <span class="hljs-comment">//一个元素就是一个一行数据</span>          val words: List[String] = Source.fromFile(filename).getLines().toList          <span class="hljs-comment">//切割获取每一条数据并合并成一个list集合</span>          val word_List: List[String] = words.flatMap((x) =&gt; &#123;            x.split(<span class="hljs-string">&quot; &quot;</span>)          &#125;)          <span class="hljs-comment">//按照单词进行分组, 然后聚合统计</span>          val word_Tuples: List[(String, Int)] = word_List.map((x) =&gt; &#123;            (x, <span class="hljs-number">1</span>)          &#125;)          val word_Map: Map[String, List[(String, Int)]] = word_Tuples.groupBy((x) =&gt; &#123;            x._1          &#125;)          val wordCountMap: Map[String, Int] = word_Map.map((x) =&gt; &#123;            val name: String = x._1            val size: Int = x._2.size            name -&gt; size          &#125;)          <span class="hljs-comment">//把统计结果反馈给Mainactor,装进WordCount</span>          sender ! WordCountResult(wordCountMap)      &#125;    &#125;  &#125;&#125;</code></pre></div><h3 id="WordCountPackage"><a href="#WordCountPackage" class="headerlink" title="WordCountPackage"></a>WordCountPackage</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<span class="hljs-comment">/**</span><span class="hljs-comment"> * 1.定义一个样例类, 描述单词统计信息</span><span class="hljs-comment"> * 2.定义一个样例类封装单词统计结果</span><span class="hljs-comment"> */</span>object WordCountPackage &#123;  <span class="hljs-comment">//1.定义一个样例类, 描述单词统计信息</span>  <span class="hljs-function"><span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename: String)</span></span><span class="hljs-function"></span><span class="hljs-function">  <span class="hljs-comment">//2.定义一个样例类封装单词统计结果</span></span><span class="hljs-function">  <span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountResult</span><span class="hljs-params">(map: Map[String, Int])</span></span><span class="hljs-function"></span><span class="hljs-function">&#125;</span></code></pre></div><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><ul><li>基本流程</li></ul><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-number">1.</span>创建上下文对象<span class="hljs-number">2.</span>读取文件<span class="hljs-number">3.</span>flatMap获取到每个单词<span class="hljs-number">4.</span>map将RDD变成 key-value结构<span class="hljs-number">5.</span>reduceByKey 求和统计<span class="hljs-number">6.</span>打印输出<span class="hljs-number">7.</span>关闭上下文对象</code></pre></div><ul><li>本地版</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day01<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WordCount</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//1.创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>).setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span>    <span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.textFile(<span class="hljs-string">&quot;src/main/data/words.txt&quot;</span>)    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span>    <span class="hljs-keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;      x.split(<span class="hljs-string">&quot; &quot;</span>)    &#125;)    <span class="hljs-comment">//4.继续对每个单词标记为1</span>    <span class="hljs-keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span>    <span class="hljs-keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)    <span class="hljs-comment">//6.生成最后的RDD, 将结果打印到控制台</span>    ouputRDD.foreach(println(_))    <span class="hljs-comment">//7.关闭上下文</span>    sc.stop()  &#125;&#125;</code></pre></div><ul><li>Linux版</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day01<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WordCount_Linux</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//0.创建输入路径和输出路径</span>    <span class="hljs-keyword">val</span> input_path = args(<span class="hljs-number">0</span>)    <span class="hljs-keyword">val</span> output_path = args(<span class="hljs-number">1</span>)    <span class="hljs-comment">//1.创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>)    <span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span>    <span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.textFile(input_path)    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span>    <span class="hljs-keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;      x.split(<span class="hljs-string">&quot; &quot;</span>)    &#125;)    <span class="hljs-comment">//4.继续对每个单词标记为1</span>    <span class="hljs-keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span>    <span class="hljs-keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)    <span class="hljs-comment">//6.生成最后的RDD, 将结果上传到HDFS</span>    ouputRDD.saveAsTextFile(output_path)    <span class="hljs-comment">//7.关闭上下文</span>    sc.stop()  &#125;&#125;</code></pre></div><h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.sparksql<span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">Row</span>, <span class="hljs-type">SparkSession</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Author: Jface</span><span class="hljs-comment"> * @Date: 2021/9/9 23:34</span><span class="hljs-comment"> * @Desc: 使用 SparkSQL 读取文本文件做 Wordcount，分别使用 DSL 和 SQL 风格实现</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Wordcount</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//1.构建上下文对象，并导包</span>    <span class="hljs-keyword">val</span> spark: <span class="hljs-type">SparkSession</span> = <span class="hljs-type">SparkSession</span>.builder()      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)      .getOrCreate()    <span class="hljs-keyword">import</span> spark.implicits._    <span class="hljs-comment">//2.读取文本文件，获取 DataSet</span>    <span class="hljs-keyword">val</span> inputDataSet: <span class="hljs-type">Dataset</span>[<span class="hljs-type">String</span>] = spark.read.textFile(<span class="hljs-string">&quot;learnSpark/datas/wordcount.data&quot;</span>)    <span class="hljs-comment">//测试看看</span>    <span class="hljs-comment">//inputDataSet.printSchema()</span>    <span class="hljs-comment">//inputDataSet.show()</span>    <span class="hljs-comment">//3.使用 DSL风格实现，导包</span>    <span class="hljs-keyword">import</span> org.apache.spark.sql.functions._    <span class="hljs-comment">//3.1 过滤脏数据</span>    <span class="hljs-keyword">val</span> resultDataset01: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = inputDataSet.where($<span class="hljs-string">&quot;value&quot;</span>.isNotNull &amp;&amp; length(trim($<span class="hljs-string">&quot;value&quot;</span>)) &gt; <span class="hljs-number">0</span>)      <span class="hljs-comment">//3.2 切割并把 value 行转成列</span>      .select(explode(split(trim($<span class="hljs-string">&quot;value&quot;</span>), <span class="hljs-string">&quot;\\s+&quot;</span>)).as(<span class="hljs-string">&quot;word&quot;</span>))      <span class="hljs-comment">//3.3 分组并聚合</span>      .groupBy($<span class="hljs-string">&quot;word&quot;</span>)      .agg(count($<span class="hljs-string">&quot;word&quot;</span>).as(<span class="hljs-string">&quot;total&quot;</span>))      <span class="hljs-comment">//3.4 倒序并只求前5条信息~</span>      .orderBy($<span class="hljs-string">&quot;total&quot;</span>.desc)      .limit(<span class="hljs-number">5</span>)    <span class="hljs-comment">//resultDataset01.printSchema()</span>    <span class="hljs-comment">//resultDataset01.show(10, truncate = false)</span>    <span class="hljs-comment">//4.使用 SQL 风格实现</span>    <span class="hljs-comment">//4.1 注册临时视图</span>    <span class="hljs-comment">//4.2 编写 SQL 并执行</span>    inputDataSet.createOrReplaceTempView(<span class="hljs-string">&quot;tmp_view_lines&quot;</span>)    <span class="hljs-keyword">val</span> resultDataSet02: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = spark.sql(      <span class="hljs-string">&quot;&quot;&quot;</span><span class="hljs-string">        |with tmp as</span><span class="hljs-string">        | (select explode(split(trim(value), &quot;\\s+&quot;)) as word</span><span class="hljs-string">        |from tmp_view_lines</span><span class="hljs-string">        |where value is not null and length(trim(value)) &gt; 0 )</span><span class="hljs-string">        |select t.word ,count(1) as total</span><span class="hljs-string">        |from tmp t</span><span class="hljs-string">        |group by t.word</span><span class="hljs-string">        |order by total desc</span><span class="hljs-string">        |&quot;&quot;&quot;</span>.stripMargin)    resultDataSet02.printSchema()    resultDataSet02.show(<span class="hljs-number">5</span>, truncate = <span class="hljs-literal">false</span>)    <span class="hljs-comment">//5.关闭上下文对象</span>    spark.stop();  &#125;&#125;</code></pre></div><h3 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h3><ul><li>前期准备：安装 netcat</li></ul><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">// 在Linux上安装 netcat</span>yum install nc -yyum install nmap -y<span class="hljs-comment">// 向 9999 端口发送数据</span>nc -lk <span class="hljs-number">9999</span></code></pre></div><ul><li>Wordcount  by UpdateStateByKey</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day06.streaming<span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">ReceiverInputDStream</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: wordcount 案例，通过 UpdateStateByKey 实现宕机后状态恢复</span><span class="hljs-comment"> * 需要利用ncat 发数据， </span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S4ocketWordcountUpdateStateByKeyRecovery</span> </span>&#123;    <span class="hljs-comment">//设置路径</span>    <span class="hljs-keyword">val</span> <span class="hljs-type">CKP</span> =<span class="hljs-string">&quot;src/main/data/ckp/&quot;</span>+<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>)    <span class="hljs-comment">//1.创建上下文对象, 指定批处理时间间隔为5秒</span>    <span class="hljs-keyword">val</span> creatingFunc =()=&gt;    &#123;      <span class="hljs-keyword">val</span> conf: <span class="hljs-type">SparkConf</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()        .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))        .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)      <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)      <span class="hljs-comment">//2. 创建一个接收文本数据流的流对象</span>      <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))      <span class="hljs-comment">//3.设置checkpoint位置</span>      ssc.checkpoint(<span class="hljs-type">CKP</span>)      <span class="hljs-comment">//4.接收socket数据</span>      <span class="hljs-keyword">val</span> inputDStream: <span class="hljs-type">ReceiverInputDStream</span>[<span class="hljs-type">String</span>] = ssc.socketTextStream(<span class="hljs-string">&quot;node1&quot;</span>, <span class="hljs-number">9999</span>)      <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 5.wordcount, 并做累计统计</span>      <span class="hljs-comment">//自定义一个函数, 实现保存State状态和数据聚合</span>      <span class="hljs-comment">//seq里面是value的数组,[1,1,], state是上次的状态, 累计值</span>      <span class="hljs-keyword">val</span> updateFunc = (seq: <span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>], state: <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>]) =&gt; &#123;        <span class="hljs-keyword">if</span> (!seq.isEmpty) &#123;          <span class="hljs-keyword">val</span> this_value: <span class="hljs-type">Int</span> = seq.sum          <span class="hljs-keyword">val</span> last_value: <span class="hljs-type">Int</span> = state.getOrElse(<span class="hljs-number">0</span>)          <span class="hljs-keyword">val</span> new_state: <span class="hljs-type">Int</span> = this_value + last_value          <span class="hljs-type">Some</span>(new_state)        &#125;        <span class="hljs-keyword">else</span> &#123;          state        &#125;      &#125;      <span class="hljs-comment">//开始做wordcount,并打印输出</span>      <span class="hljs-keyword">val</span> wordDStream: <span class="hljs-type">DStream</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] = inputDStream.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))        .map((_, <span class="hljs-number">1</span>))        .updateStateByKey(updateFunc)      wordDStream.print()    ssc    &#125;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> ssc: <span class="hljs-type">StreamingContext</span> = <span class="hljs-type">StreamingContext</span>.getOrCreate(<span class="hljs-type">CKP</span>, creatingFunc)    <span class="hljs-comment">//启动流式应用</span>    ssc.start()    <span class="hljs-comment">//让应用一直处于监听状态</span>    ssc.awaitTermination()    <span class="hljs-comment">//合理关闭流式应用</span>    ssc.stop(<span class="hljs-literal">true</span>, <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><h3 id="SparkStreaming-amp-Kafka"><a href="#SparkStreaming-amp-Kafka" class="headerlink" title="SparkStreaming &amp; Kafka"></a>SparkStreaming &amp; Kafka</h3><ul><li>自动提交 Offset</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">InputDStream</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="hljs-type">ConsumerStrategies</span>, <span class="hljs-type">KafkaUtils</span>, <span class="hljs-type">LocationStrategies</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-keyword">import</span> scala.collection.mutable.<span class="hljs-type">Set</span><span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: Spark  Kafka自动提交offset</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S1KafkaAutoCommit</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))    <span class="hljs-comment">//准备kafka连接参数</span>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">true</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span>    )    <span class="hljs-keyword">val</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](      ssc,      <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,      <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)    )    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span>    <span class="hljs-keyword">val</span> resutDStream: <span class="hljs-type">DStream</span>[<span class="hljs-type">Unit</span>] = kafkaDStream.map(x =&gt; &#123;      println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)    &#125;)    <span class="hljs-comment">//打印数据</span>    resutDStream.print()    <span class="hljs-comment">//启动并停留</span>    ssc.start()    ssc.awaitTermination()    <span class="hljs-comment">//合理化关闭</span>    ssc.stop(stopSparkContext = <span class="hljs-literal">true</span>, stopGracefully = <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><ul><li>手动提交 Offset</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">InputDStream</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="hljs-type">CanCommitOffsets</span>, <span class="hljs-type">ConsumerStrategies</span>, <span class="hljs-type">HasOffsetRanges</span>, <span class="hljs-type">KafkaUtils</span>, <span class="hljs-type">LocationStrategies</span>, <span class="hljs-type">OffsetRange</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 topic</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S2KafkaCommit</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))    <span class="hljs-comment">//准备kafka连接参数</span>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">false</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span>    )    <span class="hljs-keyword">val</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](      ssc,      <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,      <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)    )    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span>    kafkaDStream.foreachRDD(rdd =&gt; &#123;      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;        <span class="hljs-comment">//对每个批次进行处理</span>        <span class="hljs-comment">//提取并打印偏移量范围信息</span>        <span class="hljs-keyword">val</span> hasOffsetRanges: <span class="hljs-type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>]        <span class="hljs-keyword">val</span> offsetRanges: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>] = hasOffsetRanges.offsetRanges        println(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)        offsetRanges.foreach(println(_))        <span class="hljs-comment">//打印每个批次的具体信息</span>        rdd.foreach(x =&gt; &#123;          println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)        &#125;)        <span class="hljs-comment">//手动将偏移量访问信息提交到默认主题</span>        kafkaDStream.asInstanceOf[<span class="hljs-type">CanCommitOffsets</span>].commitAsync(offsetRanges)        println(<span class="hljs-string">&quot;成功提交了偏移量信息&quot;</span>)      &#125;    &#125;)    <span class="hljs-comment">//启动并停留</span>    ssc.start()    ssc.awaitTermination()    <span class="hljs-comment">//合理化关闭</span>    ssc.stop(<span class="hljs-literal">true</span>,   <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><ul><li><p>手动提交 Offset 到 MySQL</p><ul><li>S3KafkaOffsetToMysql</li></ul>  <div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><span class="hljs-keyword">import</span> org.apache.kafka.common.<span class="hljs-type">TopicPartition</span><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.<span class="hljs-type">InputDStream</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010._<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-keyword">import</span> scala.collection.mutable<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 MySQL</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S3KafkaOffsetToMysql</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))    <span class="hljs-comment">//准备kafka连接参数</span>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">false</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span>    )    <span class="hljs-comment">//去MySQL查询上次消费的位置</span>    <span class="hljs-keyword">val</span> offsetMap: mutable.<span class="hljs-type">Map</span>[<span class="hljs-type">TopicPartition</span>, <span class="hljs-type">Long</span>] = <span class="hljs-type">OffsetUtil</span>.getOffsetMap(<span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-string">&quot;spark_kafka&quot;</span>)    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span>    <span class="hljs-keyword">var</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-literal">null</span>    <span class="hljs-comment">//第一次查询, MySQL没有 offset 数据</span>    <span class="hljs-keyword">if</span> (offsetMap.isEmpty) &#123;      kafkaDStream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](        ssc,        <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,        <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams) <span class="hljs-comment">//第一次就看 Kafka 发啥</span>      )    &#125;    <span class="hljs-comment">//第二次查询, MySQL中有 offset 数据</span>    <span class="hljs-keyword">else</span> &#123;      kafkaDStream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](        ssc,        <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,        <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams, offsetMap) <span class="hljs-comment">//第二次开始就从 MySQL 获取</span>      )    &#125;    <span class="hljs-comment">//对每个批次进行处理</span>    kafkaDStream.foreachRDD(rdd =&gt; &#123;      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;        <span class="hljs-comment">//提取并打印偏移量范围信息</span>        <span class="hljs-keyword">val</span> hasOffsetRanges: <span class="hljs-type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>]        <span class="hljs-keyword">val</span> offsetRanges: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>] = hasOffsetRanges.offsetRanges        println(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)        offsetRanges.foreach(println(_))        <span class="hljs-comment">//打印每个批次的具体信息</span>        rdd.foreach(x =&gt; &#123;          println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)        &#125;)        <span class="hljs-comment">//手动将偏移量访问信息提交到MySQL</span>        <span class="hljs-type">OffsetUtil</span>.saveOffsetRanges(<span class="hljs-string">&quot;spark&quot;</span>, offsetRanges)        println(<span class="hljs-string">&quot;成功提交了偏移量到MySQL&quot;</span>)      &#125;    &#125;)    <span class="hljs-comment">//启动并停留</span>    ssc.start()    ssc.awaitTermination()    <span class="hljs-comment">//合理化关闭</span>    ssc.stop(stopSparkContext = <span class="hljs-literal">true</span>, stopGracefully = <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><ul><li>OffsetUtil</li></ul>  <div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.common.<span class="hljs-type">TopicPartition</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.<span class="hljs-type">OffsetRange</span><span class="hljs-keyword">import</span> scala.collection.mutable.<span class="hljs-type">Map</span><span class="hljs-keyword">import</span> java.sql.&#123;<span class="hljs-type">DriverManager</span>, <span class="hljs-type">ResultSet</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: 定义一个单例对象, 定义 2 个方法</span><span class="hljs-comment"> *        方法1: 从 MySQL 读取行偏移量</span><span class="hljs-comment"> *        方法2: 将行偏移量保存的 MySQL</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">OffsetUtil</span> </span>&#123;  <span class="hljs-comment">/**</span><span class="hljs-comment">   * 定义一个单例方法, 将偏移量保存到MySQL数据库</span><span class="hljs-comment">   *</span><span class="hljs-comment">   * @param groupid     消费者组id</span><span class="hljs-comment">   * @param offsetRange 行偏移量对象</span><span class="hljs-comment">   */</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">saveOffsetRanges</span></span>(groupid: <span class="hljs-type">String</span>, offsetRange: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>]) = &#123;    <span class="hljs-keyword">val</span> connection = <span class="hljs-type">DriverManager</span>.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>)    <span class="hljs-comment">//replace into表示之前有就替换,没有就插入</span>    <span class="hljs-keyword">val</span> ps = connection.prepareStatement(<span class="hljs-string">&quot;replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)&quot;</span>)    <span class="hljs-keyword">for</span> (o &lt;- offsetRange) &#123;      ps.setString(<span class="hljs-number">1</span>, o.topic)      ps.setInt(<span class="hljs-number">2</span>, o.partition)      ps.setString(<span class="hljs-number">3</span>, groupid)      ps.setLong(<span class="hljs-number">4</span>, o.untilOffset)      ps.executeUpdate()    &#125;    ps.close()    connection.close()  &#125;  <span class="hljs-comment">/**</span><span class="hljs-comment">   * 定义一个方法, 用于从 MySQL 中读取行偏移位置</span><span class="hljs-comment">   * @param groupid 消费者组id</span><span class="hljs-comment">   * @param topic   想要消费的数据主题</span><span class="hljs-comment">   */</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getOffsetMap</span></span>(groupid: <span class="hljs-type">String</span>, topic: <span class="hljs-type">String</span>) = &#123;    <span class="hljs-comment">//1.从数据库查询对应数据</span>    <span class="hljs-keyword">val</span> connection = <span class="hljs-type">DriverManager</span>.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>)    <span class="hljs-keyword">val</span> ps = connection.prepareStatement(<span class="hljs-string">&quot;select * from t_offset where groupid=?  and topic=?&quot;</span>)    ps.setString(<span class="hljs-number">1</span>, groupid)    ps.setString(<span class="hljs-number">2</span>, topic)    <span class="hljs-keyword">val</span> rs: <span class="hljs-type">ResultSet</span> = ps.executeQuery()    <span class="hljs-comment">//解析数据, 返回</span>    <span class="hljs-keyword">var</span> offsetMap = <span class="hljs-type">Map</span>[<span class="hljs-type">TopicPartition</span>, <span class="hljs-type">Long</span>]()    <span class="hljs-keyword">while</span> (rs.next()) &#123;      <span class="hljs-keyword">val</span> topicPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">TopicPartition</span>(rs.getString(<span class="hljs-string">&quot;topic&quot;</span>), rs.getInt(<span class="hljs-string">&quot;partition&quot;</span>))      offsetMap.put(topicPartition, (rs.getLong(<span class="hljs-string">&quot;offset&quot;</span>)))    &#125;    rs.close()    rs.close()    connection.close()    offsetMap  &#125;&#125;</code></pre></div></li></ul><h3 id="StructuredStreaming"><a href="#StructuredStreaming" class="headerlink" title="StructuredStreaming"></a>StructuredStreaming</h3><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.structuredStreaming<span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">Row</span>, <span class="hljs-type">SparkSession</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.sql.streaming.<span class="hljs-type">StreamingQuery</span><span class="hljs-keyword">import</span> org.apache.spark.sql.types.&#123;<span class="hljs-type">IntegerType</span>, <span class="hljs-type">StringType</span>, <span class="hljs-type">StructField</span>, <span class="hljs-type">StructType</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: wordcount 案例 之 读取文件</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S2StructuredStreamingTextFile</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//1.创建上下文对象</span>    <span class="hljs-keyword">val</span> spark: <span class="hljs-type">SparkSession</span> = <span class="hljs-type">SparkSession</span>.builder()      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)      .getOrCreate()    <span class="hljs-comment">//2.读取csv, 得到流式DataFrame, 每行就是每批次的行数据</span>    <span class="hljs-comment">//自定义 Schema 信息</span>    <span class="hljs-keyword">val</span> schema = <span class="hljs-keyword">new</span> <span class="hljs-type">StructType</span>(<span class="hljs-type">Array</span>(      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-type">StringType</span>),      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;age&quot;</span>, <span class="hljs-type">IntegerType</span>),      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;hobby&quot;</span>, <span class="hljs-type">StringType</span>))    )    <span class="hljs-keyword">val</span> inputDF: <span class="hljs-type">DataFrame</span> = spark.readStream      .format(<span class="hljs-string">&quot;csv&quot;</span>)      .option(<span class="hljs-string">&quot;sep&quot;</span>, <span class="hljs-string">&quot;;&quot;</span>)      .schema(schema)      .load(<span class="hljs-string">&quot;src/main/data/input/persons&quot;</span>)    <span class="hljs-comment">//3.进行wordcount, DSL风格</span>    inputDF.printSchema()    <span class="hljs-comment">//用 DSL 风格实现</span>    <span class="hljs-keyword">import</span> spark.implicits._    <span class="hljs-keyword">val</span> <span class="hljs-type">DF</span>: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = inputDF.where(<span class="hljs-string">&quot;age&lt;25&quot;</span>)      .groupBy(<span class="hljs-string">&quot;hobby&quot;</span>)      .count()      .orderBy($<span class="hljs-string">&quot;count&quot;</span>.desc)    <span class="hljs-comment">// 用 SQL 风格实现</span>    inputDF.createOrReplaceTempView(<span class="hljs-string">&quot;t_spark&quot;</span>)    <span class="hljs-keyword">val</span> <span class="hljs-type">DF2</span>: <span class="hljs-type">DataFrame</span> = spark.sql(      <span class="hljs-string">&quot;&quot;&quot;</span><span class="hljs-string">        |select</span><span class="hljs-string">        |hobby,</span><span class="hljs-string">        |count(1) as cnt</span><span class="hljs-string">        |from t_spark</span><span class="hljs-string">        |where age&lt;25</span><span class="hljs-string">        |group by hobby</span><span class="hljs-string">        |order by cnt desc</span><span class="hljs-string">        |&quot;&quot;&quot;</span>.stripMargin)    <span class="hljs-keyword">val</span> query: <span class="hljs-type">StreamingQuery</span> = <span class="hljs-type">DF</span>.writeStream      <span class="hljs-comment">//append 默认追加 输出新的数据, 只支持简单查询, 有聚合就不能使用</span>      <span class="hljs-comment">//complete:完整模式, 输出完整数据, 支持集合和排序</span>      <span class="hljs-comment">//update: 更新模式, 输出有更新的数据,  支持聚合但是不支持排序</span>      .outputMode(<span class="hljs-string">&quot;complete&quot;</span>)      .format(<span class="hljs-string">&quot;console&quot;</span>)      .option(<span class="hljs-string">&quot;rowNumber&quot;</span>, <span class="hljs-number">10</span>)      .option(<span class="hljs-string">&quot;truncate&quot;</span>, <span class="hljs-literal">false</span>)      <span class="hljs-comment">//4.启动流式查询</span>      .start()    <span class="hljs-comment">//5.驻留监听</span>    query.awaitTermination()    <span class="hljs-comment">//6.关闭流式查询</span>    query.stop()  &#125;&#125;</code></pre></div><h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="批处理-DataSet"><a href="#批处理-DataSet" class="headerlink" title="批处理 DataSet"></a>批处理 DataSet</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.flink.start;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.operators.Order;<span class="hljs-keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;<span class="hljs-keyword">import</span> org.apache.flink.api.java.operators.*;<span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span>: Jface</span><span class="hljs-comment"> * <span class="hljs-doctag">@Date</span>: 2021/9/5 12:37</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 基于Flink引擎实现批处理词频统计WordCount：过滤filter、排序sort等操作</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_01WordCount</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//1.准备环境-env</span>        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();        <span class="hljs-comment">//2.准备数据-source</span>        DataSource&lt;String&gt; inputDataSet = env.readTextFile(<span class="hljs-string">&quot;datas/wc.input&quot;</span>);        <span class="hljs-comment">//3.处理数据-transformation</span>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.1 过滤脏数据</span>        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataSet.filter(<span class="hljs-keyword">new</span> FilterFunction&lt;String&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">filter</span><span class="hljs-params">(String line)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span> != line &amp;&amp; line.trim().length() &gt; <span class="hljs-number">0</span>;            &#125;        &#125;)                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.2 切割</span>                .flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;                    <span class="hljs-meta">@Override</span>                    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String line, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                        <span class="hljs-keyword">for</span> (String s : line.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>)) &#123;                            out.collect(s);                        &#125;                    &#125;                &#125;)                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.3 转换二元组</span>                .map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;                    <span class="hljs-meta">@Override</span>                    <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String word)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                        <span class="hljs-keyword">return</span> Tuple2.of(word, <span class="hljs-number">1</span>);                    &#125;                &#125;)                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.4 分组求和</span>                .groupBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);        <span class="hljs-comment">//4.输出结果-sink</span>        resultDataSet.printToErr();        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> sort 排序，全局排序需要设置分区数 1</span>        SortPartitionOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sortDataSet = resultDataSet.sortPartition(<span class="hljs-string">&quot;f1&quot;</span>, Order.DESCENDING)                .setParallelism(<span class="hljs-number">1</span>);        sortDataSet.printToErr();        <span class="hljs-comment">//只选择前3的数据</span>        GroupReduceOperator&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; resultDataSet2 = sortDataSet.first(<span class="hljs-number">3</span>);        resultDataSet2.print();        <span class="hljs-comment">//5.触发执行-execute，没有写出不需要触发执行</span>    &#125;&#125;</code></pre></div><h3 id="流处理-DataStream"><a href="#流处理-DataStream" class="headerlink" title="流处理 DataStream"></a>流处理 DataStream</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.stream;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现实时流式数据处理，监听端口并做 wordcount</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StreamWordcount</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;         <span class="hljs-comment">//1.准备环境-env</span>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();         <span class="hljs-comment">//2.准备数据-source</span>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);        <span class="hljs-comment">//3.处理数据-transformation</span>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);                <span class="hljs-keyword">for</span> (String s : arr) &#123;                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span>                &#125;            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> Tuple2.of(value,<span class="hljs-number">1</span>);            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);        <span class="hljs-comment">//4.输出结果-sink</span>        resultDataSet.print();        <span class="hljs-comment">//5.触发执行-execute</span>        env.execute(StreamWordcount.class.getSimpleName());    &#125;&#125;</code></pre></div><p>流处理 Flink On Yarn</p><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.submit;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<span class="hljs-keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现流式数据处理，从socket 接收数据并做 wordcount</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Wordcount</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//0.使用工具类，解析程序传递参数</span>        ParameterTool parameterTool = ParameterTool.fromArgs(args);        <span class="hljs-keyword">if</span> (parameterTool.getNumberOfParameters() != <span class="hljs-number">2</span>) &#123;            System.out.println(<span class="hljs-string">&quot;Usage: WordCount --host &lt;host&gt; --port &lt;port&gt; ............&quot;</span>);            System.exit(-<span class="hljs-number">1</span>);        &#125;        String host = parameterTool.get(<span class="hljs-string">&quot;host&quot;</span>);        parameterTool.getInt(<span class="hljs-string">&quot;port&quot;</span>, <span class="hljs-number">9999</span>);        <span class="hljs-comment">//1.准备环境-env</span>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        <span class="hljs-comment">//2.准备数据-source</span>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);        <span class="hljs-comment">//3.处理数据-transformation</span>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataStream = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);                <span class="hljs-keyword">for</span> (String s : arr) &#123;                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span>                &#125;            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> Tuple2.of(value, <span class="hljs-number">1</span>);            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);        <span class="hljs-comment">//4.输出结果-sink</span>        resultDataStream.print();        <span class="hljs-comment">//5.触发执行-execute</span>        env.execute(Wordcount.class.getSimpleName());    &#125;&#125;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>Hadoop</tag>
      
      <tag>Scala</tag>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>管理配置文件的工具：Commons Configuration</title>
    <link href="/2020/10/20/%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%B7%A5%E5%85%B7CommonsConfiguration/"/>
    <url>/2020/10/20/%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%B7%A5%E5%85%B7CommonsConfiguration/</url>
    
    <content type="html"><![CDATA[<p>一般读取配置文件，或者说集群环境传参的方式有如下几种：</p><p>1、Main 程序留出参数入口，通过 args 接收参数，运行 jar 的时候传入参数</p><p>2、将配置文件放入 resources ，通过类加载器获取参数文件，或者创建专门工具类读取resources 中的配置文件信息</p><p>这两种方法各有优缺点，第一种虽然修改参数非常方便，但是当需要指定的参数较多时会繁琐；</p><p>第二种方式将配置文件一起打成 jar 包，当需要修改参数信息的时需要重写打包，也非常繁琐。</p><p>最近项目当中使用了一种的新的工具：Apache Commons Configuration，完美解决了我的需求</p><h2 id="Commons-Configuration-基本介绍"><a href="#Commons-Configuration-基本介绍" class="headerlink" title="Commons Configuration 基本介绍"></a>Commons Configuration 基本介绍</h2><p>Commons Configuration 软件库提供了一个通用配置接口，它使 Java 应用程序能够从各种来源读取配置数据。</p><p>支持各种格式的配置文件</p><ul><li>Properties files</li><li>XML documents</li><li>Windows INI files</li><li>Property list files (plist)</li><li>JNDI</li><li>JDBC Datasource</li><li>System properties</li><li>Applet parameters</li><li>Servlet parameters</li></ul><p>官网：<a href="https://commons.apache.org/proper/commons-configuration/">https://commons.apache.org/proper/commons-configuration/</a> </p><h3 id="Maven-依赖"><a href="#Maven-依赖" class="headerlink" title="Maven 依赖"></a>Maven 依赖</h3><div class="code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span>    <span class="hljs-comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.commons/commons-configuration2 --&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.commons<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>commons-configuration2<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.2<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>commons-beanutils<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>commons-beanutils<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.9.3<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span></code></pre></div><h3 id="新建-test-propertis-文件"><a href="#新建-test-propertis-文件" class="headerlink" title="新建 test.propertis 文件"></a>新建 test.propertis 文件</h3><div class="code-wrapper"><pre><code class="hljs ini"><span class="hljs-comment">### common configuration start</span><span class="hljs-attr">database.host</span> = db.acme.com<span class="hljs-attr">database.port</span> = <span class="hljs-number">8199</span><span class="hljs-attr">database.user</span> = admin<span class="hljs-attr">database.password</span> = ???<span class="hljs-attr">database.timeout</span> = <span class="hljs-number">60000</span><span class="hljs-comment">###common configuration end</span></code></pre></div><h3 id="读取测试"><a href="#读取测试" class="headerlink" title="读取测试"></a>读取测试</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.commons.configuration2.Configuration;<span class="hljs-keyword">import</span> org.apache.commons.configuration2.builder.fluent.Configurations;<span class="hljs-keyword">import</span> org.apache.commons.configuration2.ex.ConfigurationException;<span class="hljs-keyword">import</span> java.io.File;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span>: Jface</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 测试 Commons Configuration 的使用</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PropertiesFile</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">//1.初始化配置文件</span>        Configurations configs = <span class="hljs-keyword">new</span> Configurations();        Configuration config = <span class="hljs-keyword">null</span>;        <span class="hljs-comment">//2.读取配置文件内容</span>        <span class="hljs-keyword">try</span> &#123;            config = configs.properties(<span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;commons_configuration/src/main/resources/test.properties&quot;</span>));        &#125; <span class="hljs-keyword">catch</span> (ConfigurationException cex) &#123;            cex.printStackTrace();        &#125;        <span class="hljs-comment">//2.获取使用配置文件信息,打印测试</span>        System.out.println(config.getString(<span class="hljs-string">&quot;database.host&quot;</span>));    &#125;&#125;</code></pre></div><p>参考资料</p><p><a href="https://www.jianshu.com/p/625e833c1a49">https://www.jianshu.com/p/625e833c1a49</a> </p><p><a href="https://blog.csdn.net/wanghantong/article/details/79072474">https://blog.csdn.net/wanghantong/article/details/79072474</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>更好的日志框架：logback</title>
    <link href="/2020/10/10/%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/"/>
    <url>/2020/10/10/%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>Logback 是由 log4j 创始人设计的另一个开源日志组件</p><p>官方网站： <a href="http://logback.qos.ch/">http://logback.qos.ch</a> </p><p>它分为下面下个模块：</p><ul><li>logback-core：其它两个模块的基础模块</li><li>logback-classic：它是 log4j 的一个改良版本，同时它完整实现了 slf4j API使你可以很方便地更换成其它日志系统如 log4j 或 JDK14 Logging</li><li>logback-access：访问模块与 Servlet 容器集成提供通过 Http 来访问日志的功能</li></ul><h3 id="配置文件命名"><a href="#配置文件命名" class="headerlink" title="配置文件命名"></a>配置文件命名</h3><p>根据不同的日志系统，命名规则不尽相同</p><ul><li>Logback：<code>logback-spring.xml, logback-spring.groovy, logback.xml, logback.groovy</code></li><li>Log4j：<code>log4j-spring.properties, log4j-spring.xml, log4j.properties, log4j.xml</code></li><li>Log4j2：<code>log4j2-spring.xml, log4j2.xml</code></li><li>JDK (Java Util Logging)：<code>logging.properties</code></li></ul><h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><div class="code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">log4j.version</span>&gt;</span>1.7.7<span class="hljs-tag">&lt;/<span class="hljs-name">log4j.version</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">logback.version</span>&gt;</span>1.1.3<span class="hljs-tag">&lt;/<span class="hljs-name">logback.version</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span><span class="hljs-comment">&lt;!-- log4j日志 start--&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>ch.qos.logback<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>logback-core<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>ch.qos.logback<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>logback-classic<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.slf4j<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>log4j-over-slf4j<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;log4j.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-comment">&lt;!-- log4j日志 end--&gt;</span></code></pre></div><h3 id="配置文件内容（以-logback-xml-为例）"><a href="#配置文件内容（以-logback-xml-为例）" class="headerlink" title="配置文件内容（以 logback.xml 为例）"></a>配置文件内容（以 logback.xml 为例）</h3><div class="code-wrapper"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>    <span class="hljs-comment">&lt;!--http://logback.qos.ch/manual/configuration.html--&gt;</span>    <span class="hljs-comment">&lt;!--设置日志输出到控制台--&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">appender</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;STDOUT&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;ch.qos.logback.core.ConsoleAppender&quot;</span>&gt;</span>        <span class="hljs-comment">&lt;!-- encoder的默认实现类是ch.qos.logback.classic.encoder.PatternLayoutEncoder --&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">encoder</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">pattern</span>&gt;</span>%date %level [%thread] %logger&#123;10&#125; [%file:%line] %msg%n<span class="hljs-tag">&lt;/<span class="hljs-name">pattern</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">encoder</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">appender</span>&gt;</span>    <span class="hljs-comment">&lt;!-- name值可以是包名或具体的类名：该包（包括子包）下的类或该类将采用此logger --&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">logger</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;com.test.flink&quot;</span> <span class="hljs-attr">level</span>=<span class="hljs-string">&quot;INFO&quot;</span> <span class="hljs-attr">additivity</span>=<span class="hljs-string">&quot;false&quot;</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">appender-ref</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;STDOUT&quot;</span> /&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">logger</span>&gt;</span>    <span class="hljs-comment">&lt;!--设置日志输出为文件--&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">appender</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;FILE&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;ch.qos.logback.core.FileAppender&quot;</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">file</span>&gt;</span>warn.log<span class="hljs-tag">&lt;/<span class="hljs-name">file</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">encoder</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">pattern</span>&gt;</span>%date %level [%thread] %logger&#123;10&#125; [%file:%line] %msg%n<span class="hljs-tag">&lt;/<span class="hljs-name">pattern</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">encoder</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">appender</span>&gt;</span>    <span class="hljs-comment">&lt;!--日志输出级别--&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">root</span> <span class="hljs-attr">level</span>=<span class="hljs-string">&quot;WARN&quot;</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">appender-ref</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;STDOUT&quot;</span> /&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">appender-ref</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;FILE&quot;</span> /&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">root</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre></div><h3 id="使用-logback"><a href="#使用-logback" class="headerlink" title="使用 logback"></a>使用 logback</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">import</span> org.slf4j.Logger;<span class="hljs-keyword">import</span> org.slf4j.LoggerFactory;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLogback</span> </span>&#123;  <span class="hljs-keyword">final</span> <span class="hljs-keyword">static</span> Logger logger = LoggerFactory.getLogger(TestLogback.class);  <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;    logger.info(<span class="hljs-string">&quot;Test logback &quot;</span>);  &#125;&#125;</code></pre></div><h3 id="参考连接"><a href="#参考连接" class="headerlink" title="参考连接"></a>参考连接</h3><p><a href="https://www.jianshu.com/p/638b4e2c4068">https://www.jianshu.com/p/638b4e2c4068</a> </p><p><a href="https://www.cnblogs.com/warking/p/5710303.html">https://www.cnblogs.com/warking/p/5710303.html</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>Flink</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Spark：分布式并行计算框架（三）</title>
    <link href="/2020/10/03/Spark-Review-Day03/"/>
    <url>/2020/10/03/Spark-Review-Day03/</url>
    
    <content type="html"><![CDATA[<h2 id="Spark-on-Hive"><a href="#Spark-on-Hive" class="headerlink" title="Spark on Hive"></a>Spark on Hive</h2><blockquote><p>面试题：<code>spark on hive和hive on spark</code>区别？？？？</p></blockquote><p><img src="https://i.loli.net/2021/09/09/ZIij3s9rePu7VOM.png" alt="1630310212506"></p><blockquote><p>典型<strong>基于Spark和Hive离线数仓</strong>架构技术图，简易版本：</p></blockquote><div class="code-wrapper"><pre><code class="hljs 1c"><span class="hljs-number">1</span>、SparkSQL分析数据<span class="hljs-number">2</span>、Hive 管理元数据<span class="hljs-string">|</span>Spark on Hive 架构，离线数据仓库分析</code></pre></div><p><img src="https://i.loli.net/2021/09/09/GYA4PBNx5oZV8aT.png" alt="1630309369960"></p><div class="code-wrapper"><pre><code class="hljs css">SparkSQL与Hive集成，本质就是Spark Application应用程序，读取加载HiveMetaStore元数据。database数据库<span class="hljs-selector-tag">table</span>表字段分区.....</code></pre></div><blockquote><p>​        Spark Thrift Server将Spark Applicaiton当做一个服务运行，提供Beeline客户端和JDBC方式访问，与Hive中<strong>HiveServer2</strong>服务一样的</p></blockquote><p><img src="https://i.loli.net/2021/09/09/S6XtOoECMzY91Fq.png" alt="1630310389289"></p><div class="code-wrapper"><pre><code class="hljs ini">1、启动MySQL数据库存储Hive 元数据MetaStore2、启动HiveMetaStore服务提供提供元数据服务，读取到Hive中管理元数据信息3、启动HDFS文件系统Hive表中数据存储在HDFS服务4、启动Spark Thrift JDBC/ODBC server将Spark Application当做服务启动，通过JDBC或ODBC方式连接，也可以beeline 命令行连接编写DDL和DML语句，操作管理数据与Hive集成，启动服务时，让其连接到HiveMetaStore服务在SPARK_HOME/conf创建hive-site.xml文件，添加Hive MetaStore地址信息即可。&lt;?xml <span class="hljs-attr">version</span>=<span class="hljs-string">&quot;1.0&quot;</span>?&gt;&lt;?xml-stylesheet <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;hive.metastore.uris&lt;/name&gt;        &lt;value&gt;thrift://node1:9083&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;启动服务/export/server/spark/sbin/start-thriftserver.sh \--hiveconf <span class="hljs-attr">hive.server2.thrift.port</span>=<span class="hljs-number">10000</span> \--hiveconf <span class="hljs-attr">hive.server2.thrift.bind.host</span>=node1 \--master local<span class="hljs-section">[2]</span> \--conf <span class="hljs-attr">spark.sql.shuffle.partitions</span>=<span class="hljs-number">2</span> \--conf <span class="hljs-attr">spark.serializer</span>=org.apache.spark.serializer.KryoSerializer5、beeline客户端连接/export/server/spark/bin/beelinebeeline&gt; !connect jdbc:hive2://node1:10000</code></pre></div><h2 id="Spark-内存管理"><a href="#Spark-内存管理" class="headerlink" title="Spark 内存管理"></a>Spark 内存管理</h2><blockquote><p>知识点：Spark Application运行架构</p></blockquote><p><img src="https://i.loli.net/2021/09/09/jKbPa9UTdv3VS7r.png" alt="Spark cluster components"></p><div class="code-wrapper"><pre><code class="hljs ada"><span class="hljs-number">1</span>、Driver Program创建SparkContext实例对象申请资源运行Executor、调度Job执行<span class="hljs-number">2</span>、Executors相当于线程池，里面执行<span class="hljs-keyword">Task</span>任务（以线程方式运行），每个<span class="hljs-keyword">Task</span>任务运行需要<span class="hljs-number">1</span>CoreCPU执行<span class="hljs-keyword">Task</span>任务、缓存RDD数据每个Executor资源：内存Memeory、CPUCore核数 -&gt; 执行<span class="hljs-keyword">Task</span>任务内存Memory主要被用于<span class="hljs-number">2</span>个部分：<span class="hljs-keyword">Task</span>任务执行时，需要内存Cache缓存数据，需要内存</code></pre></div><p><img src="https://i.loli.net/2021/09/09/FoNEep87UHqLACx.png" alt="1631180184608"></p><blockquote><p>在运行Spark Application时，需要设置如下三个参数：</p></blockquote><p><img src="https://i.loli.net/2021/09/09/4BqXng9zNCmsMx1.png" alt="1631180387861"></p><blockquote><p>Executor内存管理，从发展来说，经历2个阶段：</p></blockquote><ul><li>1）、Spark 1.6之前，==内存管理【静态内存管理】==<ul><li>直接将内存划分比例，存储Storage内存占比多少和执行Execution内存占比多少，固定死了</li><li>此种内存管理，存在不合理，比如内存浪费</li></ul></li><li>2）、Spark 1.6开始，提供：==统一内存管理（动态内存管理）==，到Spark 2.0时，仅仅支持此种管理方案<ul><li>约定：Storage存储和Execution执行内存占比，默认各占50%</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/09/eQTLtw7WhAFbiUn.png" alt="1630315362244"></p><ul><li>动态彼此相互借用<ul><li>如果Storage存储内存不足，但是Execution内存多余，可以借用存储数据，反之亦然</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/09/nQbt9yAMRCfdjT4.png" alt="1630315328271"></p><blockquote><p>Executor内存划分，包含四个部分，如下图所示：</p></blockquote><div class="code-wrapper"><pre><code class="hljs stylus"><span class="hljs-number">1</span>、预留内存：<span class="hljs-number">300</span>MBJVM进程自己使用，不允许用户使用<span class="hljs-number">2</span>、可用内存（UsableMemory）UsableMemory =  --executor-memory    -    <span class="hljs-number">300</span>MB<span class="hljs-number">1</span>）、UserMemory 用户内存可用内存占比：(<span class="hljs-number">1</span> - spark<span class="hljs-selector-class">.memory</span>.fraction) = <span class="hljs-number">0.4</span>可用内存占比： spark<span class="hljs-selector-class">.memory</span><span class="hljs-selector-class">.fraction</span> = <span class="hljs-number">0.6</span><span class="hljs-number">2</span>）、存储内存spark<span class="hljs-selector-class">.memory</span><span class="hljs-selector-class">.storageFraction</span> = <span class="hljs-number">0.5</span><span class="hljs-number">3</span>）、执行内存<span class="hljs-number">1</span>- spark<span class="hljs-selector-class">.memory</span><span class="hljs-selector-class">.storageFraction</span> = <span class="hljs-number">0.5</span></code></pre></div><p><img src="https://i.loli.net/2021/09/09/JP8fBFtXqz2srjL.png" alt="1630309139547"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Spark：分布式并行计算框架（二）</title>
    <link href="/2020/09/28/Spark-Review-Day02/"/>
    <url>/2020/09/28/Spark-Review-Day02/</url>
    
    <content type="html"><![CDATA[<h2 id="1、Spark-有哪些优化"><a href="#1、Spark-有哪些优化" class="headerlink" title="1、Spark 有哪些优化"></a>1、Spark 有哪些优化</h2><h3 id="第一、公共优化"><a href="#第一、公共优化" class="headerlink" title="第一、公共优化"></a>第一、公共优化</h3><blockquote><p>1、序列化（<code>Serialization</code>）</p></blockquote><div class="code-wrapper"><pre><code class="hljs mipsasm">Spark中默认序列化方式：<span class="hljs-keyword">Java </span>序列化（<span class="hljs-keyword">Java </span>serialization）要求数据类型必须实现序列化接口Serializable，比如HBase数据库读取数据时，封装到Result设置序列化为：Kryo 序列化比<span class="hljs-keyword">Java序列化性能提升10倍以上</span><span class="hljs-keyword"></span>设置：conf<span class="hljs-meta">.set</span>(<span class="hljs-string">&quot;spark.serializer&quot;</span>, <span class="hljs-string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</code></pre></div><blockquote><p>2、数据缓存（<code>Cache</code>）</p></blockquote><div class="code-wrapper"><pre><code class="hljs markdown">可以将RDD数据缓存，设置缓存级别StorageLevel<span class="hljs-code">要么内存、要么磁盘、要么内存和磁盘（是否序列化）、要么系统内存（可以不管）</span><span class="hljs-code"></span><span class="hljs-code">DataFrame和DataSet同样可以缓存，底层就是RDD</span><span class="hljs-code"></span>什么条件下缓存数据集呢？？？<span class="hljs-code">可以提升数据计算效率</span><span class="hljs-code">1. 数据集RDD被使用多次，考虑缓存，依据数据集大小合理设置缓存级别</span><span class="hljs-code">2. 数据集来之不易，并且使用不止一次</span></code></pre></div><blockquote><p>3、分区数目调整（<code>partition</code>）</p></blockquote><div class="code-wrapper"><pre><code class="hljs stylus">RDD分布式数据集，不可变的、分区的和并行计算集合，抽象的无论批处理还是流计算（微批处理）计算结果，往往数据量很小的，当保存到外部存储系统时，最好降低分区数目比如某日物流快递公司订单数据<span class="hljs-number">1000</span>W，按照省份统计<span class="hljs-number">34</span>条resultRDD<span class="hljs-selector-class">.coalease</span>(<span class="hljs-number">1</span>)<span class="hljs-selector-class">.forchePartition</span>将数据集保存外部系统时，最好针对每个分区数据操作</code></pre></div><h3 id="第二、批处理优化"><a href="#第二、批处理优化" class="headerlink" title="第二、批处理优化"></a>第二、批处理优化</h3><blockquote><p>基本上使用SparkSQL离线数据分析，考虑SparkSQL中哪些优化即可。</p></blockquote><div class="code-wrapper"><pre><code class="hljs llvm"><span class="hljs-number">1</span>、spark.sql.shuffle.partitions Shuffle时分区数目，默认值为<span class="hljs-number">200</span>，实际项目中可能太大，有可能太小Spark <span class="hljs-number">1</span>.<span class="hljs-keyword">x</span>和Spark <span class="hljs-number">2</span>.<span class="hljs-keyword">x</span>需要调整Spark <span class="hljs-number">3</span>.<span class="hljs-keyword">x</span>开始无需调整，新特性：自适应调整只能开启功能以后，SparkSQL程序在运行计算时，依据Shuffle数据量自动设置分区数目</code></pre></div><p><img src="https://i.loli.net/2021/09/07/52ERDKFvjp6qWkb.png" alt="图1"></p><div class="code-wrapper"><pre><code class="hljs pgsql"><span class="hljs-number">2</span>、广播变量大小调整SparkSQL在执行<span class="hljs-number">2</span>个表的数据<span class="hljs-keyword">Join</span>时，自动判断是否为大表<span class="hljs-keyword">JOIN</span>小表，如果有<span class="hljs-number">1</span>个小表的话，采用广播变量的方式广播数据，进行关联分析处理，避免产生Shuffle，提升性能。参数：spark.<span class="hljs-keyword">sql</span>.autoBroadcastJoinThreshold = <span class="hljs-number">10</span>M比如小表数据大小为<span class="hljs-number">12</span>M，完全可以采用广播<span class="hljs-keyword">JOIN</span>方式，此时调整 广播表里大小阈值</code></pre></div><p><img src="https://i.loli.net/2021/09/07/lFiBTV2c1WqhYOR.png" alt="图2"></p><h3 id="第三、流计算优化"><a href="#第三、流计算优化" class="headerlink" title="第三、流计算优化"></a>第三、流计算优化</h3><blockquote><p>要么使用SparkStreaming，要么使用StructuredStreaming分析数据，底层批处理（微批处理）。</p></blockquote><div class="code-wrapper"><pre><code class="hljs stylus"><span class="hljs-number">1</span>、限制高峰数据量限制每批次处理数据量，给定一个阈值，通过压力测试确定的值<span class="hljs-number">1</span>）、SparkStreaming参数：spark<span class="hljs-selector-class">.streaming</span><span class="hljs-selector-class">.kafka</span>.maxRatePerPartition</code></pre></div><p><img src="https://i.loli.net/2021/09/07/UAnNqGPmu4cKHtJ.png" alt="图3"></p><div class="code-wrapper"><pre><code class="hljs pf"><span class="hljs-number">2</span>）、StructuredStreaming参数：<span class="hljs-keyword">max</span>OffsetsPreTrigger，每次触发时，消费最大偏移量</code></pre></div><p><img src="https://i.loli.net/2021/09/07/A2txYP86wpjNGXO.png" alt="图4"></p><div class="code-wrapper"><pre><code class="hljs">针对SparkStreaming来说，为了更好处理流式数据，避免数据量太多，导致应用性能下降：反压机制原理，依据当前处理数据量和处理时间，决定下一批次处理数据量</code></pre></div><div class="code-wrapper"><pre><code class="hljs awk"><span class="hljs-number">2</span>、数据本地性等待时间https:<span class="hljs-regexp">//</span>spark.apache.org<span class="hljs-regexp">/docs/</span><span class="hljs-number">2.4</span>.<span class="hljs-number">5</span>/tuning.html<span class="hljs-comment">#data-locality</span></code></pre></div><p>​        <a href="https://www.cnblogs.com/cc11001100/p/10301716.html">https://www.cnblogs.com/cc11001100/p/10301716.html</a></p><div class="code-wrapper"><pre><code class="hljs ada"><span class="hljs-number">1</span>、PROCESS_LOCAL：进程本地性，<span class="hljs-keyword">Task</span>任务和Data数据在同一个JVM进程中顾名思义，要处理的数据就在同一个本地进程中，即数据和<span class="hljs-keyword">Task</span>在同一个Executor JVM中，这种情况就是RDD的数据在之前就已经被缓存过了，因为BlockManager是以Executor为单位的，所以只要<span class="hljs-keyword">Task</span>所需要的Block在所属的Executor的BlockManager上已经被缓存，这个数据本地性就是PROCESS_LOCAL，这种是最好的locality，这种情况下数据不需要在网络中传输。</code></pre></div><p><img src="https://i.loli.net/2021/09/07/vte8XGdYcxpCraw.png" alt="图5"></p><div class="code-wrapper"><pre><code class="hljs asciidoc">2、NODE<span class="hljs-emphasis">_LOCAL：</span><span class="hljs-emphasis">数据在同一台节点上，但是并不不在同一个jvm中，比如数据在同一台节点上的另外一个Executor上，速度要比PROCESS_LOCAL略慢。还有一种情况是读取HDFS的块就在当前节点上，数据本地性也是NODE_</span>LOCAL。</code></pre></div><p><img src="https://i.loli.net/2021/09/07/2T8MwE5Sin6V7UH.png" alt="图6"></p><div class="code-wrapper"><pre><code class="hljs asciidoc">3、NO_PREF：<span class="hljs-code">数据从哪里访问都一样，表示数据本地性无意义，其实指的是从MySQL、MongoDB之类的数据源读取数据。</span>4、RACK<span class="hljs-emphasis">_LOCAL：机架本地性</span><span class="hljs-emphasis">数据在同一机架上的其它节点，需要经过网络传输，速度要比NODE_</span>LOCAL慢。</code></pre></div><p><img src="https://i.loli.net/2021/09/07/t1VIFcsWBr4ZgPM.png" alt="图7"></p><div class="code-wrapper"><pre><code class="hljs fortran"><span class="hljs-number">5</span>、<span class="hljs-built_in">ANY</span>：数据在其它更远的网络上，甚至都不在同一个机架上，比RACK_LOCAL更慢，一般情况下不会出现这种级别，万一出现了可能是有什么异常需要排查下原因。</code></pre></div><blockquote><p>​        在流式数据处理计算中，设置数据本地性等待时间越小越好，不需要等待，直接使用某个数据即可，哪怕时最差：ANY都可以。</p></blockquote><div class="code-wrapper"><pre><code class="hljs fortran">参数：spark.locality.<span class="hljs-keyword">wait</span>，默认为值<span class="hljs-number">3</span>s可以设置为spark.locality.<span class="hljs-keyword">wait</span>=<span class="hljs-number">10</span>ms</code></pre></div><h2 id="2、SparkSQL-底层JOIN实现方式"><a href="#2、SparkSQL-底层JOIN实现方式" class="headerlink" title="2、SparkSQL 底层JOIN实现方式"></a>2、SparkSQL 底层JOIN实现方式</h2><blockquote><p>SparkSQL底层处理2个表JOIN时方式：3种，自动选择合理方式处理数据</p></blockquote><p>参考：<a href="https://www.cnblogs.com/JP6907/p/10721436.html">https://www.cnblogs.com/JP6907/p/10721436.html</a></p><p>对于Spark来说有3中Join的实现，每种Join对应着不同的应用场景：</p><ul><li>1、<strong>Broadcast Hash Join ： 适合一张较小的表和一张大表进行join</strong></li><li>2、<strong>Shuffle Hash Join :  适合一张小表和一张大表进行join，或者是两张小表之间的join</strong></li><li>3、<strong>Sort Merge Join ： 适合两张较大的表之间进行join</strong></li></ul><p><strong>前两者都基于的是Hash Join，只不过在hash join之前需要先shuffle还是先broadcast。</strong></p><h3 id="第一、Broadcast-Hash-Join"><a href="#第一、Broadcast-Hash-Join" class="headerlink" title="第一、Broadcast Hash Join"></a>第一、Broadcast Hash Join</h3><div class="code-wrapper"><pre><code class="hljs gradle">大表和小表进行<span class="hljs-keyword">JOIN</span>将小表数据采用广播变量的方式，把数据广播到大表的每个分区中，进行<span class="hljs-keyword">JOIN</span></code></pre></div><p><img src="https://i.loli.net/2021/09/07/yG1Qb5OcD7U3du8.png" alt="图8"></p><div class="code-wrapper"><pre><code class="hljs sql">Broadcast <span class="hljs-keyword">Join</span>的条件有以下几个：<span class="hljs-number">1</span>、被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的值，默认是<span class="hljs-number">10</span>M （或者加了broadcast <span class="hljs-keyword">join</span>的hint）<span class="hljs-number">2</span>、基表不能被广播，比如<span class="hljs-keyword">left</span> <span class="hljs-keyword">outer</span> <span class="hljs-keyword">join</span>时，只能广播右表</code></pre></div><h3 id="第二、Shuffle-Hash-Join"><a href="#第二、Shuffle-Hash-Join" class="headerlink" title="第二、Shuffle Hash Join"></a>第二、Shuffle Hash Join</h3><div class="code-wrapper"><pre><code class="hljs pf">大表与小表（数据量也挺大，相对大表来说，小一点）对<span class="hljs-number">2</span>个表的数据按照相同的字段进行重分区，并且分区数目相同Shuffle ：对<span class="hljs-number">2</span>个表的数据进行重分区，字段相同，分区数目相同<span class="hljs-built_in">table</span>A表<span class="hljs-number">1</span>个分区  仅仅   <span class="hljs-built_in">table</span>B表<span class="hljs-number">1</span>个分区  数据进行JOIN</code></pre></div><p><img src="https://i.loli.net/2021/09/07/nHlFXP6vsZEiqz1.png" alt="图9"></p><h3 id="第三、Sort-Merge-Join"><a href="#第三、Sort-Merge-Join" class="headerlink" title="第三、Sort Merge Join"></a>第三、Sort Merge Join</h3><div class="code-wrapper"><pre><code class="hljs vbnet">大表对大表类似Hive中SMB <span class="hljs-keyword">JOIN</span>，表的数据分区相同、分桶数目相同，并且桶数据排序的首先，<span class="hljs-number">2</span>张表进行重分区，分区字段<span class="hljs-keyword">key</span>相同，分区数目相同然后，对每个分区中数据进行排序最后，表对表的 分区数据进行<span class="hljs-keyword">JOIN</span>，此时关联时，不会扫描整个分区中数据</code></pre></div><p><img src="https://i.loli.net/2021/09/07/EYsfzkWZe4Hwl5t.png" alt="图10"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Spark：分布式并行计算框架（一）</title>
    <link href="/2020/09/22/Spark-Review-Day01/"/>
    <url>/2020/09/22/Spark-Review-Day01/</url>
    
    <content type="html"><![CDATA[<h2 id="0、前言说明"><a href="#0、前言说明" class="headerlink" title="0、前言说明"></a>0、前言说明</h2><p>整理和汇总一下 Spark 容易混淆的概念和理论。</p><h2 id="1、Spark-框架概念"><a href="#1、Spark-框架概念" class="headerlink" title="1、Spark 框架概念"></a>1、Spark 框架概念</h2><p><img src="https://i.loli.net/2021/09/06/LBadXZ5iyTtYvSg.png" alt="图1"></p><div class="code-wrapper"><pre><code class="hljs fortran">Apache Spark™ is a unified analytics engine for large-<span class="hljs-built_in">scale</span> <span class="hljs-keyword">data</span> processing.<span class="hljs-number">1</span>、unified 统一Spark 框架可以对任意业务需求进行数据分析批处理：SparkCore、交互式分析：SparkSQL、流式计算：SparkStreaming和StructuredStreaming图计算：SparkGraphX、机器学习：SparkMLlib数据科学：PySpark和SparkR<span class="hljs-number">2</span>、 large-<span class="hljs-built_in">scale</span> 大规模海量数据类似MapReduce对海量数据处理分析</code></pre></div><p><img src="https://i.loli.net/2021/09/06/h8eibz9poSO13Pu.png" alt="图2"></p><div class="code-wrapper"><pre><code class="hljs applescript">unified 统一不仅仅是分析，也可以指：Spark 分析数据时，可以时任意数据源SparkSQL提供一套外部数据源接口，任何数据源只要实现接口，可以读写数据<span class="hljs-built_in">read</span>或<span class="hljs-built_in">write</span></code></pre></div><h2 id="2、Spark与MapReduce相比"><a href="#2、Spark与MapReduce相比" class="headerlink" title="2、Spark与MapReduce相比"></a>2、Spark与MapReduce相比</h2><blockquote><p>面试题：Spark 与MapReduce相比为什么快，有什么不同？？？？</p></blockquote><div class="code-wrapper"><pre><code class="hljs arduino">其一、Spark处理数据时，可以将中间处理结果数据存储到内存中；第二、Spark Job调度以DAG方式，并且每个任务<span class="hljs-built_in">Task</span>执行以线程（Thread）方式，并不是像MapReduce以进程（<span class="hljs-built_in">Process</span>）方式执行。</code></pre></div><p><img src="https://i.loli.net/2021/09/06/XtxmcTysVnEBSrR.png" alt="图3"></p><blockquote><p>Spark与MapReduce最大不同：<a href="">提供数据结构RDD弹性分布式数据集。</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/NpW5Pehzy2AwM7J.png" alt="图4"></p><h2 id="3、RDD是什么，如何理解"><a href="#3、RDD是什么，如何理解" class="headerlink" title="3、RDD是什么，如何理解"></a>3、RDD是什么，如何理解</h2><blockquote><p>1、RDD是什么，官方定义：<a href="">不可变、分区的、并行计算的集合，抽象概念</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/sp8T1F39igfHqWv.png" alt="图5"></p><blockquote><p>2、每个RDD内部有5个特性</p><ul><li>分区组成、每个分区被计算处理、依赖一些列RDD</li><li>可选的：KeyValue类型RDD可以设置分区器，对每个分区数据处理时找到最佳位置（数据本地性）</li></ul></blockquote><p><img src="https://i.loli.net/2021/09/06/Nkbi3qxwF1fnEcO.png" alt="图6"></p><blockquote><p>3、常见RDD，<a href="">RDD抽象类，泛型，表示具体存储数据类型未知，可以是任何类型。</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/OKHzQcLeiv2uEfk.png" alt="图7"></p><div class="code-wrapper"><pre><code class="hljs arduino"><span class="hljs-number">1</span>、HadoopRDD，表示从文件系统加载数据封装的集合<span class="hljs-number">2</span>、MapPartitionsRDD，表示经过转换后的，比如fliter、map、flatMap等<span class="hljs-number">3</span>、ShuffleRDD，表示对RDD数据进行处理，产生shuffle时RDD</code></pre></div><h2 id="4、RDD、DataFrame和DataSet区别"><a href="#4、RDD、DataFrame和DataSet区别" class="headerlink" title="4、RDD、DataFrame和DataSet区别"></a>4、RDD、DataFrame和DataSet区别</h2><div class="code-wrapper"><pre><code class="hljs mathematica"><span class="hljs-number">1</span>、<span class="hljs-variable">RDD</span>是啥？？不可变、分区的、并行计算基本<span class="hljs-number">2</span>、<span class="hljs-variable">DataFrame</span><span class="hljs-variable">DataFrame</span> <span class="hljs-operator">=</span> <span class="hljs-variable">RDD</span><span class="hljs-punctuation">[</span><span class="hljs-built_in">Row</span><span class="hljs-punctuation">]</span> <span class="hljs-operator">+</span> <span class="hljs-variable">Schema</span>（字段名称、字段类型）知道内部结构当在处理分析数据，由于知道内部结构，所以可以先进行优化，在计算分析数据<span class="hljs-number">3</span>、<span class="hljs-built_in">Dataset</span><span class="hljs-variable">Spark</span> <span class="hljs-number">1.6</span>诞生<span class="hljs-built_in">Dataset</span> <span class="hljs-operator">=</span> <span class="hljs-variable">RDD</span> <span class="hljs-operator">+</span> <span class="hljs-variable">Schema</span>知道内部结构，也知道外部结构，编程更加安全和方便，内部数据存储使用特殊编码方式，节省空间<span class="hljs-number">4</span>、<span class="hljs-variable">Spark</span> <span class="hljs-number">2.0</span>开始<span class="hljs-variable">DataFrame</span>和<span class="hljs-built_in">Dataset</span>合并，其中<span class="hljs-variable">DataFrame</span>时<span class="hljs-built_in">Dataset</span>特殊形式，数据类型为<span class="hljs-built_in">Row</span>时数据结构<span class="hljs-variable">DataFrame</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">Dataset</span><span class="hljs-punctuation">[</span><span class="hljs-built_in">Row</span><span class="hljs-punctuation">]</span></code></pre></div><p><img src="https://i.loli.net/2021/09/06/iRcO1XegMI4rwN7.png" alt="图8"></p><blockquote><p>从Spark 2.x开始，建议大家使用数据结构为Dataset/DataFrame，因此使用SparkSQL模块分析数据。</p></blockquote><h2 id="5、Spark-on-YARN执行流程"><a href="#5、Spark-on-YARN执行流程" class="headerlink" title="5、Spark on YARN执行流程"></a>5、Spark on YARN执行流程</h2><blockquote><p>将Spark 应用程序（无论批处理还是流计算），提交运行到YARN集群上，提交流程。</p></blockquote><div class="code-wrapper"><pre><code class="hljs haml">Spark Application运行有2种DeployMode：1、client客户端DriverProgram运行在提交运行客户端主机上-<span class="ruby"> DriverProgram，调度Job执行</span><span class="ruby"></span>-<span class="ruby"> AppMaster，运行在NM中容器，申请资源运行Executors</span><span class="ruby"></span>-<span class="ruby"> Executors，运行在NM中容器，执行Task任务和缓存数据</span><span class="ruby"></span>2、cluster集群DriverProgram运行在YARN集群NodeManager容器中-<span class="ruby"> AppMaster/DriverProgram，运行在NM中容器，申请资源运行Executors和Job调度执行</span><span class="ruby"></span>-<span class="ruby"> Executors，运行在NM中容器，执行Task任务和缓存数据</span></code></pre></div><blockquote><p><code>yarn-client</code>，测试使用</p></blockquote><p><img src="https://i.loli.net/2021/09/06/Y9Uv8x61aCOydqp.png" alt="图9"></p><blockquote><p><code>yarn-cluster</code>，生成环境运行方式</p></blockquote><p><img src="https://i.loli.net/2021/09/06/hpHweAk9f4JGtNU.png" alt="图10"></p><p><img src="https://i.loli.net/2021/09/06/AthRJxlM8Ir34d9.png" alt="图11"></p><h2 id="6、SparkJob调度过程"><a href="#6、SparkJob调度过程" class="headerlink" title="6、SparkJob调度过程"></a>6、SparkJob调度过程</h2><blockquote><p>词频统计WordCount程序执行DAG图</p></blockquote><p><img src="https://i.loli.net/2021/09/06/UcAPhtBTkDafM9x.png" alt="图12"></p><div class="code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>、RDD<span class="hljs-comment">#Action函数，触发一个Job执行</span><span class="hljs-built_in">count</span>、foreach、foreachPartition等等<span class="hljs-number">2</span>、第一步、构建DAG图从触发<span class="hljs-keyword">Job开始RDD，采用回溯法（倒推法，从后向前），依据RDD依赖关系，构建Job对应DAG图</span><span class="hljs-keyword"></span>【依赖、回溯法】<span class="hljs-number">3</span>、第二步、划分DAG图为Stage从触发<span class="hljs-keyword">Job开始RDD，采用回溯法，如果2个RDD之间依赖为宽依赖，划分后面为一个Stage，依次类推</span><span class="hljs-keyword"></span>DAGScheduler划分完成以后，此时每个<span class="hljs-keyword">Job由多个Stage组成，各个Stage之间相互依赖关系</span><span class="hljs-keyword"></span>后面的Stage处理前面Stage的数据相邻<span class="hljs-number">2</span>个Stage之间产生<span class="hljs-keyword">Shuffle</span><span class="hljs-keyword"></span>Stage划分为<span class="hljs-number">2</span>种类型：第一、ResultStage，产生Result结果，每个<span class="hljs-keyword">Job中最后一个Stage</span><span class="hljs-keyword"></span>第二、<span class="hljs-keyword">ShuffleMapStage，除去Job中最后一个Stage其他Stage都是</span><span class="hljs-keyword"></span><span class="hljs-number">4</span>、第三步、按照<span class="hljs-keyword">Job中Stage顺序，从前向后执行Stage中Task任务</span><span class="hljs-keyword"></span>每个Stage中有多个Task任务，逻辑相同，处理数据不同而已将Stage中Task任务打包为TaskSet，发送给Executor执行TaskScheduler问题<span class="hljs-number">1</span>：每个Stage中Task数目如何确定？？由Stage中最后一个RDD分区数目确定问题<span class="hljs-number">2</span>：每个Stage中Task任务计算模式是什么？？？？管道计算模式，Pipeline模式</code></pre></div><p><img src="https://i.loli.net/2021/09/06/sJbErFfSDTlMBGp.png" alt="图13"></p><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> cn.test.spark<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SparkWordCount</span> </span>&#123;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-type">SparkContext</span>.getOrCreate(<span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">&quot;local[1]&quot;</span>).setAppName(<span class="hljs-string">&quot;SparkWordCount&quot;</span>))<span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.parallelize(<span class="hljs-type">Seq</span>(<span class="hljs-string">&quot;111111111111111111&quot;</span>, <span class="hljs-string">&quot;222222222222222222&quot;</span>, <span class="hljs-string">&quot;3333333333333333333&quot;</span>), numSlices = <span class="hljs-number">2</span>)println(<span class="hljs-string">s&quot;RDD 分区数目：<span class="hljs-subst">$&#123;inputRDD.getNumPartitions&#125;</span>&quot;</span>)<span class="hljs-keyword">val</span> resultRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = inputRDD.filter(line =&gt; &#123;println(<span class="hljs-string">&quot;filter........................&quot;</span>)<span class="hljs-comment">// 直接返回</span><span class="hljs-literal">true</span>&#125;).flatMap(line =&gt; &#123;println(<span class="hljs-string">&quot;flatMap........................&quot;</span>)<span class="hljs-type">Seq</span>(line)&#125;).map(line =&gt; &#123;println(<span class="hljs-string">&quot;map........................&quot;</span>)line&#125;)<span class="hljs-comment">/*</span><span class="hljs-comment">filter........................</span><span class="hljs-comment">flatMap........................</span><span class="hljs-comment">map........................</span><span class="hljs-comment"></span><span class="hljs-comment">filter........................</span><span class="hljs-comment">flatMap........................</span><span class="hljs-comment">map........................</span><span class="hljs-comment"></span><span class="hljs-comment">filter........................</span><span class="hljs-comment">flatMap........................</span><span class="hljs-comment">map........................</span><span class="hljs-comment"> */</span><span class="hljs-keyword">val</span> count = resultRDD.count()println(<span class="hljs-string">s&quot;Count = <span class="hljs-subst">$&#123;count&#125;</span>&quot;</span>)&#125;&#125;</code></pre></div><p><img src="https://i.loli.net/2021/09/06/pX9NdjS1q7s2yzM.png" alt="图14"></p><h2 id="7、Spark中依赖类型"><a href="#7、Spark中依赖类型" class="headerlink" title="7、Spark中依赖类型"></a>7、Spark中依赖类型</h2><blockquote><p>主要RDD依赖分为2类：</p><ul><li><strong>窄依赖</strong>：1对1，父RDD1个分区数据对应子RDD1个分区数据</li><li><code>宽依赖</code>：1对多，父RDD1个分区数据对应子RDD多个分区数据</li></ul></blockquote><p><img src="https://i.loli.net/2021/09/06/uPGhXzADYrBcZOw.png" alt="图15"></p><div class="code-wrapper"><pre><code class="hljs mipsasm">宽依赖：<span class="hljs-keyword">Shuffle依赖</span><span class="hljs-keyword"></span>相邻RDD产生<span class="hljs-keyword">Shuffle</span><span class="hljs-keyword"></span>Spark <span class="hljs-keyword">Shuffle </span>分为<span class="hljs-number">2</span>个部分：<span class="hljs-number">1</span>、<span class="hljs-keyword">Shuffle </span>Writer（上游Stage产生）将<span class="hljs-keyword">Shuffle数据写磁盘，有3种方式，具体由底层自动选择</span><span class="hljs-keyword"></span><span class="hljs-number">2</span>、<span class="hljs-keyword">ShuffleReader（下游Stage产生）</span><span class="hljs-keyword"></span>读取<span class="hljs-keyword">Shuffle到磁盘中数据，进行处理</span></code></pre></div><p><img src="https://i.loli.net/2021/09/06/2Wmqk5Rigp63oVG.png" alt="图16"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive自动化建库建表</title>
    <link href="/2020/09/12/Hive%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BB%BA%E5%BA%93%E5%BB%BA%E8%A1%A8/"/>
    <url>/2020/09/12/Hive%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BB%BA%E5%BA%93%E5%BB%BA%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>项目数仓数据源太多，于是自己写了一个工具类，读取数据源的元数据信息，自动建库建表</p><p>以 MySQL 为例，代码如下。</p><h3 id="HiveUtil"><a href="#HiveUtil" class="headerlink" title="HiveUtil"></a>HiveUtil</h3><div class="code-wrapper"><pre><code class="hljs java">object HiveUtil &#123;  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;    createHiveTable()  &#125;  <span class="hljs-function">def <span class="hljs-title">createHiveTable</span><span class="hljs-params">()</span> </span>= &#123;    <span class="hljs-comment">//连接MySQL，读取MySQL表名有哪些字段，字段类型，字段的注释</span>    val table_arr = Array(      <span class="hljs-string">&quot;area&quot;</span>,      <span class="hljs-string">&quot;claim_info&quot;</span>,      <span class="hljs-string">&quot;dd_table&quot;</span>,      <span class="hljs-string">&quot;mort_10_13&quot;</span>,      <span class="hljs-string">&quot;policy_acuary&quot;</span>,      <span class="hljs-string">&quot;policy_benefit&quot;</span>,      <span class="hljs-string">&quot;policy_client&quot;</span>,      <span class="hljs-string">&quot;policy_surrender&quot;</span>,      <span class="hljs-string">&quot;pre_add_exp_ratio&quot;</span>,      <span class="hljs-string">&quot;prem_cv_real&quot;</span>,      <span class="hljs-string">&quot;prem_std_real&quot;</span>)    val conn = DriverManager.getConnection(<span class="hljs-string">&quot;jdbc:mysql://node3:3306/insurance&quot;</span>, <span class="hljs-string">&quot;root&quot;</span>, <span class="hljs-string">&quot;123456&quot;</span>)    val ps: PreparedStatement = conn.prepareStatement(      s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><span class="hljs-string">         |SELECT</span><span class="hljs-string">         |       COLUMN_NAME,</span><span class="hljs-string">         |       COLUMN_TYPE,</span><span class="hljs-string">         |       COLUMN_COMMENT</span><span class="hljs-string">         |FROM information_schema.COLUMNS</span><span class="hljs-string">         |WHERE upper(TABLE_NAME)  = upper(?)</span><span class="hljs-string">         |  and upper(TABLE_SCHEMA)=upper(?)</span><span class="hljs-string">         |order by ORDINAL_POSITION</span><span class="hljs-string">         |&quot;</span><span class="hljs-string">&quot;&quot;</span>.stripMargin)    <span class="hljs-keyword">var</span> rs: ResultSet = <span class="hljs-function"><span class="hljs-keyword">null</span></span><span class="hljs-function">    <span class="hljs-title">for</span> <span class="hljs-params">(tablename &lt;- table_arr)</span> </span>&#123;      ps.setString(<span class="hljs-number">1</span>,tablename)      ps.setString(<span class="hljs-number">2</span>,<span class="hljs-string">&quot;insurance&quot;</span>)      rs = ps.executeQuery()      <span class="hljs-keyword">var</span> str =        s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><span class="hljs-string">           |drop table if exists $&#123;tablename&#125;;</span><span class="hljs-string">           |create table if not exists $&#123;tablename&#125; (\n&quot;</span><span class="hljs-string">&quot;&quot;</span>.<span class="hljs-function">stripMargin</span><span class="hljs-function">      <span class="hljs-title">while</span> <span class="hljs-params">(rs.next()</span>) </span>&#123;        val column_name: String = rs.getString(<span class="hljs-number">1</span>)        val column_type: String = rs.getString(<span class="hljs-number">2</span>)        val column_comment: String = rs.getString(<span class="hljs-number">3</span>)        <span class="hljs-keyword">var</span> temp_type = <span class="hljs-function">column_type</span><span class="hljs-function">        <span class="hljs-title">if</span> <span class="hljs-params">(temp_type.contains(<span class="hljs-string">&quot;int&quot;</span>)</span>) </span>&#123;          <span class="hljs-comment">//30000-&gt;int(5)-&gt;smallint</span>          <span class="hljs-comment">//300000000-&gt;int(5-16)-&gt;int</span>          <span class="hljs-comment">//3000000000000000000-&gt;int(16-32)-&gt;bigint</span>          val <span class="hljs-keyword">int</span>: Int = <span class="hljs-string">&quot;int(11)&quot;</span>.split(<span class="hljs-string">&quot;\\(|\\)&quot;</span>)(<span class="hljs-number">1</span>).<span class="hljs-function">toInt</span><span class="hljs-function">          <span class="hljs-title">if</span> <span class="hljs-params">(<span class="hljs-keyword">int</span> &gt; <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-keyword">int</span> &lt;= <span class="hljs-number">5</span>)</span> </span>&#123;            temp_type = <span class="hljs-string">&quot;smallint&quot;</span>          &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-keyword">int</span> &gt; <span class="hljs-number">5</span> &amp;&amp; <span class="hljs-keyword">int</span> &lt;= <span class="hljs-number">16</span>) &#123;            temp_type = <span class="hljs-string">&quot;int&quot;</span>          &#125; <span class="hljs-keyword">else</span> &#123;            temp_type = <span class="hljs-string">&quot;bigint&quot;</span>          &#125;        &#125;        <span class="hljs-keyword">if</span>(temp_type.contains(<span class="hljs-string">&quot;varchar&quot;</span>) || temp_type.contains(<span class="hljs-string">&quot;text&quot;</span>))&#123;          temp_type=<span class="hljs-string">&quot;string&quot;</span>        &#125;        <span class="hljs-comment">//println(column_name,column_type,column_comment)</span>        str += s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;$&#123;column_name&#125;   $&#123;temp_type&#125;  comment &#x27;$&#123;column_comment&#125;&#x27;,\n&quot;</span><span class="hljs-string">&quot;&quot;</span>      &#125;      str = str.stripSuffix(<span class="hljs-string">&quot;,\n&quot;</span>)      str += <span class="hljs-string">&quot;) comment &#x27;&#x27; \n row format delimited fields terminated by &#x27;\\t&#x27; ; \n&quot;</span>      println(str)    &#125;    rs.close()    ps.close()    conn.close()    <span class="hljs-comment">//解析上面的元数据，拼接成hive版的ddl语句</span>  &#125;&#125;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sqoop自动化抽取数据与验证</title>
    <link href="/2020/09/12/Sqoop%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AA%8C%E8%AF%81/"/>
    <url>/2020/09/12/Sqoop%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AA%8C%E8%AF%81/</url>
    
    <content type="html"><![CDATA[<h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>最近项目业务数据源多种多样，用 Sqoop 抽取数据到数仓是一个体力活，底层又是基于 MapReduce 执行的，速度感人，关键是还得做数据校验</p><p>于是想着自己写个工具类，和自动建表建库类似，自动读取数据源表和字段信息，创建对应脚本，扔到 DolphinScheduler 上自动跑就完事。</p><h3 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h3><p>1. 自定义工具类，读取 MySQL 中 information_schema 库下的 TABLES 表 获取同名的表</p><p>2. 获取到表名的容器，然后按照固定格式以文本形式写到 HDFS上文件夹上</p><p>3. 脚本内容需要做数据校验并将校验结果，并且加上并行执行符号</p><p>4. DolphinScheduler 上新建工作流，定期执行脚本文件</p><h3 id="Sqoop-数据抽取脚本"><a href="#Sqoop-数据抽取脚本" class="headerlink" title="Sqoop 数据抽取脚本"></a>Sqoop 数据抽取脚本</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1</code></pre></div><h3 id="Sqoop-抽取数据-数据验证"><a href="#Sqoop-抽取数据-数据验证" class="headerlink" title="Sqoop 抽取数据+数据验证"></a>Sqoop 抽取数据+数据验证</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1<span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span>`mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<span class="hljs-comment">#2、查询hive的表dd_table的条数</span>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<span class="hljs-comment">#3、比较2边的数字是否一样。</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><span class="hljs-keyword">else</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><span class="hljs-keyword">fi</span></code></pre></div><h3 id="SqoopUtil"><a href="#SqoopUtil" class="headerlink" title="SqoopUtil"></a>SqoopUtil</h3><div class="code-wrapper"><pre><code class="hljs sql">object SqoopUtil &#123;  def main(args: <span class="hljs-keyword">Array</span>[String]): Unit <span class="hljs-operator">=</span> &#123;    createHiveTable()  &#125;  def createHiveTable() <span class="hljs-operator">=</span> &#123;    <span class="hljs-operator">/</span><span class="hljs-operator">/</span>连接MySQL，读取MySQL表名有哪些字段，字段类型，字段的注释    val table_arr <span class="hljs-operator">=</span> <span class="hljs-keyword">Array</span>(      &quot;area&quot;,      &quot;policy_acuary&quot;,      &quot;policy_benefit&quot;,      &quot;policy_client&quot;,      &quot;policy_surrender&quot;,    <span class="hljs-keyword">for</span> (tablename <span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span> table_arr) &#123;      <span class="hljs-operator">/</span><span class="hljs-operator">/</span>var str <span class="hljs-operator">=</span>      <span class="hljs-operator">/</span><span class="hljs-operator">/</span>  s&quot;&quot;&quot;/export/server/sqoop/bin/sqoop import  --connect jdbc:mysql://192.168.88.163:3306/insurance  --username root  --password 123456  --table $&#123;tablename&#125;  --hive-table insurance_ods.$&#123;tablename&#125;  --hive-import  --hive-overwrite  --fields-terminated-by &#x27;\\t&#x27;  -m 1 \n&quot;&quot;&quot;.stripMargin      var str1 <span class="hljs-operator">=</span>        s&quot;&quot;&quot;/export/server/sqoop/bin/sqoop import  \\           |--connect jdbc:m1ysql://192.168.88.163:3306/insurance  \\           |--username root  \\           |--password 123456  \\           |--table $&#123;tablename&#125;  \\           |--hive-table insurance_ods.$&#123;tablename&#125;  \\           |--hive-import  \\           |--hive-overwrite  \\           |--fields-terminated-by &#x27;\\t&#x27;  \\           |-m 1&quot;&quot;&quot;.stripMargin      println(str1)    &#125;  &#125;&#125;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>Sqoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据常用脚本</title>
    <link href="/2020/06/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"/>
    <url>/2020/06/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<h2 id="一键启动"><a href="#一键启动" class="headerlink" title="一键启动"></a>一键启动</h2><h3 id="一键启动常用服务"><a href="#一键启动常用服务" class="headerlink" title="一键启动常用服务"></a>一键启动常用服务</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-keyword">if</span> [ ! <span class="hljs-variable">$1</span> ]<span class="hljs-keyword">then</span> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;please input [start|stop]&quot;</span><span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span><span class="hljs-comment">#start hadoop</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> dfs ------------ &quot;</span>ssh root@node01 <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;HADOOP_HOME&#125;</span>/sbin/<span class="hljs-variable">$&#123;1&#125;</span>-dfs.sh&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> yarn ---------- &quot;</span>ssh root@node01 <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;HADOOP_HOME&#125;</span>/sbin/<span class="hljs-variable">$&#123;1&#125;</span>-yarn.sh&quot;</span>sleep 1s<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> zookeeper ----------&quot;</span><span class="hljs-comment">#start zookeeper</span><span class="hljs-keyword">for</span> (( i=1; i&lt;=3; i++ ))<span class="hljs-keyword">do</span><span class="hljs-comment"># &lt;&lt; E0F 只是一个标识，可以换做其他任意字符，多行复杂脚本使用</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> zk <span class="hljs-variable">$&#123;1&#125;</span> ...&quot;</span>  ssh root@node0<span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile; zkServer.sh <span class="hljs-variable">$&#123;1&#125;</span>&quot;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> <span class="hljs-variable">$&#123;1&#125;</span> 完成.&quot;</span><span class="hljs-keyword">done</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> kafka ------------&quot;</span><span class="hljs-comment"># start kafka</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$&#123;1&#125;</span> == <span class="hljs-string">&#x27;stop&#x27;</span> ]<span class="hljs-keyword">then</span><span class="hljs-keyword">for</span> (( i=1; i&lt;=3; i++ ))<span class="hljs-keyword">do</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> kafka <span class="hljs-variable">$&#123;1&#125;</span> ...&quot;</span>  ssh root@node0<span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-stop.sh&quot;</span>  <span class="hljs-keyword">if</span> [ `ps -ef|grep Kafka | wc -l` -gt 1 ]; <span class="hljs-keyword">then</span>  ssh root@node0<span class="hljs-variable">$i</span> `ps -ef | grep Kafka | grep -v grep | awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span> | xargs <span class="hljs-built_in">kill</span> -9`  <span class="hljs-keyword">fi</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> <span class="hljs-variable">$&#123;1&#125;</span> 完成.&quot;</span><span class="hljs-keyword">done</span><span class="hljs-keyword">else</span><span class="hljs-keyword">for</span> (( i=1; i&lt;=3; i++ ))<span class="hljs-keyword">do</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> kafka <span class="hljs-variable">$&#123;1&#125;</span> ...&quot;</span>  ssh root@node0<span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-<span class="hljs-variable">$&#123;1&#125;</span>.sh -daemon /export/servers/kafka/config/server.properties&quot;</span>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> <span class="hljs-variable">$&#123;1&#125;</span> 完成.&quot;</span><span class="hljs-keyword">done</span>sleep 1s<span class="hljs-keyword">fi</span><span class="hljs-comment"># start flink</span><span class="hljs-comment"># /export/servers/flink/bin/$&#123;1&#125;-cluster.sh</span><span class="hljs-comment"># start dolphinscheduler</span><span class="hljs-comment"># /opt/soft/dolphinscheduler/bin/start-all.sh</span><span class="hljs-comment"># systemctl restart nginx</span></code></pre></div><h3 id="查看服务启动情况"><a href="#查看服务启动情况" class="headerlink" title="查看服务启动情况"></a>查看服务启动情况</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> node01 node02 node03<span class="hljs-keyword">do</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; <span class="hljs-variable">$i</span> <span class="hljs-variable">$1</span> &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&quot;</span>ssh <span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile;$*&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h2 id="Kafka-启动与关闭"><a href="#Kafka-启动与关闭" class="headerlink" title="Kafka 启动与关闭"></a>Kafka 启动与关闭</h2><h3 id="Kafka-一键启动"><a href="#Kafka-一键启动" class="headerlink" title="Kafka 一键启动"></a>Kafka 一键启动</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>        host=node<span class="hljs-variable">$&#123;number&#125;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>        /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;export JMX_PORT=9988;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-start.sh <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> started&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="Kafka-一键关闭"><a href="#Kafka-一键关闭" class="headerlink" title="Kafka 一键关闭"></a>Kafka 一键关闭</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>  host=node<span class="hljs-variable">$&#123;number&#125;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>  /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-stop.sh&quot;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> stoped&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><h3 id="Zookeeper-启动"><a href="#Zookeeper-启动" class="headerlink" title="Zookeeper 启动"></a>Zookeeper 启动</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>ZK_HOME=/<span class="hljs-built_in">export</span>/server/zookeeper-3.4.6<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>host=node<span class="hljs-variable">$&#123;number&#125;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>/usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;ZK_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;ZK_HOME&#125;</span>/bin/zkServer.sh start&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> started&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="Zookeeper-停止"><a href="#Zookeeper-停止" class="headerlink" title="Zookeeper 停止"></a>Zookeeper 停止</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>ZK_HOME=/<span class="hljs-built_in">export</span>/server/zookeeper-3.4.6<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>host=node<span class="hljs-variable">$&#123;number&#125;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>/usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;ZK_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;ZK_HOME&#125;</span>/bin/zkServer.sh stop&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> stoped&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="Zookeeper-查看状态"><a href="#Zookeeper-查看状态" class="headerlink" title="Zookeeper 查看状态"></a>Zookeeper 查看状态</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>ZK_HOME=/<span class="hljs-built_in">export</span>/server/zookeeper-3.4.6<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>host=node<span class="hljs-variable">$&#123;number&#125;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>/usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;ZK_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;ZK_HOME&#125;</span>/bin/zkServer.sh status&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h2 id="Sqoop数据抽取与验证"><a href="#Sqoop数据抽取与验证" class="headerlink" title="Sqoop数据抽取与验证"></a>Sqoop数据抽取与验证</h2><h3 id="数据抽取"><a href="#数据抽取" class="headerlink" title="数据抽取"></a>数据抽取</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1</code></pre></div><h3 id="抽取与验证"><a href="#抽取与验证" class="headerlink" title="抽取与验证"></a>抽取与验证</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1<span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span>`mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<span class="hljs-comment">#2、查询hive的表dd_table的条数</span>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<span class="hljs-comment">#3、比较2边的数字是否一样。</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><span class="hljs-keyword">else</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><span class="hljs-keyword">fi</span></code></pre></div><h2 id="Hive-加载分区数据"><a href="#Hive-加载分区数据" class="headerlink" title="Hive 加载分区数据"></a>Hive 加载分区数据</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>dt=`date -d <span class="hljs-string">&#x27;1 days ago&#x27;</span> +<span class="hljs-string">&#x27;%Y%m%d&#x27;</span>`tableName=<span class="hljs-variable">$1</span>ssh node03 `/<span class="hljs-built_in">export</span>/server/hive/bin/hive -e <span class="hljs-string">&quot;use test_ods;alter table <span class="hljs-variable">$&#123;tableName&#125;</span> add partition(dt=<span class="hljs-variable">$&#123;dt&#125;</span>) location &#x27;hdfs://node1:8020/apps/warehouse/ods.db/<span class="hljs-variable">$&#123;tableName&#125;</span>/<span class="hljs-variable">$&#123;dt&#125;</span>&quot;</span>`<span class="hljs-keyword">if</span> [ $? -eq 0 ]; <span class="hljs-keyword">then</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;load <span class="hljs-variable">$tableName</span> partition <span class="hljs-variable">$dt</span> succesful.&quot;</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;load <span class="hljs-variable">$tableName</span> partition <span class="hljs-variable">$dt</span> error.&quot;</span><span class="hljs-keyword">fi</span></code></pre></div>]]></content>
    
    
    <categories>
      
      <category>存档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>Spark</tag>
      
      <tag>Sqoop</tag>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多表连接过滤条件在 on 和 where 的区别</title>
    <link href="/2020/05/24/%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5%E8%BF%87%E6%BB%A4%E6%9D%A1%E4%BB%B6%E5%9C%A8%20on%20%E5%92%8C%20where%20%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <url>/2020/05/24/%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5%E8%BF%87%E6%BB%A4%E6%9D%A1%E4%BB%B6%E5%9C%A8%20on%20%E5%92%8C%20where%20%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h3 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h3><p>最近项目中的小坑，记录一下。</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(    sid   <span class="hljs-type">int</span> <span class="hljs-keyword">primary</span> key  <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ,    cid   <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>,    t_sex <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">null</span>)    comment <span class="hljs-string">&#x27;学生表&#x27;</span>;<span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> t_score(    sid    <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>,    course <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">null</span>,    score  <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>)    comment <span class="hljs-string">&#x27;成绩表&#x27;</span>;<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test.student <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;李白&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;杜甫&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;白居易&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;苏轼&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;李清照&#x27;</span>,<span class="hljs-string">&#x27;女&#x27;</span>),(<span class="hljs-number">7</span>,<span class="hljs-string">&#x27;谢道韫&#x27;</span>,<span class="hljs-string">&#x27;女&#x27;</span>),(<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;郭奉孝&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>);<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test.t_score <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">90</span>),(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">50</span>),(<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">99</span>),(<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">6</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">7</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>);</code></pre></div><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><ul><li>在 on 后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 内关联 条件放在 on 和 where 没有区别</span><span class="hljs-comment">-- 非空判断放在 on 和 where没有区别，成绩表有只有3个人的成绩，只有3个结果</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid<span class="hljs-keyword">where</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/JIUxdHebiaFhlBz.png" alt="Untitled"></p><ul><li>在 where后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">join</span> t_score ts<span class="hljs-keyword">where</span> s.sid<span class="hljs-operator">=</span>ts.sid <span class="hljs-keyword">and</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/tkaNXALl8FzIiYT.png" alt="Untitled"></p><h3 id="外连接"><a href="#外连接" class="headerlink" title="外连接"></a>外连接</h3><ul><li>在 on 后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 外连接结果有非常大的区别</span><span class="hljs-comment">-- 写在 on 条件上,有7个结果</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid  <span class="hljs-keyword">and</span>  ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/AIrX7xsPdEaqUty.png" alt="Untitled"></p><ul><li>在 where 后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 写在 where 条件上,只有3个结果</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid<span class="hljs-keyword">where</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/u4tXn2UqxcTOp1z.png" alt="Untitled"></p><h3 id="原因解析"><a href="#原因解析" class="headerlink" title="原因解析"></a>原因解析</h3><p>left join 的时候全部保留左边表格的内容，并保留右边表格能匹配上条件的内容<br>on 后面的就是连接条件，无论写什么只会对右边起效，不影响左表内容<br>where 后面的条件是对全局起效，就表关联之后的结果做筛选。</p>]]></content>
    
    
    <categories>
      
      <category>Debug记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据常用命令</title>
    <link href="/2020/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2020/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><div class="code-wrapper"><pre><code class="hljs sql"># 启动service mysqld <span class="hljs-keyword">start</span>systemctl <span class="hljs-keyword">start</span> mysql[d]# 关闭service mysqld stop#设置mysql开机启动chkconfig mysqld <span class="hljs-keyword">on</span></code></pre></div><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 全局组件启动与停止</span>start-all.shstop-all.sh<span class="hljs-comment"># HDFS 启动与停止</span>start-dfs.shstop-dfs.sh<span class="hljs-comment"># Yarn 启动与停止</span>start-yarn.shstop-yarn.sh<span class="hljs-comment"># HDFS 单个启动</span>hadoop-daemon.sh start namenode <span class="hljs-comment"># HDFS 多个启动</span>hadoop-daemons.sh start datanode<span class="hljs-comment"># Yarn 单个启动</span>yarn-daemon.sh start resourcemanager <span class="hljs-comment"># Yarn 多个启动</span>yarn-daemons.sh start nodemanager<span class="hljs-comment"># MR 历史 job记录，端口号 19888</span>mr-jobhistory-daemon.sh start historyserver<span class="hljs-comment"># 退出安全模式</span>hadoop dfsadmin -safemode leave</code></pre></div><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 启动Hive 的元数据服务</span>nohup /<span class="hljs-built_in">export</span>/server/hive-2.1.0/bin/hive --service metastore   &amp;<span class="hljs-comment"># 启动Hive 客户端服务</span>nohup /<span class="hljs-built_in">export</span>/server/hive-2.1.0/bin/hiveserver2 start &amp;<span class="hljs-comment"># beeline</span>!connect jdbc:hive2://node03:10000<span class="hljs-comment"># hive元数据初始化和更新</span>schematool -dbType mysql -initSchemaschematool -dbType mysql -upgradeSchema<span class="hljs-comment"># 使用动态分区</span><span class="hljs-comment"># 开启动态分区</span><span class="hljs-built_in">set</span> hive.exec.dynamic.partition=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 开启非严格模式 </span><span class="hljs-built_in">set</span> hive.exec.dynamic.partition.mode=nonstrict;<span class="hljs-comment"># 每个节点生成动态分区的最大个数</span><span class="hljs-built_in">set</span> hive.exec.max.dynamic.partitions.pernode=10000;<span class="hljs-comment"># 生成动态分区的最大个数</span><span class="hljs-built_in">set</span> hive.exec.max.dynamic.partitions=100000;<span class="hljs-comment"># 一个任务最多可以创建的文件数目</span><span class="hljs-built_in">set</span> hive.exec.max.created.files=150000;<span class="hljs-comment"># 限定一次最多打开的文件数</span><span class="hljs-built_in">set</span> dfs.datanode.max.xcievers=8192;<span class="hljs-comment">## Hive基础优化内容</span><span class="hljs-comment"># hive压缩</span><span class="hljs-built_in">set</span> hive.exec.compress.intermediate=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.exec.compress.output=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 写入时压缩生效</span><span class="hljs-built_in">set</span> hive.exec.orc.compression.strategy=COMPRESSION;<span class="hljs-comment"># 分桶</span><span class="hljs-built_in">set</span> hive.enforce.bucketing=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.enforce.sorting=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.optimize.bucketmapjoin = <span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.auto.convert.sortmerge.join=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 并行执行</span><span class="hljs-built_in">set</span> hive.exec.parallel=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.exec.parallel.thread.number=8;<span class="hljs-comment"># 小文件合并</span>-- <span class="hljs-built_in">set</span> mapred.max.split.size=2147483648;-- <span class="hljs-built_in">set</span> mapred.min.split.size.per.node=1000000000;-- <span class="hljs-built_in">set</span> mapred.min.split.size.per.rack=1000000000;<span class="hljs-comment"># 矢量化查询</span><span class="hljs-built_in">set</span> hive.vectorized.execution.enabled=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 关联优化器</span><span class="hljs-built_in">set</span> hive.optimize.correlation=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 读取零拷贝</span><span class="hljs-built_in">set</span> hive.exec.orc.zerocopy=<span class="hljs-literal">true</span>;<span class="hljs-comment"># join数据倾斜</span><span class="hljs-built_in">set</span> hive.optimize.skewjoin=<span class="hljs-literal">true</span>;-- <span class="hljs-built_in">set</span> hive.skewjoin.key=100000;<span class="hljs-built_in">set</span> hive.optimize.skewjoin.compiletime=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.optimize.union.remove=<span class="hljs-literal">true</span>;<span class="hljs-comment"># group倾斜</span><span class="hljs-built_in">set</span> hive.groupby.skewindata=<span class="hljs-literal">false</span>;</code></pre></div><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 全局启动</span>zkServer.sh start<span class="hljs-comment"># 标准启动</span>zookeeper-daemon.sh start</code></pre></div><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="启动与停止"><a href="#启动与停止" class="headerlink" title="启动与停止"></a>启动与停止</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 启动 Kafka 启动服务，需要先启动 zookeeper</span>kafka-server-start.sh config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;<span class="hljs-comment"># 关闭 Kafka 服务</span>kafka-server-stop.sh</code></pre></div><h3 id="封装启动脚本-记得给权限"><a href="#封装启动脚本-记得给权限" class="headerlink" title="封装启动脚本, 记得给权限"></a>封装启动脚本, 记得给权限</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>        host=node<span class="hljs-variable">$&#123;number&#125;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>        /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;export JMX_PORT=9988;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-start.sh <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> started&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="封装关闭脚本，记得给权限"><a href="#封装关闭脚本，记得给权限" class="headerlink" title="封装关闭脚本，记得给权限"></a>封装关闭脚本，记得给权限</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>  host=node<span class="hljs-variable">$&#123;number&#125;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>  /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-stop.sh&quot;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> stoped&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="彻底删除-kafka-并初始化"><a href="#彻底删除-kafka-并初始化" class="headerlink" title="彻底删除 kafka 并初始化"></a>彻底删除 kafka 并初始化</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 1.检查 server.properties 配置文件中的 delete.topic.enable=true，所有节点都需要设置，生效需要重启</span><span class="hljs-comment"># 2.删除 kafka 中的 topic test_data</span>bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --delete --topic test_data<span class="hljs-comment">#3.打开 zkCli.sh 删除三组配置</span>rm-rf /brokers/topics/test_datarm-rf /config/topics/test_datarm-rf /admin/delete_topics/test_data<span class="hljs-comment">#4.如果 kafka 集群没有关闭，关闭集群</span><span class="hljs-comment">#5.清空 log.dirs=/export/data/kafka/kafka-logs 目录，也就是 kafka 集群的数据目录</span>rm -rf /<span class="hljs-built_in">export</span>/data/kafka/kafka-logs/*<span class="hljs-comment">#6.重启 kafka 集群</span><span class="hljs-comment">#7.重新创建新的 topic</span>bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --topic test_data -- partitions 3 --replication-factor 2</code></pre></div><h3 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h3><div class="code-wrapper"><pre><code class="hljs bash">kafka-topics.sh --zookeeper node3:2181 --create --topic spark_kafka --partitions 3 --replication-factor 1kafka-topics.sh --zookeeper node3:2181 --list</code></pre></div><h3 id="启动生产者和消费者"><a href="#启动生产者和消费者" class="headerlink" title="启动生产者和消费者"></a>启动生产者和消费者</h3><div class="code-wrapper"><pre><code class="hljs bash">kafka-console-producer.sh --broker-list node3:9092 --topic spark_kafkakafka-console-consumer.sh --from-beginning --bootstrap-server node3:9092 --topic spark_kafkakafka-console-consumer.sh --from-beginning --bootstrap-server node3:9092 --topic __consumer_offsets</code></pre></div><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="启动spark-thriftserver"><a href="#启动spark-thriftserver" class="headerlink" title="启动spark-thriftserver"></a>启动spark-thriftserver</h3><div class="code-wrapper"><pre><code class="hljs bash">start-thriftserver.sh \  --hiveconf hive.server2.thrift.port=10001 \  --hiveconf hive.server2.thrift.bind.host=node3 \  --master <span class="hljs-built_in">local</span>[*]</code></pre></div><h3 id="启动-Spark-HistoryServer服务-端口号-18080"><a href="#启动-Spark-HistoryServer服务-端口号-18080" class="headerlink" title="启动 Spark HistoryServer服务, 端口号 18080"></a>启动 Spark HistoryServer服务, 端口号 18080</h3><div class="code-wrapper"><pre><code class="hljs bash">sbin/start-history-server.sh</code></pre></div><h3 id="structured-Streaming"><a href="#structured-Streaming" class="headerlink" title="structured Streaming"></a>structured Streaming</h3><div class="code-wrapper"><pre><code class="hljs bash">--memory sinkCREATE TABLE db_spark.tb_word_count (  id int NOT NULL AUTO_INCREMENT,  word varchar(255) NOT NULL,  count int NOT NULL,  PRIMARY KEY (id),  UNIQUE KEY word (word)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;REPLACE INTO  tb_word_count (id, word, count) VALUES (NULL, ?, ?);</code></pre></div><h3 id="spark-yarn-Pi-测试"><a href="#spark-yarn-Pi-测试" class="headerlink" title="spark yarn Pi 测试"></a>spark yarn Pi 测试</h3><div class="code-wrapper"><pre><code class="hljs bash">/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \--master yarn \--class org.apache.spark.examples.SparkPi \<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \10</code></pre></div><h3 id="WordCount-yarn"><a href="#WordCount-yarn" class="headerlink" title="WordCount yarn"></a>WordCount yarn</h3><div class="code-wrapper"><pre><code class="hljs bash">/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \--master yarn \--driver-memory 512m \--executor-memory 512m \--executor-cores 1 \--num-executors 2 \--queue default \--class cn.test.spark._2SparkWordCount \/opt/spark-chapter01-1.0-SNAPSHOT.jar</code></pre></div><h3 id="Spark-submit"><a href="#Spark-submit" class="headerlink" title="Spark-submit"></a>Spark-submit</h3><div class="code-wrapper"><pre><code class="hljs bash">【 Run application <span class="hljs-built_in">local</span> on 8 cores】/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master <span class="hljs-built_in">local</span>[8] \<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \  100<span class="hljs-comment"># Run on a Spark standalone cluster in client deploy mode</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master spark://207.184.161.138:7077 \  --executor-memory 20G \  --total-executor-cores 100 \<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \  1000<span class="hljs-comment"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master spark://207.184.161.138:7077 \  --deploy-mode cluster \  --supervise \  --executor-memory 20G \  --total-executor-cores 100 \  /path/to/examples.jar \  1000<span class="hljs-comment"># Run on a YARN cluster</span><span class="hljs-built_in">export</span> HADOOP_CONF_DIR=XXX./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master yarn \  --deploy-mode cluster \  <span class="hljs-comment"># can be client for client mode</span>  --executor-memory 20G \  --num-executors 50 \  /path/to/examples.jar \  1000<span class="hljs-comment"># Run a Python application on a Spark standalone cluster</span>./bin/spark-submit \  --master spark://207.184.161.138:7077 \  examples/src/main/python/pi.py \  1000<span class="hljs-comment"># Run on a Mesos cluster in cluster deploy mode with supervise</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master mesos://207.184.161.138:7077 \  --deploy-mode cluster \  --supervise \  --executor-memory 20G \  --total-executor-cores 100 \  http://path/to/examples.jar \  1000<span class="hljs-comment"># Run on a Kubernetes cluster in cluster deploy mode</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master k8s://xx.yy.zz.ww:443 \  --deploy-mode cluster \  --executor-memory 20G \  --num-executors 50 \  http://path/to/examples.jar \  1000</code></pre></div><h2 id="Sqoop数据抽取和数据验证"><a href="#Sqoop数据抽取和数据验证" class="headerlink" title="Sqoop数据抽取和数据验证"></a>Sqoop数据抽取和数据验证</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1<span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span>`mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<span class="hljs-comment">#2、查询hive的表dd_table的条数</span>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<span class="hljs-comment">#3、比较2边的数字是否一样。</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><span class="hljs-keyword">else</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><span class="hljs-keyword">fi</span></code></pre></div><h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="Flink-on-Yarn"><a href="#Flink-on-Yarn" class="headerlink" title="Flink on Yarn"></a>Flink on Yarn</h3><ul><li>Session 模式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 先创建 Session 会话， d 表示后台运行，s 表示每个 jm 的 slot 个数</span>flink/bin/yarn-session.sh -d -jm 1024 -tm 1024 -s 2<span class="hljs-comment"># 提交任务</span>flink/bin/flink run /<span class="hljs-built_in">export</span>/server/flink/examples/batch/WordCount.jar \--input hdfs://node1.test.cn:8020/wordcount/input</code></pre></div><ul><li>Job 分离模式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 直接提交任务，m 表示 jm 的地址,环境变量需要提前配置</span>/<span class="hljs-built_in">export</span>/server/flink/bin/flink run \-m yarn-cluster -yjm 1024 -ytm 1024 \/<span class="hljs-built_in">export</span>/server/flink/examples/batch/WordCount.jar \--input hdfs://node1.test.cn:8020/wordcount/input</code></pre></div><h2 id="其它命令"><a href="#其它命令" class="headerlink" title="其它命令"></a>其它命令</h2><h3 id="ES-启动"><a href="#ES-启动" class="headerlink" title="ES 启动"></a>ES 启动</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /<span class="hljs-built_in">export</span>/server/es/elasticsearch-7.6.1//<span class="hljs-built_in">export</span>/server/es/elasticsearch-7.6.1/bin/elasticsearch &gt;&gt;/dev/null 2&gt;&amp;1 &amp;</code></pre></div><h3 id="markdown代码折叠"><a href="#markdown代码折叠" class="headerlink" title="markdown代码折叠"></a>markdown代码折叠</h3><div class="code-wrapper"><pre><code class="hljs markdown"><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span></span>点击查看完整代码<span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></span></code></pre></div><h3 id="免秘钥登录"><a href="#免秘钥登录" class="headerlink" title="免秘钥登录"></a>免秘钥登录</h3><div class="code-wrapper"><pre><code class="hljs bash">ssh-keygen -t rsassh-copy-id node1scp /root/.ssh/authorized_keys node2:/root/.sshscp /root/.ssh/authorized_keys node3:/root/.ssh</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>存档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>Spark</tag>
      
      <tag>Sqoop</tag>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="/2020/04/27/first_post/"/>
    <url>/2020/04/27/first_post/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/07/DO2Jlm6oKBYgEfN.jpg"></p><p>努力写博客, 总结经验教训, 学习永远在路上<br>感觉 GitHub Page 真的太方便了，随时随地可以开始写<br>打算把常用的资料文档命令放到博客上，debug 记录也放上来，还有学习笔记与项目总结</p>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
