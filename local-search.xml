<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Redis常见面试题</title>
    <link href="/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <url>/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>学习和整理 Redis 相关的知识当中，这里汇总了一下经常被问到的 Redis 面试题</p><p>Redis 的八股无外乎这三个：缓存穿透、缓存击穿、缓存雪崩。</p><h1 id="分片集群问题"><a href="#分片集群问题" class="headerlink" title="分片集群问题"></a>分片集群问题</h1><h3 id="1-Redis的多数据机制了解多少"><a href="#1-Redis的多数据机制了解多少" class="headerlink" title="1.Redis的多数据机制了解多少"></a>1.Redis的多数据机制了解多少</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>Redis支持多个数据库,单机模式下有从db0到db15, 数据库之间不能共享<br><span class="hljs-number">2.</span>分片集群中只有一个数据库空间,不会使用到Redis的多数据库<br></code></pre></td></tr></table></figure><h3 id="2-懂Redis的批量操作吗"><a href="#2-懂Redis的批量操作吗" class="headerlink" title="2.懂Redis的批量操作吗"></a>2.懂Redis的批量操作吗</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>有mget,mset,hmget,hmset,hgetall,hvals,<br><span class="hljs-number">2.</span>分片集群中, 不同的key会分到不同的slot中, 不能直接使用mget,mset<br>如何解决: 加上相同的前缀,用大括号&#123;&#125;包裹<br></code></pre></td></tr></table></figure><h3 id="3-Redis的集群机制中-你觉得有什么不足的地方吗"><a href="#3-Redis的集群机制中-你觉得有什么不足的地方吗" class="headerlink" title="3.Redis的集群机制中, 你觉得有什么不足的地方吗?"></a>3.Redis的集群机制中, 你觉得有什么不足的地方吗?</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>如果<span class="hljs-keyword">value</span>是hash类型, 对象非常大, 即对应属性非常多, 也只能存入一个集群的节点中<br><span class="hljs-number">2.</span>批量操作也很麻烦, 属性太多写得很长<br></code></pre></td></tr></table></figure><h3 id="4-在Redis集群模式下-如何进行批量操作"><a href="#4-在Redis集群模式下-如何进行批量操作" class="headerlink" title="4.在Redis集群模式下, 如何进行批量操作?"></a>4.在Redis集群模式下, 如何进行批量操作?</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>如何执行的key数量很少, 串行<span class="hljs-keyword">get</span>操作<br><span class="hljs-number">2.</span>如果一定要批量操作, 加上相同的前缀,前缀用&#123;&#125;包裹<br></code></pre></td></tr></table></figure><h3 id="4-5-什么是Redis的事务"><a href="#4-5-什么是Redis的事务" class="headerlink" title="4.5 什么是Redis的事务"></a>4.5 什么是Redis的事务</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>Redis事务是一些命令的集合, 执行的时候是串行执行<br><span class="hljs-number">2.</span>分片集群中,不同的key可能会被分到不同的slot中, Redis的事务不生效<br><span class="hljs-number">3.</span>Redis的事务不支持回滚操作, 基本用不上.<br></code></pre></td></tr></table></figure><h1 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h1><h3 id="5-什么是缓存穿透-如何解决"><a href="#5-什么是缓存穿透-如何解决" class="headerlink" title="5.什么是缓存穿透,如何解决"></a>5.什么是缓存穿透,如何解决</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sql">现象：<br>客户端高并发不断向Redis请求一个不存在的Key，MySQL中也没有<br>由于Redis没有，导致这个并发全部落在MySQL上<br><br>解决<br><span class="hljs-number">1.</span>对于那些每秒访问频次过高的IP进行限制，拒绝访问<br><span class="hljs-number">2.</span>如果第一次redis中没有，读MYSQL，MySQL也没有，在Redis中设置一个<span class="hljs-operator">=</span><span class="hljs-operator">=</span>临时<span class="hljs-operator">=</span><span class="hljs-operator">=</span>默认值<br><span class="hljs-number">3.</span>利用BitMap类型构建布隆过滤器<span class="hljs-operator">*</span><span class="hljs-operator">*</span>(只保证MySQL数据库没有这个数据, 不保证一定有)<span class="hljs-operator">*</span><span class="hljs-operator">*</span><br></code></pre></td></tr></table></figure><h3 id="6-什么是缓存击穿-如何解决"><a href="#6-什么是缓存击穿-如何解决" class="headerlink" title="6.什么是缓存击穿,如何解决"></a>6.什么是缓存击穿,如何解决</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sql">现象：<br>有一个Key，经常需要高并发的访问，这个Key有过期时间的，一旦达到过期时间<span class="hljs-operator">=</span><span class="hljs-operator">=</span>，这个Key被删除，所有高并发落到了MySQL中，被击穿了<br><br>解决<br><span class="hljs-number">1.</span>资源充足的情况下，设置永不过期<br><span class="hljs-number">2.</span>对这个Key做一个互斥锁，只允许一个请求去读取，其他的所有请求先阻塞掉<br>第一个请求redis中没有读取到，读了MySQL，再将这个数据放到Redis中<br>释放所有阻塞的请求<br></code></pre></td></tr></table></figure><h3 id="7-什么是缓存雪崩-如何解决"><a href="#7-什么是缓存雪崩-如何解决" class="headerlink" title="7.什么是缓存雪崩 ,如何解决"></a>7.什么是缓存雪崩 ,如何解决</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql">现象：<br>大量的Key在同一个时间段过期，大量的Key的请求在Redis中都没有，都去请求MySQL，导致MySQL奔溃<br><br>解决<br><span class="hljs-number">1.</span>资源充足允许的情况下，设置大部分的Key不过期<br><span class="hljs-number">2.</span>给所有Key设置过期时间时加上随机值，让Key不再同一时间过期<br></code></pre></td></tr></table></figure><h3 id="8-Redis中的Key如何设计"><a href="#8-Redis中的Key如何设计" class="headerlink" title="8.Redis中的Key如何设计?"></a>8.Redis中的Key如何设计?</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>使用统一的命名规范<br><span class="hljs-number">2.</span>一般使用业务名(或数据库名)为前缀，用冒号分隔，例如，业务名:表名:id。<br>例如：shop:usr:msg_code（电商:用户:验证码）<br><span class="hljs-number">4.</span>控制key名称的长度，不要使用过长的key<br><span class="hljs-number">5.</span>在保证语义清晰的情况下，尽量减少Key的长度。有些常用单词可使用缩写，例如，<span class="hljs-keyword">user</span>缩写为u，messages缩写为msg<br><span class="hljs-number">6.</span>名称中不要包含特殊字符、包含空格、单双引号以及其他转义字符<br></code></pre></td></tr></table></figure><h3 id="9-为什么Redis是单线程的"><a href="#9-为什么Redis是单线程的" class="headerlink" title="9.为什么Redis是单线程的?"></a>9.为什么Redis是单线程的?</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>因为Redis是基于内存的操作，CPU不是Redis的瓶颈<br><span class="hljs-number">2.</span>Redis的瓶颈最有可能是机器内存的大小或者网络带宽<br><span class="hljs-number">3.</span>单线程容易实现，而且CPU不会成为瓶颈，所以没必要使用多线程增加复杂度<br><span class="hljs-number">4.</span>可以使用多Redis压榨CPU，提高性能<br></code></pre></td></tr></table></figure><h3 id="10-为什么Redis的性能很高"><a href="#10-为什么Redis的性能很高" class="headerlink" title="10.为什么Redis的性能很高?"></a>10.为什么Redis的性能很高?</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1.</span>基于内存操作, 快<br><span class="hljs-number">2.</span>用C语言编写, 数据结构简单, 对数据操作也简单<br><span class="hljs-number">3.</span>采用单线程, 避免不必要的线程切换和资源抢占<br><span class="hljs-number">4.</span>IO多路复用模型, 非阻塞IO<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>面试准备</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WordCount案例汇总</title>
    <link href="/2020/11/06/WordCount%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB%20/"/>
    <url>/2020/11/06/WordCount%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB%20/</url>
    
    <content type="html"><![CDATA[<h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount</p><p>经典永不过时，使用的很多函数和方法也是常用的。</p><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Map规则, 用来实现把: k1, v1 -&gt; k2, v2, 需要 继承Mapper类, 重写map方法.</span><br><span class="hljs-comment"> * 各个数据解释:</span><br><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><br><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountMapTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 重写map方法,用来将K1 V2 转换成 K2 V2</span><br><span class="hljs-comment">     *</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k1</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> value   v1</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写出K2,V2</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;<br>        <span class="hljs-comment">//1.获取行偏移量,没有什么用处,我们用于测试看看的</span><br>        <span class="hljs-keyword">long</span> index = key.get();<br>        System.out.println(<span class="hljs-string">&quot;行偏移量是: &quot;</span> + index);<br>        <span class="hljs-comment">//2.获取整行数据</span><br>        String line = value.toString();<br>        <span class="hljs-comment">//3.读取并做非空校验,判断值是否相等,也判断地址值是否相等</span><br>        <span class="hljs-keyword">if</span> (line != <span class="hljs-keyword">null</span> &amp;&amp; !<span class="hljs-string">&quot;&quot;</span>.equals(line)) &#123;<br>            <span class="hljs-comment">//4.切割获取K2,V2</span><br>            String[] str = line.split(<span class="hljs-string">&quot; &quot;</span>);<br>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; str.length; i++) &#123;<br>                String s = str[i];<br>                context.write(<span class="hljs-keyword">new</span> Text(s), <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>));<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Reduce规则, 用来实现把: k2, v2的集合 -&gt; k3, v3, 需要 继承Reducer类, 重写reduce方法.</span><br><span class="hljs-comment"> * 各个数据解释:</span><br><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><br><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * shuffle阶段: 分区, 排序, 规约, 分组之后, 数据如下:</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v2(的集合): 每个单词的所有次数的集合, 例如: &#123;1, 1&#125;,  &#123;1, 1, 1&#125;, &#123;1, 1&#125;</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * k3: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><br><span class="hljs-comment"> * v3: 每个单词的总次数, 例如: 2, 3, 2</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountReduceTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;<br><br>    <span class="hljs-comment">//重写reduce方法</span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 重写reduce方法,用于把k2,v2 转换成k3,v3</span><br><span class="hljs-comment">     *</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k2</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> values  v2的集合(已经经过了分组)</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写k3,v3</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;<br>        <span class="hljs-comment">//1.获取k3,就是每个单词</span><br>        String word = key.toString();<br>        <span class="hljs-comment">//2.获取v3,就是单词出现的次数</span><br>        <span class="hljs-comment">//2.1先对v2集合求和</span><br>        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (IntWritable value : values) &#123;<br>            count += value.get();<br>        &#125;<br>        <span class="hljs-comment">//2.2写出v3</span><br>        <span class="hljs-comment">//context.write(new Text(word),new IntWritable(count));</span><br>        <span class="hljs-comment">//因为v2和v3是一样的,我们可以优化一下</span><br>        context.write(key, <span class="hljs-keyword">new</span> IntWritable(count));<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="WordCountMain简写版"><a href="#WordCountMain简写版" class="headerlink" title="WordCountMain简写版"></a>WordCountMain简写版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 这里写的是驱动类, 即: 封装MR程序的核心8步的. 它有两种写法:</span><br><span class="hljs-comment"> * 1. 官方示例版, 即: 完整版.   理解即可, 因为稍显复杂, 用的人较少.</span><br><span class="hljs-comment"> * 2. 简化版.  推荐掌握.</span><br><span class="hljs-comment"> * 这里是简化版写法</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span><br>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);<br>        <span class="hljs-comment">//2.封装MR程序核心8步</span><br>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span><br>        job.setInputFormatClass(TextInputFormat.class);<br>        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/input/wordcount.txt&quot;</span>));<br>        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span><br>        job.setMapperClass(WordCountMapTask.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.3 分区,用默认的</span><br>        <span class="hljs-comment">//2.4 排序,用默认的</span><br>        <span class="hljs-comment">//2.5 规约,用默认的</span><br>        <span class="hljs-comment">//2.6 分组,用默认的</span><br>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span><br>        job.setReducerClass(WorkCountReduceTask.class);<br>        job.setOutputValueClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span><br>        job.setOutputFormatClass(TextOutputFormat.class);<br>        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/output&quot;</span>));<br>        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span><br>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);<br>        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span><br>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="WordCountMain-jar包版"><a href="#WordCountMain-jar包版" class="headerlink" title="WordCountMain jar包版"></a>WordCountMain jar包版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 这个代码一会儿是要打包成jar包, 然后放到Yarn集群中运行的, 需要做如下的几件事儿:</span><br><span class="hljs-comment"> * 1. 在驱动类中设置 jar包的启动类.</span><br><span class="hljs-comment"> * job.setJarByClass(WordCountMain3.class);</span><br><span class="hljs-comment"> * 2. 修改数据源文件 和 目的地文件的路径, 改为: 外部传入.</span><br><span class="hljs-comment"> * TextInputFormat.addInputPath(job, new Path(args[0]));</span><br><span class="hljs-comment"> * TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="hljs-comment"> * 3. 对我们当前的工程进行打包动作, 打包成: 胖jar, 具体操作为: 取消pom.xml文件中最后一个插件的注释, 然后打包即可.</span><br><span class="hljs-comment"> * 细节: 修改jar包名字为: wordcount.jar, 方便我们操作.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 4. 在HDFS集群中创建:   /wordcount/input/ 目录</span><br><span class="hljs-comment"> * 5. 把wordcount.txt 上传到该目录下.</span><br><span class="hljs-comment"> * 6. 把之前打好的 jar包也上传到 Linux系统中.</span><br><span class="hljs-comment"> * 7. 运行该jar包即可, 记得: 传入 数据源文件路径, 目的地目录路径.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 名词解释:</span><br><span class="hljs-comment"> * 胖jar: 指的是一个jar包中还包含有其他的jar包, 这样的jar包就称之为: 胖jar.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 问题1: 为什么需要打包成 胖jar?</span><br><span class="hljs-comment"> * 答案:</span><br><span class="hljs-comment"> * 因为目前我们的工程需要依赖 Hadoop环境, 而我们已经在pom.xml文件中配置了,</span><br><span class="hljs-comment"> * 如果运行的环境中(例如: Linux系统等)没有hadoop环境, 并且我们打包时也没有把hadoop环境打包进去,</span><br><span class="hljs-comment"> * 将来运行jar包的时候就会出错.</span><br><span class="hljs-comment"> * &lt;p&gt;</span><br><span class="hljs-comment"> * 问题2: 当前工程一定要打包成 胖jar吗?</span><br><span class="hljs-comment"> * 答案: 不用, 因为我们的 jar包一会儿是放到 Yarn集群中运行的, 它已经自带Hadoop环境, 所以这里可以不打包 胖jar, 只打包我们自己的代码.</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain3</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span><br>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);<br>        <span class="hljs-comment">//细节1: 在驱动类中设置 jar包的启动类.</span><br>        job.setJarByClass(WorkCountMain3.class);<br><br>        <span class="hljs-comment">//2.封装MR程序核心8步</span><br>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span><br>        job.setInputFormatClass(TextInputFormat.class);<br>        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">0</span>]));<br>        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span><br>        job.setMapperClass(WordCountMapTask.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.3 分区,用默认的</span><br>        <span class="hljs-comment">//2.4 排序,用默认的</span><br>        <span class="hljs-comment">//2.5 规约,用默认的</span><br>        <span class="hljs-comment">//2.6 分组,用默认的</span><br>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span><br>        job.setReducerClass(WorkCountReduceTask.class);<br>        job.setOutputValueClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span><br>        job.setOutputFormatClass(TextOutputFormat.class);<br>        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">1</span>]));<br>        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span><br>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);<br>        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span><br>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>基本流程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java">基本流程<br><span class="hljs-number">1.</span>传输文件路径到自定义的Actor类,并接收返回值,解析返回值得出统计结果<br><span class="hljs-number">2.</span>自定义Actor类, 接收文件路径并做解析统计单词再返回给发送者<br><br>需要分别定义<span class="hljs-number">3</span>个类<br>Main入口<br><span class="hljs-number">1.</span>用于发送文件路径,封装在自定义的单例类里面<br><span class="hljs-number">2.</span>接收返回值,并做判断是否完成传输, 如果完成就开始解析<br><span class="hljs-number">3.</span>通过apply方法解析结果,合并结果得出最后结果<br><br>自定义的Actor类<br><span class="hljs-number">1.</span>接收文件路径信息,做分析统计<br><span class="hljs-number">2.</span>把结果封装在单例类中,返回给发送者<br><br>自定义的单例类<br><span class="hljs-number">1.</span>用于封装发送信息的单例类<br><span class="hljs-number">2.</span>用于返回统计的单例类<br><br></code></pre></td></tr></table></figure><h3 id="MainActor"><a href="#MainActor" class="headerlink" title="MainActor"></a>MainActor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs java">`<span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<br><br><span class="hljs-keyword">import</span> java.io.File<br><span class="hljs-keyword">import</span> scala.actors.Future<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.发送文件名给WordCountActor</span><br><span class="hljs-comment"> * 2.接收WordCountActor返回结果并合并</span><br><span class="hljs-comment"> */</span><br>object MainActor &#123;<br><br>  <span class="hljs-comment">//发送文件名给WordCountActor</span><br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//1.获取文件名</span><br>    val fileDir = <span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;./data&quot;</span>)<br>    val files: Array[File] = fileDir.listFiles()<br>    <span class="hljs-comment">// 测试是成功获取文件名</span><br>    <span class="hljs-comment">// files.foreach(println(_))</span><br><br>    <span class="hljs-comment">//2.发送给wordcountactor</span><br>    val future_Array: Array[Future[Any]] = files.map(f = file =&gt; &#123;<br>      val name = file.toString<br>      <span class="hljs-comment">//每一个文件名新建对应的线程</span><br>      val actor = <span class="hljs-keyword">new</span> WordCountActor<br>      <span class="hljs-comment">//开启线程并发送给我认定任务</span><br>      actor.start()<br>      <span class="hljs-comment">//发送的消息封装在这里面并获取结果</span><br>      val future: Future[Any] = actor !! WordCountTask(name)<br>      future<br>    &#125;)<br><br>    <span class="hljs-comment">//接收WordCountActor返回结果并合并</span><br>    <span class="hljs-comment">//先判断是否全部文件都处理完毕都有结果,是再处理</span><br>    <span class="hljs-keyword">while</span> (!(future_Array.filter((x) =&gt; &#123;<br>      !x.isSet<br>    &#125;)).isEmpty) &#123;&#125;<br>    <span class="hljs-comment">//走到这里, 证明我们可以处理,使用apply获取数据</span><br>    <span class="hljs-comment">//里面的键值对就是多个文件统计结果, 我们还需要合并去重</span><br>    val wordCount: Array[Map[String, Int]] = future_Array.map((x) =&gt; &#123;<br>      val results: Any = x.apply()<br>      val result = results.asInstanceOf[WordCountResult]<br>      val map: Map[String, Int] = result.map<br>      map<br>    &#125;)<br>    <span class="hljs-comment">//wordCount.foreach(println(_))</span><br>    <span class="hljs-comment">//测试结果</span><br>    <span class="hljs-comment">// Map(e -&gt; 2, f -&gt; 1, a -&gt; 1, b -&gt; 1, c -&gt; 1)</span><br>    <span class="hljs-comment">// Map(e -&gt; 1, a -&gt; 2, b -&gt; 1, c -&gt; 2, d -&gt; 3)</span><br><br>    <span class="hljs-comment">//合并结果, 先合并成一个Array</span><br>    val flatten: Array[(String, Int)] = wordCount.flatten<br>    <span class="hljs-comment">//根据Map的key值分组</span><br>    val wordGroup: Map[String, Array[(String, Int)]] = flatten.groupBy((x) =&gt; &#123;<br>      x._1<br>    &#125;)<br>    val finalResult: Map[String, Int] = wordGroup.map((x) =&gt; &#123;<br>      val name = x._1<br>      val size = x._2.size<br>      name -&gt; size<br>    &#125;)<br><br>    finalResult.foreach(println(_))<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="WordCountActor"><a href="#WordCountActor" class="headerlink" title="WordCountActor"></a>WordCountActor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<br><br><span class="hljs-keyword">import</span> scala.actors.Actor<br><span class="hljs-keyword">import</span> scala.io.Source<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.接收MainActor的文件名称并进行单词统计</span><br><span class="hljs-comment"> * 2.将单词统计结果返回给MainActor</span><br><span class="hljs-comment"> */</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountActor</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Actor</span> </span>&#123;<br>  <span class="hljs-function">override def <span class="hljs-title">act</span><span class="hljs-params">()</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//接收消息</span><br>    loop &#123;<br>      react &#123;<br>        <span class="hljs-function"><span class="hljs-keyword">case</span> <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename)</span> </span>=&gt;<br>          println(<span class="hljs-string">&quot;收到了文件名: &quot;</span> + filename)<br>          <span class="hljs-comment">//解析消息, 通过Source解析消息, 定义文件来源再转化成列表</span><br>          <span class="hljs-comment">//一个元素就是一个一行数据</span><br>          val words: List[String] = Source.fromFile(filename).getLines().toList<br>          <span class="hljs-comment">//切割获取每一条数据并合并成一个list集合</span><br>          val word_List: List[String] = words.flatMap((x) =&gt; &#123;<br>            x.split(<span class="hljs-string">&quot; &quot;</span>)<br>          &#125;)<br>          <span class="hljs-comment">//按照单词进行分组, 然后聚合统计</span><br>          val word_Tuples: List[(String, Int)] = word_List.map((x) =&gt; &#123;<br>            (x, <span class="hljs-number">1</span>)<br>          &#125;)<br>          val word_Map: Map[String, List[(String, Int)]] = word_Tuples.groupBy((x) =&gt; &#123;<br>            x._1<br>          &#125;)<br>          val wordCountMap: Map[String, Int] = word_Map.map((x) =&gt; &#123;<br>            val name: String = x._1<br>            val size: Int = x._2.size<br>            name -&gt; size<br>          &#125;)<br><br>          <span class="hljs-comment">//把统计结果反馈给Mainactor,装进WordCount</span><br>          sender ! WordCountResult(wordCountMap)<br>      &#125;<br><br>    &#125;<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="WordCountPackage"><a href="#WordCountPackage" class="headerlink" title="WordCountPackage"></a>WordCountPackage</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 1.定义一个样例类, 描述单词统计信息</span><br><span class="hljs-comment"> * 2.定义一个样例类封装单词统计结果</span><br><span class="hljs-comment"> */</span><br>object WordCountPackage &#123;<br><br>  <span class="hljs-comment">//1.定义一个样例类, 描述单词统计信息</span><br>  <span class="hljs-function"><span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename: String)</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">  <span class="hljs-comment">//2.定义一个样例类封装单词统计结果</span></span><br><span class="hljs-function">  <span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountResult</span><span class="hljs-params">(map: Map[String, Int])</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">&#125;</span><br></code></pre></td></tr></table></figure><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><ul><li>基本流程</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-number">1.</span>创建上下文对象<br><span class="hljs-number">2.</span>读取文件<br><span class="hljs-number">3.</span>flatMap获取到每个单词<br><span class="hljs-number">4.</span>map将RDD变成 key-value结构<br><span class="hljs-number">5.</span>reduceByKey 求和统计<br><span class="hljs-number">6.</span>打印输出<br><span class="hljs-number">7.</span>关闭上下文对象<br></code></pre></td></tr></table></figure><ul><li>本地版</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day01<br><br><span class="hljs-keyword">import</span> org.apache.spark.rdd.RDD<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br>object WordCount &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>).setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc: SparkContext = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span><br>    val inputRDD: RDD[String] = sc.textFile(<span class="hljs-string">&quot;src/main/data/words.txt&quot;</span>)<br>    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span><br>    val flatRDD = inputRDD.flatMap((x) =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//4.继续对每个单词标记为1</span><br>    val wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))<br>    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span><br>    val ouputRDD = wordOneRDD.reduceByKey(_ + _)<br>    <span class="hljs-comment">//6.生成最后的RDD, 将结果打印到控制台</span><br>    ouputRDD.foreach(println(_))<br>    <span class="hljs-comment">//7.关闭上下文</span><br>    sc.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>Linux版</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day01<br><br><span class="hljs-keyword">import</span> org.apache.spark.rdd.RDD<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br>object WordCount_Linux &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//0.创建输入路径和输出路径</span><br>    val input_path = args(<span class="hljs-number">0</span>)<br>    val output_path = args(<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>)<br>    val sc: SparkContext = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span><br>    val inputRDD: RDD[String] = sc.textFile(input_path)<br>    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span><br>    val flatRDD = inputRDD.flatMap((x) =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//4.继续对每个单词标记为1</span><br>    val wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))<br>    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span><br>    val ouputRDD = wordOneRDD.reduceByKey(_ + _)<br>    <span class="hljs-comment">//6.生成最后的RDD, 将结果上传到HDFS</span><br>    ouputRDD.saveAsTextFile(output_path)<br>    <span class="hljs-comment">//7.关闭上下文</span><br>    sc.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day05.sparkSQL<br><br><span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;Dataset, SparkSession&#125;<br><br>object S1WordcountSQL &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val spark: SparkSession = SparkSession.builder()<br>      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      .getOrCreate()<br>    <span class="hljs-comment">//读取文件生成DataSet, 只有一列时 列名自动为 value</span><br>    val inputDS: Dataset[String] = spark.read.textFile(<span class="hljs-string">&quot;src/main/data/words.txt&quot;</span>)<br>    <span class="hljs-comment">//扁平化单词, 得到新的DataSet</span><br>    <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 转换数据类型都需要转换???</span><br>    <span class="hljs-keyword">import</span> spark.implicits._<br>    val wordDS: Dataset[String] = inputDS.flatMap(x =&gt; &#123;<br>      x.split(<span class="hljs-string">&quot; &quot;</span>)<br>    &#125;)<br>    wordDS.printSchema()<br>    wordDS.show()<br>    <span class="hljs-comment">//对新的DataSet进行SQL风格 Wordcount</span><br>    println(<span class="hljs-string">&quot;QL风格 Wordcount&quot;</span>)<br>    println()<br><br>    wordDS.createOrReplaceTempView(<span class="hljs-string">&quot;t_words&quot;</span>)<br>    spark.sql(<br>      <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">        |select value,count(1) cnt</span><br><span class="hljs-string">        |from t_words</span><br><span class="hljs-string">        |group by value</span><br><span class="hljs-string">        |order by cnt desc</span><br><span class="hljs-string">        |&quot;</span><span class="hljs-string">&quot;&quot;</span>.stripMargin).show()<br><br>    <span class="hljs-comment">//对新的DataSet进行DSL风格 Wordcount</span><br>    wordDS.groupBy(<span class="hljs-string">&quot;value&quot;</span>)<br>      .count() <span class="hljs-comment">//自动拼接一列count名</span><br>      .orderBy($<span class="hljs-string">&quot;count&quot;</span>.desc) <span class="hljs-comment">//转成column对象</span><br>      .show()<br><br>    <span class="hljs-comment">//关闭上下文对象</span><br>    spark.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h3><ul><li>前期准备：安装 netcat</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 在Linux上安装 netcat</span><br>yum install nc -y<br>yum install nmap -y<br><span class="hljs-comment">// 向 9999 端口发送数据</span><br>nc -lk <span class="hljs-number">9999</span><br></code></pre></td></tr></table></figure><ul><li>Wordcount  by UpdateStateByKey</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day06.streaming<br><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: wordcount 案例，通过 UpdateStateByKey 实现宕机后状态恢复</span><br><span class="hljs-comment"> * 需要利用ncat 发数据， </span><br><span class="hljs-comment"> */</span><br><br>object S4ocketWordcountUpdateStateByKeyRecovery &#123;<br>    <span class="hljs-comment">//设置路径</span><br>    val CKP =<span class="hljs-string">&quot;src/main/data/ckp/&quot;</span>+<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>)<br>    <span class="hljs-comment">//1.创建上下文对象, 指定批处理时间间隔为5秒</span><br>    val creatingFunc =()=&gt;<br>    &#123;<br>      val conf: SparkConf = <span class="hljs-keyword">new</span> SparkConf()<br>        .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>        .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>      <span class="hljs-comment">//2. 创建一个接收文本数据流的流对象</span><br>      val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br><br>      <span class="hljs-comment">//3.设置checkpoint位置</span><br>      ssc.checkpoint(CKP)<br>      <span class="hljs-comment">//4.接收socket数据</span><br>      val inputDStream: ReceiverInputDStream[String] = ssc.socketTextStream(<span class="hljs-string">&quot;node1&quot;</span>, <span class="hljs-number">9999</span>)<br>      <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 5.wordcount, 并做累计统计</span><br>      <span class="hljs-comment">//自定义一个函数, 实现保存State状态和数据聚合</span><br>      <span class="hljs-comment">//seq里面是value的数组,[1,1,], state是上次的状态, 累计值</span><br>      val updateFunc = (seq: Seq[Int], state: Option[Int]) =&gt; &#123;<br>        <span class="hljs-keyword">if</span> (!seq.isEmpty) &#123;<br>          val this_value: Int = seq.sum<br>          val last_value: Int = state.getOrElse(<span class="hljs-number">0</span>)<br>          val new_state: Int = this_value + <span class="hljs-function">last_value</span><br><span class="hljs-function">          <span class="hljs-title">Some</span><span class="hljs-params">(new_state)</span></span><br><span class="hljs-function">        &#125;</span><br><span class="hljs-function">        <span class="hljs-keyword">else</span> </span>&#123;<br>          state<br>        &#125;<br>      &#125;<br>      <span class="hljs-comment">//开始做wordcount,并打印输出</span><br>      val wordDStream: DStream[(String, Int)] = inputDStream.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>        .map((_, <span class="hljs-number">1</span>))<br>        .updateStateByKey(updateFunc)<br>      wordDStream.print()<br>    ssc<br>    &#125;<br><br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    val ssc: StreamingContext = StreamingContext.getOrCreate(CKP, creatingFunc)<br><br>    <span class="hljs-comment">//启动流式应用</span><br>    ssc.start()<br>    <span class="hljs-comment">//让应用一直处于监听状态</span><br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理关闭流式应用</span><br>    ssc.stop(<span class="hljs-keyword">true</span>, <span class="hljs-keyword">true</span>)<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="SparkStreaming-amp-Kafka"><a href="#SparkStreaming-amp-Kafka" class="headerlink" title="SparkStreaming &amp; Kafka"></a>SparkStreaming &amp; Kafka</h3><ul><li>自动提交 Offset</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord<br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><span class="hljs-keyword">import</span> scala.collection.mutable.Set<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: Spark  Kafka自动提交offset</span><br><span class="hljs-comment"> */</span><br>object S1KafkaAutoCommit &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    val kafkaParams = Map(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-keyword">true</span>: java.lang.Boolean) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br><br>    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](<br>      ssc,<br>      LocationStrategies.PreferConsistent,<br>      ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)<br>    )<br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    val resutDStream: DStream[Unit] = kafkaDStream.map(x =&gt; &#123;<br>      println(s<span class="hljs-string">&quot;topic=$&#123;x.topic()&#125;,partition=$&#123;x.partition()&#125;,offset=$&#123;x.offset()&#125;,key=$&#123;x.key()&#125;,value=$&#123;x.value()&#125;&quot;</span>)<br>    &#125;)<br>    <span class="hljs-comment">//打印数据</span><br>    resutDStream.print()<br>    <span class="hljs-comment">//启动并停留</span><br>    ssc.start()<br>    ssc.awaitTermination()<br>    <span class="hljs-comment">//合理化关闭</span><br>    ssc.stop(stopSparkContext = <span class="hljs-keyword">true</span>, stopGracefully = <span class="hljs-keyword">true</span>)<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>手动提交 Offset</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord<br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;CanCommitOffsets, ConsumerStrategies, HasOffsetRanges, KafkaUtils, LocationStrategies, OffsetRange&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: Spark  Kafka 手动提交 offset 到默认 topic</span><br><span class="hljs-comment"> */</span><br>object S2KafkaCommit &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    val kafkaParams = Map(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-keyword">false</span>: java.lang.Boolean) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br><br>    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](<br>      ssc,<br>      LocationStrategies.PreferConsistent,<br>      ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)<br>    )<br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    kafkaDStream.foreachRDD(rdd =&gt; &#123;<br>      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;<br>        <span class="hljs-comment">//对每个批次进行处理</span><br>        <span class="hljs-comment">//提取并打印偏移量范围信息</span><br>        val hasOffsetRanges: HasOffsetRanges = rdd.asInstanceOf[HasOffsetRanges]<br>        val offsetRanges: Array[OffsetRange] = hasOffsetRanges.<span class="hljs-function">offsetRanges</span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)</span></span><br><span class="hljs-function">        offsetRanges.<span class="hljs-title">foreach</span><span class="hljs-params">(println(_)</span>)</span><br><span class="hljs-function">        <span class="hljs-comment">//打印每个批次的具体信息</span></span><br><span class="hljs-function">        rdd.<span class="hljs-title">foreach</span><span class="hljs-params">(x =&gt; &#123;</span></span><br><span class="hljs-params"><span class="hljs-function">          println(s<span class="hljs-string">&quot;topic=$&#123;x.topic()&#125;,partition=$&#123;x.partition()&#125;,offset=$&#123;x.offset()&#125;,key=$&#123;x.key()&#125;,value=$&#123;x.value()&#125;&quot;</span>)</span></span><br><span class="hljs-function">        &#125;)</span><br><span class="hljs-function">        <span class="hljs-comment">//手动将偏移量访问信息提交到默认主题</span></span><br><span class="hljs-function">        kafkaDStream.asInstanceOf[CanCommitOffsets].<span class="hljs-title">commitAsync</span><span class="hljs-params">(offsetRanges)</span></span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;成功提交了偏移量信息&quot;</span>)</span></span><br><span class="hljs-function">      &#125;</span><br><span class="hljs-function">    &#125;)</span><br><span class="hljs-function"></span><br><span class="hljs-function">    <span class="hljs-comment">//启动并停留</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">start</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">awaitTermination</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    <span class="hljs-comment">//合理化关闭</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">stop</span><span class="hljs-params">(<span class="hljs-keyword">true</span>,   <span class="hljs-keyword">true</span>)</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">  &#125;</span><br><span class="hljs-function"></span><br><span class="hljs-function">&#125;</span><br></code></pre></td></tr></table></figure><ul><li><p>手动提交 Offset 到 MySQL</p><ul><li>S3KafkaOffsetToMysql</li></ul>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord<br><span class="hljs-keyword">import</span> org.apache.kafka.common.TopicPartition<br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.InputDStream<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010._<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;<br><br><span class="hljs-keyword">import</span> scala.collection.mutable<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: Spark  Kafka 手动提交 offset 到默认 MySQL</span><br><span class="hljs-comment"> */</span><br>object S3KafkaOffsetToMysql &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//创建上下文对象</span><br>    val conf = <span class="hljs-keyword">new</span> SparkConf()<br>      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    val sc = <span class="hljs-keyword">new</span> SparkContext(conf)<br>    val ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))<br>    <span class="hljs-comment">//准备kafka连接参数</span><br>    val kafkaParams = Map(<br>      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,<br>      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//key的反序列化规则</span><br>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[StringDeserializer], <span class="hljs-comment">//value的反序列化规则</span><br>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span><br>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span><br>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span><br>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span><br>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span><br>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span><br>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-keyword">false</span>: java.lang.Boolean) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span><br>    )<br>    <span class="hljs-comment">//去MySQL查询上次消费的位置</span><br>    val offsetMap: mutable.Map[TopicPartition, Long] = OffsetUtil.getOffsetMap(<span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-string">&quot;spark_kafka&quot;</span>)<br><br>    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span><br>    <span class="hljs-keyword">var</span> kafkaDStream: InputDStream[ConsumerRecord[String, String]] = <span class="hljs-keyword">null</span><br>    <span class="hljs-comment">//第一次查询, MySQL没有 offset 数据</span><br>    <span class="hljs-keyword">if</span> (offsetMap.isEmpty) &#123;<br>      kafkaDStream = KafkaUtils.createDirectStream[String, String](<br>        ssc,<br>        LocationStrategies.PreferConsistent,<br>        ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams) <span class="hljs-comment">//第一次就看 Kafka 发啥</span><br>      )<br>    &#125;<br>    <span class="hljs-comment">//第二次查询, MySQL中有 offset 数据</span><br>    <span class="hljs-keyword">else</span> &#123;<br>      kafkaDStream = KafkaUtils.createDirectStream[String, String](<br>        ssc,<br>        LocationStrategies.PreferConsistent,<br>        ConsumerStrategies.Subscribe[String, String](Set(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams, offsetMap) <span class="hljs-comment">//第二次开始就从 MySQL 获取</span><br>      )<br>    &#125;<br><br>    <span class="hljs-comment">//对每个批次进行处理</span><br>    kafkaDStream.foreachRDD(rdd =&gt; &#123;<br>      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;<br>        <span class="hljs-comment">//提取并打印偏移量范围信息</span><br>        val hasOffsetRanges: HasOffsetRanges = rdd.asInstanceOf[HasOffsetRanges]<br>        val offsetRanges: Array[OffsetRange] = hasOffsetRanges.<span class="hljs-function">offsetRanges</span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)</span></span><br><span class="hljs-function">        offsetRanges.<span class="hljs-title">foreach</span><span class="hljs-params">(println(_)</span>)</span><br><span class="hljs-function">        <span class="hljs-comment">//打印每个批次的具体信息</span></span><br><span class="hljs-function">        rdd.<span class="hljs-title">foreach</span><span class="hljs-params">(x =&gt; &#123;</span></span><br><span class="hljs-params"><span class="hljs-function">          println(s<span class="hljs-string">&quot;topic=$&#123;x.topic()&#125;,partition=$&#123;x.partition()&#125;,offset=$&#123;x.offset()&#125;,key=$&#123;x.key()&#125;,value=$&#123;x.value()&#125;&quot;</span>)</span></span><br><span class="hljs-function">        &#125;)</span><br><span class="hljs-function">        <span class="hljs-comment">//手动将偏移量访问信息提交到MySQL</span></span><br><span class="hljs-function">        OffsetUtil.<span class="hljs-title">saveOffsetRanges</span><span class="hljs-params">(<span class="hljs-string">&quot;spark&quot;</span>, offsetRanges)</span></span><br><span class="hljs-function">        <span class="hljs-title">println</span><span class="hljs-params">(<span class="hljs-string">&quot;成功提交了偏移量到MySQL&quot;</span>)</span></span><br><span class="hljs-function">      &#125;</span><br><span class="hljs-function">    &#125;)</span><br><span class="hljs-function"></span><br><span class="hljs-function">    <span class="hljs-comment">//启动并停留</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">start</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">awaitTermination</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    <span class="hljs-comment">//合理化关闭</span></span><br><span class="hljs-function">    ssc.<span class="hljs-title">stop</span><span class="hljs-params">(stopSparkContext = <span class="hljs-keyword">true</span>, stopGracefully = <span class="hljs-keyword">true</span>)</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">  &#125;</span><br><span class="hljs-function"></span><br><span class="hljs-function">&#125;</span><br></code></pre></td></tr></table></figure><ul><li>OffsetUtil</li></ul>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.streaming<br><br><span class="hljs-keyword">import</span> org.apache.kafka.common.TopicPartition<br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.OffsetRange<br><span class="hljs-keyword">import</span> scala.collection.mutable.Map<br><br><span class="hljs-keyword">import</span> java.sql.&#123;DriverManager, ResultSet&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 定义一个单例对象, 定义 2 个方法</span><br><span class="hljs-comment"> *        方法1: 从 MySQL 读取行偏移量</span><br><span class="hljs-comment"> *        方法2: 将行偏移量保存的 MySQL</span><br><span class="hljs-comment"> */</span><br>object OffsetUtil &#123;<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * 定义一个单例方法, 将偏移量保存到MySQL数据库</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> groupid     消费者组id</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> offsetRange 行偏移量对象</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function">def <span class="hljs-title">saveOffsetRanges</span><span class="hljs-params">(groupid: String, offsetRange: Array[OffsetRange])</span> </span>= &#123;<br>    val connection = DriverManager.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>)<br>    <span class="hljs-comment">//replace into表示之前有就替换,没有就插入</span><br>    val ps = connection.prepareStatement(<span class="hljs-string">&quot;replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)&quot;</span>)<br>    <span class="hljs-keyword">for</span> (o &lt;- offsetRange) &#123;<br>      ps.setString(<span class="hljs-number">1</span>, o.topic)<br>      ps.setInt(<span class="hljs-number">2</span>, o.partition)<br>      ps.setString(<span class="hljs-number">3</span>, groupid)<br>      ps.setLong(<span class="hljs-number">4</span>, o.untilOffset)<br>      ps.executeUpdate()<br>    &#125;<br>    ps.close()<br>    connection.close()<br>  &#125;<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * 定义一个方法, 用于从 MySQL 中读取行偏移位置</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> groupid 消费者组id</span><br><span class="hljs-comment">   * <span class="hljs-doctag">@param</span> topic   想要消费的数据主题</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function">def <span class="hljs-title">getOffsetMap</span><span class="hljs-params">(groupid: String, topic: String)</span> </span>= &#123;<br><br>    <span class="hljs-comment">//1.从数据库查询对应数据</span><br>    val connection = DriverManager.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>,<br>      <span class="hljs-string">&quot;root&quot;</span>)<br><br>    val ps = connection.prepareStatement(<span class="hljs-string">&quot;select * from t_offset where groupid=?  and topic=?&quot;</span>)<br>    ps.setString(<span class="hljs-number">1</span>, groupid)<br>    ps.setString(<span class="hljs-number">2</span>, topic)<br>    val rs: ResultSet = ps.executeQuery()<br>    <span class="hljs-comment">//解析数据, 返回</span><br>    <span class="hljs-keyword">var</span> offsetMap = Map[TopicPartition, Long]()<br>    <span class="hljs-keyword">while</span> (rs.next()) &#123;<br>      val topicPartition = <span class="hljs-keyword">new</span> TopicPartition(rs.getString(<span class="hljs-string">&quot;topic&quot;</span>), rs.getInt(<span class="hljs-string">&quot;partition&quot;</span>))<br><br>      offsetMap.put(topicPartition, (rs.getLong(<span class="hljs-string">&quot;offset&quot;</span>)))<br>    &#125;<br>    rs.close()<br>    rs.close()<br>    connection.close()<br>    offsetMap<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure></li></ul><h3 id="StructuredStreaming"><a href="#StructuredStreaming" class="headerlink" title="StructuredStreaming"></a>StructuredStreaming</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day07.structuredStreaming<br><br><span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;DataFrame, Dataset, Row, SparkSession&#125;<br><span class="hljs-keyword">import</span> org.apache.spark.sql.streaming.StreamingQuery<br><span class="hljs-keyword">import</span> org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: wordcount 案例 之 读取文件</span><br><span class="hljs-comment"> */</span><br>object S2StructuredStreamingTextFile &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    <span class="hljs-comment">//1.创建上下文对象</span><br>    val spark: SparkSession = SparkSession.builder()<br>      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))<br>      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)<br>      .getOrCreate()<br>    <span class="hljs-comment">//2.读取csv, 得到流式DataFrame, 每行就是每批次的行数据</span><br>    <span class="hljs-comment">//自定义 Schema 信息</span><br>    val schema = <span class="hljs-keyword">new</span> StructType(Array(<br>      StructField(<span class="hljs-string">&quot;name&quot;</span>, StringType),<br>      StructField(<span class="hljs-string">&quot;age&quot;</span>, IntegerType),<br>      StructField(<span class="hljs-string">&quot;hobby&quot;</span>, StringType))<br>    )<br>    val inputDF: DataFrame = spark.readStream<br>      .format(<span class="hljs-string">&quot;csv&quot;</span>)<br>      .option(<span class="hljs-string">&quot;sep&quot;</span>, <span class="hljs-string">&quot;;&quot;</span>)<br>      .schema(schema)<br>      .load(<span class="hljs-string">&quot;src/main/data/input/persons&quot;</span>)<br>    <span class="hljs-comment">//3.进行wordcount, DSL风格</span><br>    inputDF.printSchema()<br>    <span class="hljs-comment">//用 DSL 风格实现</span><br>    <span class="hljs-keyword">import</span> spark.implicits._<br>    val DF: Dataset[Row] = inputDF.where(<span class="hljs-string">&quot;age&lt;25&quot;</span>)<br>      .groupBy(<span class="hljs-string">&quot;hobby&quot;</span>)<br>      .count()<br>      .orderBy($<span class="hljs-string">&quot;count&quot;</span>.desc)<br><br>    <span class="hljs-comment">// 用 SQL 风格实现</span><br>    inputDF.createOrReplaceTempView(<span class="hljs-string">&quot;t_spark&quot;</span>)<br>    val DF2: DataFrame = spark.sql(<br>      <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">        |select</span><br><span class="hljs-string">        |hobby,</span><br><span class="hljs-string">        |count(1) as cnt</span><br><span class="hljs-string">        |from t_spark</span><br><span class="hljs-string">        |where age&lt;25</span><br><span class="hljs-string">        |group by hobby</span><br><span class="hljs-string">        |order by cnt desc</span><br><span class="hljs-string">        |&quot;</span><span class="hljs-string">&quot;&quot;</span>.stripMargin)<br><br>    val query: StreamingQuery = DF.writeStream<br>      <span class="hljs-comment">//append 默认追加 输出新的数据, 只支持简单查询, 有聚合就不能使用</span><br>      <span class="hljs-comment">//complete:完整模式, 输出完整数据, 支持集合和排序</span><br>      <span class="hljs-comment">//update: 更新模式, 输出有更新的数据,  支持聚合但是不支持排序</span><br>      .outputMode(<span class="hljs-string">&quot;complete&quot;</span>)<br>      .format(<span class="hljs-string">&quot;console&quot;</span>)<br>      .option(<span class="hljs-string">&quot;rowNumber&quot;</span>, <span class="hljs-number">10</span>)<br>      .option(<span class="hljs-string">&quot;truncate&quot;</span>, <span class="hljs-keyword">false</span>)<br>      <span class="hljs-comment">//4.启动流式查询</span><br>      .start()<br>    <span class="hljs-comment">//5.驻留监听</span><br>    query.awaitTermination()<br>    <span class="hljs-comment">//6.关闭流式查询</span><br>    query.stop()<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="批处理-DataSet"><a href="#批处理-DataSet" class="headerlink" title="批处理 DataSet"></a>批处理 DataSet</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.flink.start;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.operators.Order;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.operators.*;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span>: Jface</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Date</span>: 2021/9/5 12:37</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 基于Flink引擎实现批处理词频统计WordCount：过滤filter、排序sort等操作</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_01WordCount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//1.准备环境-env</span><br>        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();<br>        <span class="hljs-comment">//2.准备数据-source</span><br>        DataSource&lt;String&gt; inputDataSet = env.readTextFile(<span class="hljs-string">&quot;datas/wc.input&quot;</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.1 过滤脏数据</span><br>        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataSet.filter(<span class="hljs-keyword">new</span> FilterFunction&lt;String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">filter</span><span class="hljs-params">(String line)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span> != line &amp;&amp; line.trim().length() &gt; <span class="hljs-number">0</span>;<br>            &#125;<br>        &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.2 切割</span><br>                .flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>                    <span class="hljs-meta">@Override</span><br>                    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String line, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                        <span class="hljs-keyword">for</span> (String s : line.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>)) &#123;<br>                            out.collect(s);<br>                        &#125;<br>                    &#125;<br>                &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.3 转换二元组</span><br>                .map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>                    <span class="hljs-meta">@Override</span><br>                    <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String word)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                        <span class="hljs-keyword">return</span> Tuple2.of(word, <span class="hljs-number">1</span>);<br>                    &#125;<br>                &#125;)<br>                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.4 分组求和</span><br>                .groupBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataSet.printToErr();<br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> sort 排序，全局排序需要设置分区数 1</span><br>        SortPartitionOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sortDataSet = resultDataSet.sortPartition(<span class="hljs-string">&quot;f1&quot;</span>, Order.DESCENDING)<br>                .setParallelism(<span class="hljs-number">1</span>);<br>        sortDataSet.printToErr();<br>        <span class="hljs-comment">//只选择前3的数据</span><br>        GroupReduceOperator&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; resultDataSet2 = sortDataSet.first(<span class="hljs-number">3</span>);<br>        resultDataSet2.print();<br><br>        <span class="hljs-comment">//5.触发执行-execute，没有写出不需要触发执行</span><br><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="流处理-DataStream"><a href="#流处理-DataStream" class="headerlink" title="流处理 DataStream"></a>流处理 DataStream</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.stream;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现实时流式数据处理，监听端口并做 wordcount</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StreamWordcount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>         <span class="hljs-comment">//1.准备环境-env</span><br>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();<br>         <span class="hljs-comment">//2.准备数据-source</span><br>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span><br>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);<br>                <span class="hljs-keyword">for</span> (String s : arr) &#123;<br>                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span><br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span><br>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> Tuple2.of(value,<span class="hljs-number">1</span>);<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span><br>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataSet.print();<br>        <span class="hljs-comment">//5.触发执行-execute</span><br>        env.execute(StreamWordcount.class.getSimpleName());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>流处理 Flink On Yarn</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.submit;<br><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现流式数据处理，从socket 接收数据并做 wordcount</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Wordcount</span> </span>&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>        <span class="hljs-comment">//0.使用工具类，解析程序传递参数</span><br>        ParameterTool parameterTool = ParameterTool.fromArgs(args);<br>        <span class="hljs-keyword">if</span> (parameterTool.getNumberOfParameters() != <span class="hljs-number">2</span>) &#123;<br>            System.out.println(<span class="hljs-string">&quot;Usage: WordCount --host &lt;host&gt; --port &lt;port&gt; ............&quot;</span>);<br>            System.exit(-<span class="hljs-number">1</span>);<br>        &#125;<br>        String host = parameterTool.get(<span class="hljs-string">&quot;host&quot;</span>);<br>        parameterTool.getInt(<span class="hljs-string">&quot;port&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//1.准备环境-env</span><br>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();<br>        <span class="hljs-comment">//2.准备数据-source</span><br>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);<br>        <span class="hljs-comment">//3.处理数据-transformation</span><br>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span><br>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataStream = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);<br>                <span class="hljs-keyword">for</span> (String s : arr) &#123;<br>                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span><br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span><br>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;<br>                <span class="hljs-keyword">return</span> Tuple2.of(value, <span class="hljs-number">1</span>);<br>            &#125;<br>            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span><br>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//4.输出结果-sink</span><br>        resultDataStream.print();<br>        <span class="hljs-comment">//5.触发执行-execute</span><br>        env.execute(Wordcount.class.getSimpleName());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>开发样例</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>Hadoop</tag>
      
      <tag>Scala</tag>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Spark：分布式并行计算框架（一）</title>
    <link href="/2020/09/22/Spark-Review-Day01/"/>
    <url>/2020/09/22/Spark-Review-Day01/</url>
    
    <content type="html"><![CDATA[<h2 id="0、前言说明"><a href="#0、前言说明" class="headerlink" title="0、前言说明"></a>0、前言说明</h2><p>整理和汇总一下 Spark 容易混淆的概念和理论。</p><h2 id="1、Spark-框架概念"><a href="#1、Spark-框架概念" class="headerlink" title="1、Spark 框架概念"></a>1、Spark 框架概念</h2><p><img src="https://i.loli.net/2021/09/06/LBadXZ5iyTtYvSg.png" alt="1629968614342"></p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs fortran">Apache Spark™ is a unified analytics engine for large-<span class="hljs-built_in">scale</span> <span class="hljs-keyword">data</span> processing.<br><br><span class="hljs-number">1</span>、unified 统一<br>Spark 框架可以对任意业务需求进行数据分析<br>批处理：SparkCore、交互式分析：SparkSQL、流式计算：SparkStreaming和StructuredStreaming<br>图计算：SparkGraphX、机器学习：SparkMLlib<br>数据科学：PySpark和SparkR<br><br><span class="hljs-number">2</span>、 large-<span class="hljs-built_in">scale</span> 大规模<br>海量数据<br>类似MapReduce对海量数据处理分析<br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/06/h8eibz9poSO13Pu.png" alt="1630828094671"></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">unified 统一不仅仅是分析，也可以指：Spark 分析数据时，可以时任意数据源<br>SparkSQL提供一套外部数据源接口，任何数据源只要实现接口，可以读写数据<span class="hljs-built_in">read</span>或<span class="hljs-built_in">write</span><br></code></pre></td></tr></table></figure><h2 id="2、Spark与MapReduce相比"><a href="#2、Spark与MapReduce相比" class="headerlink" title="2、Spark与MapReduce相比"></a>2、Spark与MapReduce相比</h2><blockquote><p>面试题：Spark 与MapReduce相比为什么快，有什么不同？？？？</p></blockquote><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs arduino">其一、Spark处理数据时，可以将中间处理结果数据存储到内存中；<br>第二、Spark Job调度以DAG方式，并且每个任务<span class="hljs-built_in">Task</span>执行以线程（Thread）方式，并不是像MapReduce以进程（<span class="hljs-built_in">Process</span>）方式执行。<br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/06/XtxmcTysVnEBSrR.png" alt="1629969282252"></p><blockquote><p>Spark与MapReduce最大不同：<a href="">提供数据结构RDD弹性分布式数据集。</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/NpW5Pehzy2AwM7J.png" alt="1629969337722"></p><h2 id="3、RDD是什么，如何理解"><a href="#3、RDD是什么，如何理解" class="headerlink" title="3、RDD是什么，如何理解"></a>3、RDD是什么，如何理解</h2><blockquote><p>1、RDD是什么，官方定义：<a href="">不可变、分区的、并行计算的集合，抽象概念</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/sp8T1F39igfHqWv.png" alt="1629969511151"></p><blockquote><p>2、每个RDD内部有5个特性</p><ul><li>分区组成、每个分区被计算处理、依赖一些列RDD</li><li>可选的：KeyValue类型RDD可以设置分区器，对每个分区数据处理时找到最佳位置（数据本地性）</li></ul></blockquote><p><img src="https://i.loli.net/2021/09/06/Nkbi3qxwF1fnEcO.png" alt="1629969541474"></p><blockquote><p>3、常见RDD，<a href="">RDD抽象类，泛型，表示具体存储数据类型未知，可以是任何类型。</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/OKHzQcLeiv2uEfk.png" alt="1629969704615"></p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-number">1</span>、HadoopRDD，表示从文件系统加载数据封装的集合<br><span class="hljs-number">2</span>、MapPartitionsRDD，表示经过转换后的，比如fliter、map、flatMap等<br><span class="hljs-number">3</span>、ShuffleRDD，表示对RDD数据进行处理，产生shuffle时RDD<br></code></pre></td></tr></table></figure><h2 id="4、RDD、DataFrame和DataSet区别"><a href="#4、RDD、DataFrame和DataSet区别" class="headerlink" title="4、RDD、DataFrame和DataSet区别"></a>4、RDD、DataFrame和DataSet区别</h2><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-number">1</span>、<span class="hljs-variable">RDD</span>是啥？？<br>不可变、分区的、并行计算基本<br><span class="hljs-number">2</span>、<span class="hljs-variable">DataFrame</span><br><span class="hljs-variable">DataFrame</span> <span class="hljs-operator">=</span> <span class="hljs-variable">RDD</span><span class="hljs-punctuation">[</span><span class="hljs-built_in">Row</span><span class="hljs-punctuation">]</span> <span class="hljs-operator">+</span> <span class="hljs-variable">Schema</span>（字段名称、字段类型）<br>知道内部结构<br>当在处理分析数据，由于知道内部结构，所以可以先进行优化，在计算分析数据<br><span class="hljs-number">3</span>、<span class="hljs-built_in">Dataset</span><br><span class="hljs-variable">Spark</span> <span class="hljs-number">1.6</span>诞生<br><span class="hljs-built_in">Dataset</span> <span class="hljs-operator">=</span> <span class="hljs-variable">RDD</span> <span class="hljs-operator">+</span> <span class="hljs-variable">Schema</span><br>知道内部结构，也知道外部结构，编程更加安全和方便，内部数据存储使用特殊编码方式，节省空间<br><span class="hljs-number">4</span>、<span class="hljs-variable">Spark</span> <span class="hljs-number">2.0</span>开始<br><span class="hljs-variable">DataFrame</span>和<span class="hljs-built_in">Dataset</span>合并，其中<span class="hljs-variable">DataFrame</span>时<span class="hljs-built_in">Dataset</span>特殊形式，数据类型为<span class="hljs-built_in">Row</span>时数据结构<br><span class="hljs-variable">DataFrame</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">Dataset</span><span class="hljs-punctuation">[</span><span class="hljs-built_in">Row</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/06/iRcO1XegMI4rwN7.png" alt="1629970036519"></p><blockquote><p>从Spark 2.x开始，建议大家使用数据结构为Dataset/DataFrame，因此使用SparkSQL模块分析数据。</p></blockquote><h2 id="5、Spark-on-YARN执行流程"><a href="#5、Spark-on-YARN执行流程" class="headerlink" title="5、Spark on YARN执行流程"></a>5、Spark on YARN执行流程</h2><blockquote><p>将Spark 应用程序（无论批处理还是流计算），提交运行到YARN集群上，提交流程。</p></blockquote><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs haml">Spark Application运行有2种DeployMode：<br>1、client客户端<br>DriverProgram运行在提交运行客户端主机上<br>-<span class="ruby"> DriverProgram，调度Job执行</span><br><span class="ruby"></span>-<span class="ruby"> AppMaster，运行在NM中容器，申请资源运行Executors</span><br><span class="ruby"></span>-<span class="ruby"> Executors，运行在NM中容器，执行Task任务和缓存数据</span><br><span class="ruby"></span><br>2、cluster集群<br>DriverProgram运行在YARN集群NodeManager容器中<br>-<span class="ruby"> AppMaster/DriverProgram，运行在NM中容器，申请资源运行Executors和Job调度执行</span><br><span class="ruby"></span>-<span class="ruby"> Executors，运行在NM中容器，执行Task任务和缓存数据</span><br></code></pre></td></tr></table></figure><blockquote><p><code>yarn-client</code>，测试使用</p></blockquote><p><img src="https://i.loli.net/2021/09/06/Y9Uv8x61aCOydqp.png" alt="1629970339728"></p><blockquote><p><code>yarn-cluster</code>，生成环境运行方式</p></blockquote><p><img src="https://i.loli.net/2021/09/06/hpHweAk9f4JGtNU.png" alt="1629970376799"></p><p><img src="https://i.loli.net/2021/09/06/AthRJxlM8Ir34d9.png" alt="1630831356243"></p><h2 id="6、SparkJob调度过程"><a href="#6、SparkJob调度过程" class="headerlink" title="6、SparkJob调度过程"></a>6、SparkJob调度过程</h2><blockquote><p>词频统计WordCount程序执行DAG图</p></blockquote><p><img src="https://i.loli.net/2021/09/06/UcAPhtBTkDafM9x.png" alt="1629970655391"></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>、RDD<span class="hljs-comment">#Action函数，触发一个Job执行</span><br><span class="hljs-built_in">count</span>、foreach、foreachPartition等等<br><br><br><span class="hljs-number">2</span>、第一步、构建DAG图<br>从触发<span class="hljs-keyword">Job开始RDD，采用回溯法（倒推法，从后向前），依据RDD依赖关系，构建Job对应DAG图</span><br><span class="hljs-keyword"></span>【依赖、回溯法】<br><br><br><span class="hljs-number">3</span>、第二步、划分DAG图为Stage<br>从触发<span class="hljs-keyword">Job开始RDD，采用回溯法，如果2个RDD之间依赖为宽依赖，划分后面为一个Stage，依次类推</span><br><span class="hljs-keyword"></span>DAGScheduler<br>划分完成以后，此时每个<span class="hljs-keyword">Job由多个Stage组成，各个Stage之间相互依赖关系</span><br><span class="hljs-keyword"></span>后面的Stage处理前面Stage的数据<br>相邻<span class="hljs-number">2</span>个Stage之间产生<span class="hljs-keyword">Shuffle</span><br><span class="hljs-keyword"></span>Stage划分为<span class="hljs-number">2</span>种类型：<br>第一、ResultStage，产生Result结果，每个<span class="hljs-keyword">Job中最后一个Stage</span><br><span class="hljs-keyword"></span>第二、<span class="hljs-keyword">ShuffleMapStage，除去Job中最后一个Stage其他Stage都是</span><br><span class="hljs-keyword"></span><br><br><span class="hljs-number">4</span>、第三步、按照<span class="hljs-keyword">Job中Stage顺序，从前向后执行Stage中Task任务</span><br><span class="hljs-keyword"></span>每个Stage中有多个Task任务，逻辑相同，处理数据不同而已<br>将Stage中Task任务打包为TaskSet，发送给Executor执行<br>TaskScheduler<br>问题<span class="hljs-number">1</span>：每个Stage中Task数目如何确定？？<br>由Stage中最后一个RDD分区数目确定<br><br>问题<span class="hljs-number">2</span>：每个Stage中Task任务计算模式是什么？？？？<br>管道计算模式，Pipeline模式<br><br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/06/sJbErFfSDTlMBGp.png" alt="1629971290799"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> cn.test.spark<br><br><span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><br><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SparkWordCount</span> </span>&#123;<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br><span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-type">SparkContext</span>.getOrCreate(<br><span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">&quot;local[1]&quot;</span>).setAppName(<span class="hljs-string">&quot;SparkWordCount&quot;</span>)<br>)<br><br><span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.parallelize(<br><span class="hljs-type">Seq</span>(<span class="hljs-string">&quot;111111111111111111&quot;</span>, <span class="hljs-string">&quot;222222222222222222&quot;</span>, <span class="hljs-string">&quot;3333333333333333333&quot;</span>), numSlices = <span class="hljs-number">2</span><br>)<br>println(<span class="hljs-string">s&quot;RDD 分区数目：<span class="hljs-subst">$&#123;inputRDD.getNumPartitions&#125;</span>&quot;</span>)<br><br><br><span class="hljs-keyword">val</span> resultRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = inputRDD<br>.filter(line =&gt; &#123;<br>println(<span class="hljs-string">&quot;filter........................&quot;</span>)<br><span class="hljs-comment">// 直接返回</span><br><span class="hljs-literal">true</span><br>&#125;)<br>.flatMap(line =&gt; &#123;<br>println(<span class="hljs-string">&quot;flatMap........................&quot;</span>)<br><span class="hljs-type">Seq</span>(line)<br>&#125;)<br>.map(line =&gt; &#123;<br>println(<span class="hljs-string">&quot;map........................&quot;</span>)<br>line<br>&#125;)<br><span class="hljs-comment">/*</span><br><span class="hljs-comment">filter........................</span><br><span class="hljs-comment">flatMap........................</span><br><span class="hljs-comment">map........................</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">filter........................</span><br><span class="hljs-comment">flatMap........................</span><br><span class="hljs-comment">map........................</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">filter........................</span><br><span class="hljs-comment">flatMap........................</span><br><span class="hljs-comment">map........................</span><br><span class="hljs-comment"> */</span><br><br><span class="hljs-keyword">val</span> count = resultRDD.count()<br>println(<span class="hljs-string">s&quot;Count = <span class="hljs-subst">$&#123;count&#125;</span>&quot;</span>)<br><br>&#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/06/pX9NdjS1q7s2yzM.png" alt="1630833745356"></p><h2 id="7、Spark中依赖类型"><a href="#7、Spark中依赖类型" class="headerlink" title="7、Spark中依赖类型"></a>7、Spark中依赖类型</h2><blockquote><p>主要RDD依赖分为2类：</p><ul><li><strong>窄依赖</strong>：1对1，父RDD1个分区数据对应子RDD1个分区数据</li><li><code>宽依赖</code>：1对多，父RDD1个分区数据对应子RDD多个分区数据</li></ul></blockquote><p><img src="https://i.loli.net/2021/09/06/uPGhXzADYrBcZOw.png" alt="1630826688052"></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">宽依赖：<span class="hljs-keyword">Shuffle依赖</span><br><span class="hljs-keyword"></span>相邻RDD产生<span class="hljs-keyword">Shuffle</span><br><span class="hljs-keyword"></span><br>Spark <span class="hljs-keyword">Shuffle </span>分为<span class="hljs-number">2</span>个部分：<br><span class="hljs-number">1</span>、<span class="hljs-keyword">Shuffle </span>Writer（上游Stage产生）<br>将<span class="hljs-keyword">Shuffle数据写磁盘，有3种方式，具体由底层自动选择</span><br><span class="hljs-keyword"></span><br><span class="hljs-number">2</span>、<span class="hljs-keyword">ShuffleReader（下游Stage产生）</span><br><span class="hljs-keyword"></span>读取<span class="hljs-keyword">Shuffle到磁盘中数据，进行处理</span><br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/06/2Wmqk5Rigp63oVG.png" alt="1630834291605"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive自动化建库建表</title>
    <link href="/2020/09/12/Hive%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BB%BA%E5%BA%93%E5%BB%BA%E8%A1%A8/"/>
    <url>/2020/09/12/Hive%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BB%BA%E5%BA%93%E5%BB%BA%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>项目数仓数据源太多，于是自己写了一个工具类，读取数据源的元数据信息，自动建库建表</p><p>以 MySQL 为例，代码如下。</p><h3 id="HiveUtil"><a href="#HiveUtil" class="headerlink" title="HiveUtil"></a>HiveUtil</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs java">object HiveUtil &#123;<br>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;<br>    createHiveTable()<br>  &#125;<br><br>  <span class="hljs-function">def <span class="hljs-title">createHiveTable</span><span class="hljs-params">()</span> </span>= &#123;<br>    <span class="hljs-comment">//连接MySQL，读取MySQL表名有哪些字段，字段类型，字段的注释</span><br>    val table_arr = Array(<br>      <span class="hljs-string">&quot;area&quot;</span>,<br>      <span class="hljs-string">&quot;claim_info&quot;</span>,<br>      <span class="hljs-string">&quot;dd_table&quot;</span>,<br>      <span class="hljs-string">&quot;mort_10_13&quot;</span>,<br>      <span class="hljs-string">&quot;policy_acuary&quot;</span>,<br>      <span class="hljs-string">&quot;policy_benefit&quot;</span>,<br>      <span class="hljs-string">&quot;policy_client&quot;</span>,<br>      <span class="hljs-string">&quot;policy_surrender&quot;</span>,<br>      <span class="hljs-string">&quot;pre_add_exp_ratio&quot;</span>,<br>      <span class="hljs-string">&quot;prem_cv_real&quot;</span>,<br>      <span class="hljs-string">&quot;prem_std_real&quot;</span>)<br><br>    val conn = DriverManager.getConnection(<span class="hljs-string">&quot;jdbc:mysql://node3:3306/insurance&quot;</span>, <span class="hljs-string">&quot;root&quot;</span>, <span class="hljs-string">&quot;123456&quot;</span>)<br>    val ps: PreparedStatement = conn.prepareStatement(<br>      s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">         |SELECT</span><br><span class="hljs-string">         |       COLUMN_NAME,</span><br><span class="hljs-string">         |       COLUMN_TYPE,</span><br><span class="hljs-string">         |       COLUMN_COMMENT</span><br><span class="hljs-string">         |FROM information_schema.COLUMNS</span><br><span class="hljs-string">         |WHERE upper(TABLE_NAME)  = upper(?)</span><br><span class="hljs-string">         |  and upper(TABLE_SCHEMA)=upper(?)</span><br><span class="hljs-string">         |order by ORDINAL_POSITION</span><br><span class="hljs-string">         |&quot;</span><span class="hljs-string">&quot;&quot;</span>.stripMargin)<br>    <span class="hljs-keyword">var</span> rs: ResultSet = <span class="hljs-function"><span class="hljs-keyword">null</span></span><br><span class="hljs-function">    <span class="hljs-title">for</span> <span class="hljs-params">(tablename &lt;- table_arr)</span> </span>&#123;<br>      ps.setString(<span class="hljs-number">1</span>,tablename)<br>      ps.setString(<span class="hljs-number">2</span>,<span class="hljs-string">&quot;insurance&quot;</span>)<br>      rs = ps.executeQuery()<br>      <span class="hljs-keyword">var</span> str =<br>        s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">           |drop table if exists $&#123;tablename&#125;;</span><br><span class="hljs-string">           |create table if not exists $&#123;tablename&#125; (\n&quot;</span><span class="hljs-string">&quot;&quot;</span>.<span class="hljs-function">stripMargin</span><br><span class="hljs-function">      <span class="hljs-title">while</span> <span class="hljs-params">(rs.next()</span>) </span>&#123;<br>        val column_name: String = rs.getString(<span class="hljs-number">1</span>)<br>        val column_type: String = rs.getString(<span class="hljs-number">2</span>)<br>        val column_comment: String = rs.getString(<span class="hljs-number">3</span>)<br>        <span class="hljs-keyword">var</span> temp_type = <span class="hljs-function">column_type</span><br><span class="hljs-function">        <span class="hljs-title">if</span> <span class="hljs-params">(temp_type.contains(<span class="hljs-string">&quot;int&quot;</span>)</span>) </span>&#123;<br>          <span class="hljs-comment">//30000-&gt;int(5)-&gt;smallint</span><br>          <span class="hljs-comment">//300000000-&gt;int(5-16)-&gt;int</span><br>          <span class="hljs-comment">//3000000000000000000-&gt;int(16-32)-&gt;bigint</span><br>          val <span class="hljs-keyword">int</span>: Int = <span class="hljs-string">&quot;int(11)&quot;</span>.split(<span class="hljs-string">&quot;\\(|\\)&quot;</span>)(<span class="hljs-number">1</span>).<span class="hljs-function">toInt</span><br><span class="hljs-function">          <span class="hljs-title">if</span> <span class="hljs-params">(<span class="hljs-keyword">int</span> &gt; <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-keyword">int</span> &lt;= <span class="hljs-number">5</span>)</span> </span>&#123;<br>            temp_type = <span class="hljs-string">&quot;smallint&quot;</span><br>          &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-keyword">int</span> &gt; <span class="hljs-number">5</span> &amp;&amp; <span class="hljs-keyword">int</span> &lt;= <span class="hljs-number">16</span>) &#123;<br>            temp_type = <span class="hljs-string">&quot;int&quot;</span><br>          &#125; <span class="hljs-keyword">else</span> &#123;<br>            temp_type = <span class="hljs-string">&quot;bigint&quot;</span><br>          &#125;<br>        &#125;<br>        <span class="hljs-keyword">if</span>(temp_type.contains(<span class="hljs-string">&quot;varchar&quot;</span>) || temp_type.contains(<span class="hljs-string">&quot;text&quot;</span>))&#123;<br>          temp_type=<span class="hljs-string">&quot;string&quot;</span><br>        &#125;<br>        <span class="hljs-comment">//println(column_name,column_type,column_comment)</span><br>        str += s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;$&#123;column_name&#125;   $&#123;temp_type&#125;  comment &#x27;$&#123;column_comment&#125;&#x27;,\n&quot;</span><span class="hljs-string">&quot;&quot;</span><br>      &#125;<br>      str = str.stripSuffix(<span class="hljs-string">&quot;,\n&quot;</span>)<br>      str += <span class="hljs-string">&quot;) comment &#x27;&#x27; \n row format delimited fields terminated by &#x27;\\t&#x27; ; \n&quot;</span><br>      println(str)<br>    &#125;<br><br>    rs.close()<br>    ps.close()<br>    conn.close()<br>    <span class="hljs-comment">//解析上面的元数据，拼接成hive版的ddl语句</span><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sqoop自动化抽取数据与验证</title>
    <link href="/2020/09/12/Sqoop%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AA%8C%E8%AF%81/"/>
    <url>/2020/09/12/Sqoop%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AA%8C%E8%AF%81/</url>
    
    <content type="html"><![CDATA[<h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>最近项目业务数据源多种多样，用 Sqoop 抽取数据到数仓是一个体力活，底层又是基于 MapReduce 执行的，速度感人，关键是还得做数据校验</p><p>于是想着自己写个工具类，和自动建表建库类似，自动读取数据源表和字段信息，创建对应脚本，扔到 DolphinScheduler 上自动跑就完事。</p><h3 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h3><p>1. 自定义工具类，读取 MySQL 中 information_schema 库下的 TABLES 表 获取同名的表</p><p>2. 获取到表名的容器，然后按照固定格式以文本形式写到 HDFS上文件夹上</p><p>3. 脚本内容需要做数据校验并将校验结果，并且加上并行执行符号</p><p>4. DolphinScheduler 上新建工作流，定期执行脚本文件</p><h3 id="Sqoop-数据抽取脚本"><a href="#Sqoop-数据抽取脚本" class="headerlink" title="Sqoop 数据抽取脚本"></a>Sqoop 数据抽取脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<br><span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \<br>--connect jdbc:mysql://192.168.88.163:3306/insurance \<br>--username root \<br>--password 123456 \<br>--table dd_table \<br>--hive-table insurance_ods.dd_table \<br>--hive-import \<br>--hive-overwrite \<br>--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \<br>--delete-target-dir \<br>-m 1<br><br></code></pre></td></tr></table></figure><h3 id="Sqoop-抽取数据-数据验证"><a href="#Sqoop-抽取数据-数据验证" class="headerlink" title="Sqoop 抽取数据+数据验证"></a>Sqoop 抽取数据+数据验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<br><span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \<br>--connect jdbc:mysql://192.168.88.163:3306/insurance \<br>--username root \<br>--password 123456 \<br>--table dd_table \<br>--hive-table insurance_ods.dd_table \<br>--hive-import \<br>--hive-overwrite \<br>--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \<br>--delete-target-dir \<br>-m 1<br><br><span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span><br>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \<br>--connect jdbc:mysql://192.168.88.163:3306/insurance \<br>--username root \<br>--password 123456 \<br>--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span><br>`<br>mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<br><span class="hljs-comment">#2、查询hive的表dd_table的条数</span><br>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<br><br><span class="hljs-comment">#3、比较2边的数字是否一样。</span><br><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><br><span class="hljs-keyword">else</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><h3 id="SqoopUtil"><a href="#SqoopUtil" class="headerlink" title="SqoopUtil"></a>SqoopUtil</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs sql">object SqoopUtil &#123;<br>  def main(args: <span class="hljs-keyword">Array</span>[String]): Unit <span class="hljs-operator">=</span> &#123;<br>    createHiveTable()<br>  &#125;<br><br>  def createHiveTable() <span class="hljs-operator">=</span> &#123;<br>    <span class="hljs-operator">/</span><span class="hljs-operator">/</span>连接MySQL，读取MySQL表名有哪些字段，字段类型，字段的注释<br>    val table_arr <span class="hljs-operator">=</span> <span class="hljs-keyword">Array</span>(<br>      &quot;area&quot;,<br>      &quot;policy_acuary&quot;,<br>      &quot;policy_benefit&quot;,<br>      &quot;policy_client&quot;,<br>      &quot;policy_surrender&quot;,<br>    <span class="hljs-keyword">for</span> (tablename <span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span> table_arr) &#123;<br>      <span class="hljs-operator">/</span><span class="hljs-operator">/</span>var str <span class="hljs-operator">=</span><br>      <span class="hljs-operator">/</span><span class="hljs-operator">/</span>  s&quot;&quot;&quot;/export/server/sqoop/bin/sqoop import  --connect jdbc:mysql://192.168.88.163:3306/insurance  --username root  --password 123456  --table $&#123;tablename&#125;  --hive-table insurance_ods.$&#123;tablename&#125;  --hive-import  --hive-overwrite  --fields-terminated-by &#x27;\\t&#x27;  -m 1 \n&quot;&quot;&quot;.stripMargin<br>      var str1 <span class="hljs-operator">=</span><br>        s&quot;&quot;&quot;/export/server/sqoop/bin/sqoop import  \\<br>           |--connect jdbc:m1ysql://192.168.88.163:3306/insurance  \\<br>           |--username root  \\<br>           |--password 123456  \\<br>           |--table $&#123;tablename&#125;  \\<br>           |--hive-table insurance_ods.$&#123;tablename&#125;  \\<br>           |--hive-import  \\<br>           |--hive-overwrite  \\<br>           |--fields-terminated-by &#x27;\\t&#x27;  \\<br>           |-m 1&quot;&quot;&quot;.stripMargin<br>      println(str1)<br>    &#125;<br><br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>Sqoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多表连接过滤条件在 on 和 where 的区别</title>
    <link href="/2020/05/24/%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5%E8%BF%87%E6%BB%A4%E6%9D%A1%E4%BB%B6%E5%9C%A8%20on%20%E5%92%8C%20where%20%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <url>/2020/05/24/%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5%E8%BF%87%E6%BB%A4%E6%9D%A1%E4%BB%B6%E5%9C%A8%20on%20%E5%92%8C%20where%20%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h3 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h3><p>最近项目中的小坑，记录一下。</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student<br>(<br>    sid   <span class="hljs-type">int</span> <span class="hljs-keyword">primary</span> key  <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ,<br>    cid   <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>,<br>    t_sex <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">null</span><br>)<br>    comment <span class="hljs-string">&#x27;学生表&#x27;</span>;<br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> t_score<br>(<br>    sid    <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>,<br>    course <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">null</span>,<br>    score  <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span><br>)<br>    comment <span class="hljs-string">&#x27;成绩表&#x27;</span>;<br><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test.student <span class="hljs-keyword">values</span><br>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;李白&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),<br>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;杜甫&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),<br>(<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;白居易&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),<br>(<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;苏轼&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),<br>(<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;李清照&#x27;</span>,<span class="hljs-string">&#x27;女&#x27;</span>),<br>(<span class="hljs-number">7</span>,<span class="hljs-string">&#x27;谢道韫&#x27;</span>,<span class="hljs-string">&#x27;女&#x27;</span>),<br>(<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;郭奉孝&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>);<br><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test.t_score <span class="hljs-keyword">values</span><br>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">90</span>),<br>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">50</span>),<br>(<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">99</span>),<br>(<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),<br>(<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),<br>(<span class="hljs-number">6</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),<br>(<span class="hljs-number">7</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),<br>(<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>);<br><br></code></pre></td></tr></table></figure><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><ul><li>在 on 后面</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- 内关联 条件放在 on 和 where 没有区别</span><br><span class="hljs-comment">-- 非空判断放在 on 和 where没有区别，成绩表有只有3个人的成绩，只有3个结果</span><br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student s<br><span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid<br><span class="hljs-keyword">where</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;<br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/03/JIUxdHebiaFhlBz.png" alt="Untitled"></p><ul><li>在 where后面</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student s<br><span class="hljs-keyword">join</span> t_score ts<br><span class="hljs-keyword">where</span> s.sid<span class="hljs-operator">=</span>ts.sid <span class="hljs-keyword">and</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;<br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/03/tkaNXALl8FzIiYT.png" alt="Untitled"></p><h3 id="外连接"><a href="#外连接" class="headerlink" title="外连接"></a>外连接</h3><ul><li>在 on 后面</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- 外连接结果有非常大的区别</span><br><span class="hljs-comment">-- 写在 on 条件上,有7个结果</span><br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student s<br><span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid  <span class="hljs-keyword">and</span>  ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;<br><br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/03/AIrX7xsPdEaqUty.png" alt="Untitled"></p><ul><li>在 where 后面</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- 写在 where 条件上,只有3个结果</span><br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">from</span> student s<br><span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid<br><span class="hljs-keyword">where</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;<br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/09/03/u4tXn2UqxcTOp1z.png" alt="Untitled"></p><h3 id="原因解析"><a href="#原因解析" class="headerlink" title="原因解析"></a>原因解析</h3><p>left join 的时候全部保留左边表格的内容，并保留右边表格能匹配上条件的内容<br>on 后面的就是连接条件，无论写什么只会对右边起效，不影响左表内容<br>where 后面的条件是对全局起效，就表关联之后的结果做筛选。</p>]]></content>
    
    
    <categories>
      
      <category>Debug记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据常用命令</title>
    <link href="/2020/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2020/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 全局组件启动与停止</span><br>start-all.sh<br>stop-all.sh<br><br><span class="hljs-comment"># HDFS 启动与停止</span><br>start-dfs.sh<br>stop-dfs.sh<br><br><span class="hljs-comment"># Yarn 启动与停止</span><br>start-yarn.sh<br>stop-yarn.sh<br><br><span class="hljs-comment"># HDFS 单个启动</span><br>hadoop-daemon.sh start namenode <br><br><span class="hljs-comment"># HDFS 多个启动</span><br>hadoop-daemons.sh start datanode<br><br><span class="hljs-comment"># Yarn 单个启动</span><br>yarn-daemon.sh start resourcemanager <br><br><span class="hljs-comment"># Yarn 多个启动</span><br>yarn-daemons.sh start nodemanager<br><br><span class="hljs-comment"># MR 历史 job记录，端口号 19888</span><br>mr-jobhistory-daemon.sh start historyserver<br><br></code></pre></td></tr></table></figure><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 启动Hive 的元数据服务</span><br>nohup /<span class="hljs-built_in">export</span>/server/hive-2.1.0/bin/hive --service metastore   &amp;<br><br><span class="hljs-comment"># 启动Hive 客户端服务</span><br>nohup /<span class="hljs-built_in">export</span>/server/hive-2.1.0/bin/hiveserver2 start &amp;<br><br><span class="hljs-comment"># hive元数据初始化和更新</span><br>schematool -dbType mysql -initSchema<br>schematool -dbType mysql -upgradeSchema<br></code></pre></td></tr></table></figure><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 全局启动</span><br>zkServer.sh start<br><br><span class="hljs-comment"># 标准启动</span><br>zookeeper-daemon.sh start<br></code></pre></td></tr></table></figure><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="启动与停止"><a href="#启动与停止" class="headerlink" title="启动与停止"></a>启动与停止</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 启动 Kafka 启动服务，需要先启动 zookeeper</span><br>kafka-server-start.sh config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;<br><br><span class="hljs-comment"># 关闭 Kafka 服务</span><br>kafka-server-stop.sh<br><br></code></pre></td></tr></table></figure><h3 id="封装启动脚本-记得给权限"><a href="#封装启动脚本-记得给权限" class="headerlink" title="封装启动脚本, 记得给权限"></a>封装启动脚本, 记得给权限</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<br><br><span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<br><span class="hljs-keyword">do</span><br>        host=node<span class="hljs-variable">$&#123;number&#125;</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span><br>        /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;export JMX_PORT=9988;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-start.sh <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> started&quot;</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h3 id="封装关闭脚本，记得给权限"><a href="#封装关闭脚本，记得给权限" class="headerlink" title="封装关闭脚本，记得给权限"></a>封装关闭脚本，记得给权限</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<br><br><span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<br><span class="hljs-keyword">do</span><br>  host=node<span class="hljs-variable">$&#123;number&#125;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span><br>  /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-stop.sh&quot;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> stoped&quot;</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h3 id="彻底删除kafka主题"><a href="#彻底删除kafka主题" class="headerlink" title="彻底删除kafka主题"></a>彻底删除kafka主题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 命令行</span><br>kafka-topics.sh --zookeeper node3:2181 --list <br>kafka-topics.sh --zookeeper node3:2181 --delete --topic spark_kafka<br><br><span class="hljs-comment"># 客户端</span><br>【zkCli.sh】<br>ls /config/topics<br>rmr /config/topics/spark_kafka<br>rmr /brokers/topics/spark_kafka<br>rmr /admin/delete_topics/spark_kafka<br></code></pre></td></tr></table></figure><h3 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">kafka-topics.sh --zookeeper node3:2181 --create --topic spark_kafka --partitions 3 --replication-factor 1<br>kafka-topics.sh --zookeeper node3:2181 --list<br></code></pre></td></tr></table></figure><h3 id="启动生产者和消费者"><a href="#启动生产者和消费者" class="headerlink" title="启动生产者和消费者"></a>启动生产者和消费者</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">kafka-console-producer.sh --broker-list node3:9092 --topic spark_kafka<br>kafka-console-consumer.sh --from-beginning --bootstrap-server node3:9092 --topic spark_kafka<br>kafka-console-consumer.sh --from-beginning --bootstrap-server node3:9092 --topic __consumer_offsets<br></code></pre></td></tr></table></figure><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="启动spark-thriftserver"><a href="#启动spark-thriftserver" class="headerlink" title="启动spark-thriftserver"></a>启动spark-thriftserver</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">start-thriftserver.sh \<br>  --hiveconf hive.server2.thrift.port=10001 \<br>  --hiveconf hive.server2.thrift.bind.host=node3 \<br>  --master <span class="hljs-built_in">local</span>[*]<br></code></pre></td></tr></table></figure><h3 id="启动-Spark-HistoryServer服务-端口号-18080"><a href="#启动-Spark-HistoryServer服务-端口号-18080" class="headerlink" title="启动 Spark HistoryServer服务, 端口号 18080"></a>启动 Spark HistoryServer服务, 端口号 18080</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sbin/start-history-server.sh<br></code></pre></td></tr></table></figure><h3 id="structured-Streaming"><a href="#structured-Streaming" class="headerlink" title="structured Streaming"></a>structured Streaming</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">--memory sink<br>CREATE TABLE db_spark.tb_word_count (<br>  id int NOT NULL AUTO_INCREMENT,<br>  word varchar(255) NOT NULL,<br>  count int NOT NULL,<br>  PRIMARY KEY (id),<br>  UNIQUE KEY word (word)<br>) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;<br><br>REPLACE INTO  tb_word_count (id, word, count) VALUES (NULL, ?, ?);<br></code></pre></td></tr></table></figure><h3 id="spark-yarn-Pi-测试"><a href="#spark-yarn-Pi-测试" class="headerlink" title="spark yarn Pi 测试"></a>spark yarn Pi 测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \<br>--master yarn \<br>--class org.apache.spark.examples.SparkPi \<br><span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \<br>10<br></code></pre></td></tr></table></figure><h3 id="WordCount-yarn"><a href="#WordCount-yarn" class="headerlink" title="WordCount yarn"></a>WordCount yarn</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \<br>--master yarn \<br>--driver-memory 512m \<br>--executor-memory 512m \<br>--executor-cores 1 \<br>--num-executors 2 \<br>--queue default \<br>--class cn.itcast.spark._2SparkWordCount \<br>/opt/spark-chapter01-1.0-SNAPSHOT.jar<br></code></pre></td></tr></table></figure><h3 id="Spark-submit"><a href="#Spark-submit" class="headerlink" title="Spark-submit"></a>Spark-submit</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs bash">【 Run application <span class="hljs-built_in">local</span> on 8 cores】<br>/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master <span class="hljs-built_in">local</span>[8] \<br><span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \<br>  100<br><br><span class="hljs-comment"># Run on a Spark standalone cluster in client deploy mode</span><br>./bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master spark://207.184.161.138:7077 \<br>  --executor-memory 20G \<br>  --total-executor-cores 100 \<br><span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \<br>  1000<br><br><span class="hljs-comment"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span><br>./bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master spark://207.184.161.138:7077 \<br>  --deploy-mode cluster \<br>  --supervise \<br>  --executor-memory 20G \<br>  --total-executor-cores 100 \<br>  /path/to/examples.jar \<br>  1000<br><br><span class="hljs-comment"># Run on a YARN cluster</span><br><span class="hljs-built_in">export</span> HADOOP_CONF_DIR=XXX<br>./bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master yarn \<br>  --deploy-mode cluster \  <span class="hljs-comment"># can be client for client mode</span><br>  --executor-memory 20G \<br>  --num-executors 50 \<br>  /path/to/examples.jar \<br>  1000<br><br><span class="hljs-comment"># Run a Python application on a Spark standalone cluster</span><br>./bin/spark-submit \<br>  --master spark://207.184.161.138:7077 \<br>  examples/src/main/python/pi.py \<br>  1000<br><br><span class="hljs-comment"># Run on a Mesos cluster in cluster deploy mode with supervise</span><br>./bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master mesos://207.184.161.138:7077 \<br>  --deploy-mode cluster \<br>  --supervise \<br>  --executor-memory 20G \<br>  --total-executor-cores 100 \<br>  http://path/to/examples.jar \<br>  1000<br><br><span class="hljs-comment"># Run on a Kubernetes cluster in cluster deploy mode</span><br>./bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master k8s://xx.yy.zz.ww:443 \<br>  --deploy-mode cluster \<br>  --executor-memory 20G \<br>  --num-executors 50 \<br>  http://path/to/examples.jar \<br>  1000<br></code></pre></td></tr></table></figure><h2 id="Sqoop数据抽取和数据验证"><a href="#Sqoop数据抽取和数据验证" class="headerlink" title="Sqoop数据抽取和数据验证"></a>Sqoop数据抽取和数据验证</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<br><span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \<br>--connect jdbc:mysql://192.168.88.163:3306/insurance \<br>--username root \<br>--password 123456 \<br>--table dd_table \<br>--hive-table insurance_ods.dd_table \<br>--hive-import \<br>--hive-overwrite \<br>--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \<br>--delete-target-dir \<br>-m 1<br><br><span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span><br>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \<br>--connect jdbc:mysql://192.168.88.163:3306/insurance \<br>--username root \<br>--password 123456 \<br>--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span><br>`<br>mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<br><span class="hljs-comment">#2、查询hive的表dd_table的条数</span><br>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<br><br><span class="hljs-comment">#3、比较2边的数字是否一样。</span><br><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><br><span class="hljs-keyword">else</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="Flink-on-Yarn"><a href="#Flink-on-Yarn" class="headerlink" title="Flink on Yarn"></a>Flink on Yarn</h3><ul><li>Session 模式</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 先创建 Session 会话， d 表示后台运行，s 表示每个 jm 的 slot 个数</span><br>flink/bin/yarn-session.sh -d -jm 1024 -tm 1024 -s 2<br><br><span class="hljs-comment"># 提交任务</span><br>flink/bin/flink run /<span class="hljs-built_in">export</span>/server/flink/examples/batch/WordCount.jar \<br>--input hdfs://node1.test.cn:8020/wordcount/input<br></code></pre></td></tr></table></figure><ul><li>Job 分离模式</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 直接提交任务，m 表示 jm 的地址,环境变量需要提前配置</span><br>/<span class="hljs-built_in">export</span>/server/flink/bin/flink run \<br>-m yarn-cluster -yjm 1024 -ytm 1024 \<br>/<span class="hljs-built_in">export</span>/server/flink/examples/batch/WordCount.jar \<br>--input hdfs://node1.test.cn:8020/wordcount/input<br></code></pre></td></tr></table></figure><h2 id="其它命令"><a href="#其它命令" class="headerlink" title="其它命令"></a>其它命令</h2><h3 id="markdown代码折叠"><a href="#markdown代码折叠" class="headerlink" title="markdown代码折叠"></a>markdown代码折叠</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span></span><br><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span></span>点击查看完整代码<span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span></span><br><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span></span><br><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span></span><br><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></span><br></code></pre></td></tr></table></figure><h3 id="免秘钥登录"><a href="#免秘钥登录" class="headerlink" title="免秘钥登录"></a>免秘钥登录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t rsa<br>ssh-copy-id node1<br>scp /root/.ssh/authorized_keys node2:/root/.ssh<br>scp /root/.ssh/authorized_keys node3:/root/.ssh<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>存档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>Spark</tag>
      
      <tag>Sqoop</tag>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="/2020/04/27/first_post/"/>
    <url>/2020/04/27/first_post/</url>
    
    <content type="html"><![CDATA[<p>努力写博客, 总结经验教训, 学习永远在路上<br>感觉 GitHub Page 真的太方便了，随时随地可以开始写<br>打算把常用的资料文档命令放到博客上，debug 记录也放上来，还有学习笔记与项目总结</p>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
