<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>W06 | 工作中使用MySQL遇到的几个小坑</title>
    <link href="/2023/01/15/W06-%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BD%BF%E7%94%A8MySQL%E9%81%87%E5%88%B0%E7%9A%84%E5%87%A0%E4%B8%AA%E5%B0%8F%E5%9D%91/"/>
    <url>/2023/01/15/W06-%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BD%BF%E7%94%A8MySQL%E9%81%87%E5%88%B0%E7%9A%84%E5%87%A0%E4%B8%AA%E5%B0%8F%E5%9D%91/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/15/FgLoEhyf589UCZN.jpg"></p><p>MySQL 是工作当中经常使用到一个开源数据库，我当前工作主要使用 MySQL 作为报表存储数据库，以及承接数据提供给到下游业务使用。使用过程中遇到很多很多坑，都是小问题但是碰到了处理起来也是比较繁琐，特别记录一下。</p><h3 id="分区问题"><a href="#分区问题" class="headerlink" title="分区问题"></a>分区问题</h3><p>MySQL 数据库使用 InnoDB 引擎的时候是支持分区的，MySQL数据库的分区是局部分区索引，一个分区中既存了数据跟索引。聚集索引和非聚集索引都存放在分区当中。MySQL 分区分为 RANGE分区，LIST分区，HASH分区，KEY分区，生产环境一般使用RANGE分区，这个分区通常会跟 hive 数仓里面的分区字段一致，分区字段相当于索引，通常是 YYMMDD 这样的日期数据，</p><ul><li>如果表设置了索引，那么分区字段必须是索引</li><li>RANGE 分区日期字段，分区只能从小到大创建。这个在批量导入分区数据的时候需要特别特别注意，先创建分区再插入数据，其次是分区需要从小到大创建，我被这个坑了许多次。</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 创建分区表，假设分区字段是日期,to_days() 函数作用是返回参数日期跟年份 0 之间差的天数</span><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> table_name (user_id      <span class="hljs-type">INT</span>, nick_name    <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">50</span>), dayno        <span class="hljs-type">DATE</span>    ) ENGINE<span class="hljs-operator">=</span>InnoDB <span class="hljs-keyword">DEFAULT</span> CHARSET<span class="hljs-operator">=</span>utf8mb4 <span class="hljs-keyword">PARTITION</span> <span class="hljs-keyword">BY</span> <span class="hljs-keyword">RANGE</span>(to_days(dayno)) ( <span class="hljs-keyword">PARTITION</span> p20230111 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738897</span>), <span class="hljs-keyword">PARTITION</span> p20230112 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738898</span>), <span class="hljs-keyword">PARTITION</span> p20230113 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738899</span>), <span class="hljs-keyword">PARTITION</span> p20230114 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738900</span>), <span class="hljs-keyword">PARTITION</span> p20230115 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738901</span>) );<span class="hljs-comment">-- 新增分区</span><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> table_name <span class="hljs-keyword">add</span> <span class="hljs-keyword">PARTITION</span>( <span class="hljs-keyword">PARTITION</span> p20230116 <span class="hljs-keyword">VALUES</span> LESS THAN (<span class="hljs-number">738902</span>) ENGINE <span class="hljs-operator">=</span> InnoDB,<span class="hljs-comment">-- 删除分区</span> <span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> table_name <span class="hljs-keyword">drop</span> <span class="hljs-keyword">PARTITION</span> p20230116 ;<span class="hljs-comment">-- 查询当前表分区</span><span class="hljs-keyword">show</span> <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span>_name;<span class="hljs-comment">-- 或者通过查询元数据</span><span class="hljs-keyword">select</span> partition_name,table_rows<span class="hljs-keyword">from</span> information_schema.partitions<span class="hljs-keyword">where</span> table_name <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;table_name&#x27;</span>;</code></pre></div><h3 id="大小写敏感问题"><a href="#大小写敏感问题" class="headerlink" title="大小写敏感问题"></a>大小写敏感问题</h3><p>MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写。MySQL 大小写敏感配置相关的两个参数，lower_case_file_system 和 lower_case_table_names。</p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 查询当前 MySQL 大小写敏感情况</span><span class="hljs-keyword">show</span> <span class="hljs-keyword">global</span> variables <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%lower_case%&#x27;</span>;<span class="hljs-operator">+</span><span class="hljs-comment">------------------------+-------+</span><span class="hljs-operator">|</span> Variable_name          <span class="hljs-operator">|</span> <span class="hljs-keyword">Value</span> <span class="hljs-operator">|</span><span class="hljs-operator">+</span><span class="hljs-comment">------------------------+-------+</span><span class="hljs-operator">|</span> lower_case_file_system <span class="hljs-operator">|</span> <span class="hljs-keyword">ON</span>    <span class="hljs-operator">|</span><span class="hljs-operator">|</span> lower_case_table_names <span class="hljs-operator">|</span> <span class="hljs-number">2</span>     <span class="hljs-operator">|</span><span class="hljs-operator">+</span><span class="hljs-comment">------------------------+-------+</span><span class="hljs-number">2</span> <span class="hljs-keyword">rows</span> <span class="hljs-keyword">in</span> <span class="hljs-keyword">set</span> (<span class="hljs-number">0.02</span> sec)<span class="hljs-comment">-- 如何修改</span>修改 MySQL 配置文件 my.cnf 当中的参数即可。</code></pre></div><ul><li>lower_case_file_system</li></ul><p>代表当前系统文件是否大小写敏感，只读参数，无法修改。ON 大小写不敏感，OFF 大小写敏感。</p><ul><li>lower_case_table_names，</li></ul><p>代表表名是否大小写敏感，可以修改，参数有0、1、2三种。</p><ul><li><ul><li>0 大小写敏感。</li><li>1 大小写不敏感。</li><li>2 大小写不敏感。跟 1 参数的区别是创建的库表名称保持原样保存在磁盘上但是执行 SQL 语句的时候会自动转换成小写。 1 参数创建的库表名称会转换成小写保存在磁盘上</li></ul></li></ul><h3 id="字符集问题"><a href="#字符集问题" class="headerlink" title="字符集问题"></a>字符集问题</h3><p>通常我们创建 MySQL 表的时候都会指定字符集为 utf8，这个通用于各种工具产品，并且对中文兼容性好。但是但是，但是这个字符集不支持 emoji 符号，我在处理爬虫跟日志数据的时候，就被坑了。字符集设置为 utf8mb4，能支持更多格式并且兼容 emoji 符号存储。</p><ul><li>MySQL 中字符集相关变量，设置直接 set 即可</li></ul><div class="code-wrapper"><pre><code class="hljs sql">character_set_client：客户端请求数据的字符集character_set_connection：从客户端接收到数据，然后传输的字符集character_set_database：默认数据库的字符集，character_set_results：结果集的字符集character_set_server：数据库服务器的默认字符集character_set_system：存储元数据的字符集,默认 utf8，不需要设置</code></pre></div><h3 id="数据类型问题"><a href="#数据类型问题" class="headerlink" title="数据类型问题"></a>数据类型问题</h3><p>因为不同的数据库会有不同的数据类型，从 hive 数仓导入到 MySQL 数据库，尤其需要注意设置匹配的字段类型，一方面是基于兼容性考虑保证数据正常导入且没有改变，另一方适合的字段类型也能节约成本。</p><ul><li>小数类型，财务精准数据，建议使用 DECIMAL 类型，指定步长避免精度损失。更建议的做法是在 hive 数仓上游就处理好步长。</li><li>字符串类型，建议使用可变字符串类型 VARCHAR 指定字符串长度，任何情况下都不建议使用其它的字符串类型。</li><li>日期类型，不包含时间使用 DATE ，包含时间使用 DATETIME，MySQL 库不建议将时间数据存储为字符串，会影响查询性能。</li><li>数值类型，注意不同的 INT 类型在只取整数无符号的情况能存储的最大数值范围，这个我同事经常被坑。</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 各种 INT 类型的取值范围，在只取整数的情况下</span>TINYINT: (<span class="hljs-number">0</span>，<span class="hljs-number">255</span>)<span class="hljs-type">INT</span>: (<span class="hljs-number">0</span>，<span class="hljs-number">4294967295</span>)<span class="hljs-type">BIGINT</span>: (<span class="hljs-number">0</span>，<span class="hljs-number">18446744073709551615</span>)</code></pre></div><h3 id="存储过程跟函数"><a href="#存储过程跟函数" class="headerlink" title="存储过程跟函数"></a>存储过程跟函数</h3><p>MySQL 支持自定义函数跟存储过程，通常来说存储过程会比函数功能更强大，比如有修改全局数据库状态的权限，可以返回参数，能做为独立程序执行，而函数通常作为查询语句的一部分来使用，有严格的限制。日常工作中我使用并没有经常使用到自定义函数，但是经常使用存储过程，主要用于检测 MySQL 分区状态，创建、清空、删除分区等操作，因为自定义程度很高，基本能和脚本语言编写的判断程序达到一样的效果。</p><ul><li>创建一个存储过程的格式</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span>    [DEFINER <span class="hljs-operator">=</span> &#123; <span class="hljs-keyword">user</span> <span class="hljs-operator">|</span> <span class="hljs-built_in">CURRENT_USER</span> &#125;]　<span class="hljs-keyword">PROCEDURE</span> sp_name ([proc_parameter[,...]])    [characteristic ...] routine_body proc_parameter:    [ <span class="hljs-keyword">IN</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">OUT</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">INOUT</span> ] param_name type characteristic:    COMMENT <span class="hljs-string">&#x27;string&#x27;</span>  <span class="hljs-operator">|</span> <span class="hljs-keyword">LANGUAGE</span> <span class="hljs-keyword">SQL</span>  <span class="hljs-operator">|</span> [<span class="hljs-keyword">NOT</span>] <span class="hljs-keyword">DETERMINISTIC</span>  <span class="hljs-operator">|</span> &#123; <span class="hljs-keyword">CONTAINS</span> <span class="hljs-keyword">SQL</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">NO</span> <span class="hljs-keyword">SQL</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">READS</span> <span class="hljs-keyword">SQL</span> DATA <span class="hljs-operator">|</span> <span class="hljs-keyword">MODIFIES</span> <span class="hljs-keyword">SQL</span> DATA &#125;  <span class="hljs-operator">|</span> <span class="hljs-keyword">SQL</span> SECURITY &#123; DEFINER <span class="hljs-operator">|</span> INVOKER &#125; routine_body:　　Valid <span class="hljs-keyword">SQL</span> routine statement [begin_label:] <span class="hljs-keyword">BEGIN</span>　　[statement_list]　　　　……<span class="hljs-keyword">END</span> [end_label]</code></pre></div><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.cnblogs.com/GrimMjx/p/10526821.html">https://www.cnblogs.com/GrimMjx/p/10526821.html</a></p><p><a href="https://www.jianshu.com/p/f2eabcef6577">https://www.jianshu.com/p/f2eabcef6577</a></p><p><a href="https://blog.csdn.net/qq_43563999/article/details/105820952">https://blog.csdn.net/qq_43563999/article/details/105820952</a></p><p><a href="https://www.runoob.com/w3cnote/mysql-stored-procedure.html">https://www.runoob.com/w3cnote/mysql-stored-procedure.html</a></p>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>W05 | 坚持运动一年让我的生活充满力量</title>
    <link href="/2023/01/08/W05-%E5%9D%9A%E6%8C%81%E8%BF%90%E5%8A%A8%E4%B8%80%E5%B9%B4%E8%AE%A9%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB%E5%85%85%E6%BB%A1%E5%8A%9B%E9%87%8F/"/>
    <url>/2023/01/08/W05-%E5%9D%9A%E6%8C%81%E8%BF%90%E5%8A%A8%E4%B8%80%E5%B9%B4%E8%AE%A9%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB%E5%85%85%E6%BB%A1%E5%8A%9B%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/08/A67QsXtmySWvi9c.jpg"></p><p>本周超级猩猩出了 2022 年度运动报告，全年除了有一两个月因为疫情跟阳了门店停业，每周都坚持去超级猩猩上团课。全年锻炼 126 天，完成 161 次训练，上了 18 节早课，33 节晚课，上了 89 节 BC ，28 节 BJ 跟 23 节 RPM，全年上课时长 9410 分钟。感谢坚持努力的自己，过去一年平均 3 天就去上一次团课，我还是坚持了下来，爱上了运动，运动也改变了我，让我的生活充满力量。</p><img src="https://s2.loli.net/2023/01/08/g9jTq6cHrNSytAp.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/XLTWCKE9yPt2p1s.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/Oc5utQmihgfN4wM.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/6XlGrjChukMLHvZ.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/2Y3Mhkf7U8JP1nc.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/9Ze6LO7QgMlqY2i.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/BOfUXTZSjduJqH7.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/oJqiXVEF7N9euWH.png" alt="" style="zoom:50%;" /><img src="https://s2.loli.net/2023/01/08/mlvw6W4rBD8AG7d.jpg" alt="" style="zoom: 67%;" /><h3 id="坚持运动让我的生活充满力量"><a href="#坚持运动让我的生活充满力量" class="headerlink" title="坚持运动让我的生活充满力量"></a>坚持运动让我的生活充满力量</h3><h4 id="体能增强，身体协调性增强"><a href="#体能增强，身体协调性增强" class="headerlink" title="体能增强，身体协调性增强"></a>体能增强，身体协调性增强</h4><p>坚持运动这一年，我的体能明显增加，刚刚开始 BC 我第 5 小节之后基本干不动，都在摸鱼，现在轻轻松松可以 hold 住全程，燃脂保持在 600 千卡以上。身体协调性也更强了，跳踢、侧踢、波比跳都不在话下，撩起来腿就能踢出去，核心很稳。体能协调性提升的好处非常明显，免疫力增强，日常生活更有动力跟力量，这次阳了之后，我基本没有啥大的症状，3 天就恢复状态。体能增强之后还有一个好处：跟别人讨论沟通更有底气了，因为如果言语上说服不了，我可以尝试物理说服，没在怕的。</p><img src="https://s2.loli.net/2023/01/08/pz3yH5BLE2dcnDU.jpg" alt="7 天转阴记录" style="zoom:50%;" /><h4 id="更自信，更热情开朗"><a href="#更自信，更热情开朗" class="headerlink" title="更自信，更热情开朗"></a>更自信，更热情开朗</h4><p>身边朋友都说我开始运动之后，整个人精神面貌焕然一新。运动锻炼让我能够发泄情绪，把平时在工作跟生活中积攒的负能量发泄出来，多余的精力释放掉，让我能更好的专注于当前的生活。运动也让我的体态更优美，特别开始上 BJ 舞蹈课之后，身体协调性跟节奏变好了，能跟上各种斯比特。因为精神状态跟身体体态的变化，让我对自己的外貌更有自信，接人待物也更开朗，对于生活充满了热情。同时运动也改变了我穿搭的风格，霍比特人身高的我，平时都是有啥穿啥，开始锻炼之后开始走运动风格，白色运动 T 恤 + 运动短裤 + 运动训练鞋，百搭街头风，也特别能凸显个人积极向上的精神面貌。</p><img src="https://s2.loli.net/2023/01/08/DKecnCdmoGjgpSU.jpg" alt="我现在的穿搭" style="zoom: 50%;" /><h4 id="进入新圈子，认识新伙伴"><a href="#进入新圈子，认识新伙伴" class="headerlink" title="进入新圈子，认识新伙伴"></a>进入新圈子，认识新伙伴</h4><p>运动作为一种爱好，让我进入了一个全新的圈子。我喜欢超级猩猩原因之一就是团课不需要社交，上完即走无需交流，但是我还是认识了一群一起上课的 “好友”。我们一起上课、操房里眼神上鼓励交流、BC 的时候 PK 波比跳，但我确实不晓得他们的名字，也不晓得他们从哪里的，做什么工作的，我们仅仅碰巧经常一起上同一节课而已。我特别喜欢这样的社交方式，互相眼熟，有共同的爱好，也能互相鼓励，但是没有绑定社交关系，没有加微信留电话，弱性的社交关系让我非常放松没有社交压力，同时也能知道我们是一起坚持运动锻炼的 “ 好友 ”。</p><img src="https://s2.loli.net/2023/01/08/vXcT4EHZ1M2DKVI.png" alt="过去一年跟 1306 个猩友相遇" style="zoom:50%;" /><h3 id="新-1-年新的运动-flag"><a href="#新-1-年新的运动-flag" class="headerlink" title="新 1 年新的运动 flag"></a>新 1 年新的运动 flag</h3><p>2022 年运动收获满满，但是小遗憾是 BC 没能完成 100 节成就。新一年要继续保持运动锻炼，先立下新一年的运动 flag。运动锻炼是一生的事情，让运动成为日常生活的一部分。</p><ul><li><strong>完成 BC 100 节跟 BJ 100 节</strong></li><li><strong>体验更多类型课程，燃脂、塑性、HIIT、舞蹈、瑜伽</strong></li><li><strong>早课上满 30 节</strong> </li></ul>]]></content>
    
    
    <categories>
      
      <category>普通生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>W04 | Hive / Spark 如何避免单节点全局排序？</title>
    <link href="/2023/01/02/W04-Spark%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%8D%95%E8%8A%82%E7%82%B9%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F/"/>
    <url>/2023/01/02/W04-Spark%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%8D%95%E8%8A%82%E7%82%B9%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/02/D6hZklc1OntfFdJ.jpg"></p><p>最近因为经常对接模型算法，营销模型的一个应用场景是：按照模型打分取 TOPN 用户进行营销投放，由此就会产生一个全局排序的场景：<strong>在用户量过亿的情况下，单点全局排序极其容易出现 OOM。</strong>经历了几次线上事故之后，决心要彻底解决这个问题，跟同事请教了下，可以通过 <strong>“加盐打散”</strong> 来解决这个问题。</p><h3 id="加盐打散"><a href="#加盐打散" class="headerlink" title="加盐打散"></a>加盐打散</h3><p>产生全局排序的原因就是因为所有的 key 都需要互相比较才能产生全局排序序号，加盐打散的思路就是：<strong>通过对 key 加盐之后，拆分成不同的分组，用分组排序代替全局排序，实现分布式全局排序。</strong>就拿我日常接触最多的用户模型打分来说，基本思路如下：</p><div class="code-wrapper"><pre><code class="hljs plain">1、对用户加盐进行分组，并且获取不同组的先后顺序跟用户在组内的排序2、获取每个组的全局初始排序3、每个用户的全局排序 = 分组初始排序 + 在组内的排序</code></pre></div><h3 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h3><p>假设有表 model_score ，字段有 user_id 跟 score ，里面有 10 亿条数据，结构如下，要对表里所有的 user_id 按照 score 大小排序。</p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> `model_score`(   `user_id` <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">100</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,   `score` <span class="hljs-keyword">DOUBLE</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>)ENGINE<span class="hljs-operator">=</span>InnoDB <span class="hljs-keyword">DEFAULT</span> CHARSET<span class="hljs-operator">=</span>utf8;</code></pre></div><ul><li>对用户加盐进行分组，获取用户在组内的排序</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">with</span> t1 <span class="hljs-keyword">as</span> (<span class="hljs-keyword">select</span>   user_id  ,score  ,new_score  <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> new_score <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) <span class="hljs-keyword">as</span> rank1 <span class="hljs-comment">-- 获取用户分组内排序</span>  <span class="hljs-keyword">from</span> (  <span class="hljs-keyword">select</span> user_id,score,<span class="hljs-built_in">ceil</span>(score<span class="hljs-operator">*</span><span class="hljs-number">10000</span>) <span class="hljs-keyword">as</span> new_score  <span class="hljs-keyword">from</span> model_score  ) tmp),</code></pre></div><ul><li>获取每个分组全局初始排序</li></ul><div class="code-wrapper"><pre><code class="hljs sql">t2 <span class="hljs-keyword">as</span> (<span class="hljs-keyword">select</span> new_score,<span class="hljs-built_in">sum</span>(cnt) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> new_score <span class="hljs-keyword">desc</span> <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> unbounded preceding <span class="hljs-keyword">and</span> <span class="hljs-number">1</span> preceding ) <span class="hljs-keyword">as</span> rank2   <span class="hljs-comment">-- 从负无穷行到当前 -1 行的求和，获取到该分组用户最初的全局排序</span><span class="hljs-keyword">from</span> ( <span class="hljs-keyword">select</span> new_score,<span class="hljs-built_in">count</span>(<span class="hljs-number">1</span>) <span class="hljs-keyword">as</span> cnt<span class="hljs-keyword">from</span> t1 <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> new_score) tmp   ),</code></pre></div><ul><li>合并获取到用户的全局排序</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> t1.user_id,t1.score ,(t1.rank1 <span class="hljs-operator">+</span> t2.rank2) <span class="hljs-keyword">as</span> rank   <span class="hljs-comment">-- 每个用户的全局排序 = 分组初始排序 + 在组内的排序 </span><span class="hljs-keyword">from</span> t1 <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t2 <span class="hljs-keyword">on</span> t1.new_score <span class="hljs-operator">=</span> t2.new_score</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
      <tag>Spark</tag>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>W03 | 数据开发当中如何验证数据结果准确性</title>
    <link href="/2022/12/25/W03-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%BD%93%E4%B8%AD%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C%E5%87%86%E7%A1%AE%E6%80%A7/"/>
    <url>/2022/12/25/W03-%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%BD%93%E4%B8%AD%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C%E5%87%86%E7%A1%AE%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/12/25/O7ZVahpqkvtN2Dw.jpg"></p><h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>数据开发日常工作经常需要跟业务方核验数据，校验数据源、业务逻辑是否准确。这里的数据准确性跟 ETL 中的“精确一次性语义” 保证数据不丢失不重复不一样，说的是数据报表或者用户标签特征是否符合既定业务逻辑。</p><p>以我浅薄的经验来说，验证数据准确性主要从：明细数据逻辑验证、业务逻辑验证、白盒测试这 3 个角度去做。</p><h3 id="通过明细数据从逻辑上做数据验证"><a href="#通过明细数据从逻辑上做数据验证" class="headerlink" title="通过明细数据从逻辑上做数据验证"></a>通过明细数据从逻辑上做数据验证</h3><p>通常一个数据报表逻辑都会相对复杂，多个表关联是一定会有的，因为各种业务逻辑的原因，指不定还有非常多的 case when 跟字段二次处理，逻辑上也会一层套一层，使用的明细层（DWS）最后结果是报表层（RPT）。自己开发的报表验证是否准确第一个想到的从逻辑明细上去做验证。</p><p>这个意思是，拿几条明细数据验证，看看这些用户是否按照既定的业务逻辑进入对应报表统计分类里面，特别需要验证代表性的用户，验证我们的 case when、二次处理是否生效等。</p><p>使用明细数据验证统计逻辑是否准确是我日常工作当中最常用也是最高效的一种方式。</p><h3 id="通过业务逻辑做数据验证"><a href="#通过业务逻辑做数据验证" class="headerlink" title="通过业务逻辑做数据验证"></a>通过业务逻辑做数据验证</h3><p>本身报表就是根据业务需求创建的，那么是否准确也应该由业务方来验证，我们自行验证的话就是通过判断报表指标是否符合基本的业务逻辑。这里说的业务逻辑主要只的报表指标数值是否符合业务方预计范围之内，是否符合普遍的规律跟比例。</p><p>比如说，营销投放当中触达率不可能超过 100 %、获客成本应该在一个大致范围之内，应该跟投放平台反馈的成本接近，如果我们报表计算结果指标跟这些数值相差较大，那么就应该从头开始验证，可能是哪一个环节计算错误，或者存在脏数据。</p><h3 id="通过白盒测试验证数据"><a href="#通过白盒测试验证数据" class="headerlink" title="通过白盒测试验证数据"></a>通过白盒测试验证数据</h3><p>白盒测试是指边看着程序代码逻辑，一遍操作系统查看数据变化情况。在我们数据开发角度，其实就是亲自验证下用户是否按照我们既定的逻辑被统计到对应的指标当中。</p><p>拿个埋点数据举例，就是看看拿实机测试，看看上报数据是否正常，对应操作是否上报了正确的埋点标识并且正常被报表通缉到对应指标当中。</p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>周更挑战</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>W02 | 推荐下今天发现的几个cheatsheet</title>
    <link href="/2022/12/18/W02_%E6%8E%A8%E8%8D%90%E4%B8%8B%E4%BB%8A%E5%A4%A9%E5%8F%91%E7%8E%B0%E7%9A%84%E5%87%A0%E4%B8%AAcheatsheet/"/>
    <url>/2022/12/18/W02_%E6%8E%A8%E8%8D%90%E4%B8%8B%E4%BB%8A%E5%A4%A9%E5%8F%91%E7%8E%B0%E7%9A%84%E5%87%A0%E4%B8%AAcheatsheet/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/12/18/h13sx5OYpqvIDFb.png" alt="vim cheat sheet"></p><p>今天发现了一个神奇的东西，名字叫 Cheat Sheet，就是各种语言工具的快捷键列表，这个对于我这样记不住各种东西的菜鸟帮助太大了，平时边用边记。</p><p>老年跟菜鸟的区别可能就是你对各种工具快捷键的熟悉程度。记录下常用的几个，纳入自己工作流当中。</p><ul><li><strong>Python 语言（有中文且也有其他工具语言的）</strong></li></ul><p><a href="https://cheatography.com/simpleapples/cheat-sheets/python-3/">https://cheatography.com/simpleapples/cheat-sheets/python-3/</a></p><ul><li><strong>VIM （也有其他语言工具）</strong></li></ul><p><a href="https://quickref.me/vim">https://quickref.me/vim</a></p><ul><li><strong>另一个 VIM</strong> </li></ul><p><a href="https://vim.rtorr.com/lang/zh_cn">https://vim.rtorr.com/lang/zh_cn</a></p><ul><li><strong>正则表达</strong></li></ul><p><a href="https://ihateregex.io/">https://ihateregex.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Linux</tag>
      
      <tag>周更挑战</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>W01 | 什么是新怪谈—从《鼠王》开始说</title>
    <link href="/2022/12/11/W01_%E4%BB%80%E4%B9%88%E6%98%AF%E6%96%B0%E6%80%AA%E8%B0%88%E2%80%94%E4%BB%8E%E3%80%8A%E9%BC%A0%E7%8E%8B%E3%80%8B%E5%BC%80%E5%A7%8B%E8%AF%B4/"/>
    <url>/2022/12/11/W01_%E4%BB%80%E4%B9%88%E6%98%AF%E6%96%B0%E6%80%AA%E8%B0%88%E2%80%94%E4%BB%8E%E3%80%8A%E9%BC%A0%E7%8E%8B%E3%80%8B%E5%BC%80%E5%A7%8B%E8%AF%B4/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/12/17/Nsp3iElyHnRMoID.jpg" alt="鼠王封面"></p><p>频繁在机核的播客里被安利《鼠王》，忘记哪一期节目，介绍了《美国众神》的作者尼尔盖曼作品，就提到通俗奇幻小说的发展史。目前的这类奇幻类小说主要分三类：旧时代民间传说、克苏鲁世界、新怪谈，当然还有史蒂芬金自己独特的体系，不在讨论范围之内。当时就很好奇，为啥新怪谈叫新怪谈，新在哪里。最近机核上了鼠王有声书版本，听音频书容易分神，趁着热度火速去读了鼠王。 </p><h4 id="“怪”是理所当然"><a href="#“怪”是理所当然" class="headerlink" title="“怪”是理所当然"></a><strong>“怪”是理所当然</strong></h4><p>感觉新怪谈第一个特点就是“怪谈”不怪，或者说故事里的人物不会感觉奇怪，理所当然的接受了各种“超自然事物”。就鼠王来说，鼠王登场的一系列操作，惊艳冷酷，但邵尔立刻就接受了这个事实，其它的登场人物同样如此。不会去思考鼠王如何出现，也不会想着上报国家，上报科学研究，大家似乎全都接受了鼠王出现的事实，而且不在于他为何出现，又有何价值，对人类发展有什么影响，只当成一个新事物看待。 </p><h4 id="融入世界"><a href="#融入世界" class="headerlink" title="融入世界"></a><strong>融入世界</strong></h4><p>第二个特殊之处在于，鼠王鸟王蛛王都生活在都市之中，融入世界。他们是都市的部分，他们不是躲在深山老林，他们和都市中其它物种一样，有自己的住宿，有自己的生活，而且是随着时代一起发展的，他们完美融入了“正常人”的世界之中。老鼠鸟蜘蛛都和人类一起生活着。 </p><h4 id="故事不一定是主角"><a href="#故事不一定是主角" class="headerlink" title="故事不一定是主角"></a><strong>故事不一定是主角</strong></h4><p>第三个是我感觉，新怪谈故事情节不一定是第一位的。比说并没有介绍鼠王等人从何而来，也没有说到最后结局会如何，更没有说舞会现场如何收场。很多正常小说应该去解释和说明的地方鼠王都没有解释。其次是承载的文化元素。鼠王这本书融入的伦敦底下音乐文化我是一无所知的，只能依靠文字描写和歌曲名称去想象那是一种什么的的音乐，鼠王融入的是这么一种元素，其它的新怪谈小说比如《遗落南境》又是容易了SCP基金会的元素？尼尔盖曼的《美国众神》又是另一种文化。可以说新怪谈小说都会惨杂着其它元素的内容。从整个作品来说，故事情节不一定是最重要的，故事也不一定完整，但是从故事中渗透出来的文化符号以及表达的想法欲望一定是第一位的。 </p><p>最近的阅读口味很多都是被机核种草的。读完鼠王特别想想玩《极乐迪斯科》，这是一种新的文化体系，曾经被种草克苏鲁，现在又是新怪谈，感谢机核。</p><p><em>（本文首发于豆瓣：</em><a href="https://book.douban.com/review/13384309/"><em>什么是新怪谈—从《鼠王》开始说</em></a><em>）</em></p>]]></content>
    
    
    <categories>
      
      <category>阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>周更挑战</tag>
      
      <tag>读书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高效年轻人的7个办公习惯</title>
    <link href="/2022/04/10/%E9%AB%98%E6%95%88%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%847%E4%B8%AA%E5%8A%9E%E5%85%AC%E4%B9%A0%E6%83%AF/"/>
    <url>/2022/04/10/%E9%AB%98%E6%95%88%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%847%E4%B8%AA%E5%8A%9E%E5%85%AC%E4%B9%A0%E6%83%AF/</url>
    
    <content type="html"><![CDATA[<p>偶然在即刻上看到飞书的 ZARA 分享了这篇小文章，高效年轻人的 7 个办公习惯，都是我目前所欠缺和需要学习的，至少我现在<br>‘elevator list’ 就没有做到，经常是口头表达，而不是先准备好问题和资料。</p><ol><li>如果 leader 布置了一个任务，做到30%的时候先问老板方向对不对，不要憋大招。定期主动跟需求方同步进展，不要等到被问/被催。周报上写“进展”而不是“动作”。“跟xx开会讨论xx事项”不算进展，讨论出来的结果才算进展。</li><li>意识到 leader 的时问非常宝贵，准备 “elevator list”，这是一份文档/笔记，在工作中如果有想跟leader请教/商量/同步的事情，随时记录进去。最终效果是，如果你突然在电梯遇到 leader,或者你发现跟老板在同一辆出租车里去见客户，就可以马上拿出这个 list，利用这段时间去探讨你日常记录的这些问题。珍惜一切跟leader独处的时间，把它变成学习请教的机会。</li><li>主动跟 leader 约定期的 1:1，并每次提前准备要讨论的话题（建议准备文档）</li><li>如果被拉到一个会上，主动记录会议纪要和todo， 并且会后发到会议群里</li><li>珍惜自己的时间，永远去想怎么提升效率。不要觉得因为自己年轻，自己的时间就是便宜的。</li><li>不要闷头干活，不要单打独斗，主动跟跨部门同事约饭、约咖啡，不要社恐，学会协作。</li><li>如果 leader 布置了一个任务，你不理解这件事情的意义，那就主动问“为什么”。如果leader说“别问问什么，做就是了”，建议尽快换工作。</li></ol>]]></content>
    
    
    <categories>
      
      <category>工作记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>项目管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQLBoy日常工作技巧</title>
    <link href="/2022/02/09/SQLBoy%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    <url>/2022/02/09/SQLBoy%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
    
    <content type="html"><![CDATA[<p>入职新工作三周了，虽然还处理 SQLBoy 阶段，但是学习到了非常多小技巧，有必要记录一下，持续更新，避免遗忘。</p><ul><li>规范需求记录，脚本备份，文档归类，代码片段，数字字典</li><li>封装公共参数和大数据脚本执行参数到脚本当中，执行脚本只需要引入变量</li><li>每一种 SQL 脚本方式封装一个方法，固定脚本执行格式</li><li>SQL 中空值和 null 同时过滤：字段 &gt; ‘’</li><li>历史数据回溯，通过封装脚本传参调用执行脚本进行</li><li>任务统一规范命令，数据丢失任务失败方便排除定位</li><li>等值判断注意非空校验和异常数据处理</li><li>等值判断先确定数值格式匹配，大小匹配，格式兼容</li><li>需求闭环，全链路自动化分析</li><li>代码自查，逻辑明确，自我溯源</li><li>明确需求，避免重复遗忘丢失误判</li><li>先理清思路，确认细节，规划执行路径和测试路径，再具体操作</li><li>整理小知识，定期存档回顾</li><li>验证数据准确性，包括：字段类型、数量、topN、趋势等</li><li>跨部门沟通，主动提供上下文，把相关人当做小白，从最基础的理念给合作部门介绍</li></ul>]]></content>
    
    
    <categories>
      
      <category>工作记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>SQL</tag>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开发环境准备</title>
    <link href="/2022/01/20/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
    <url>/2022/01/20/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<p>最近换了 M1 MBP，新电脑开发环境需要从头部署，汇总一下我开发环境常用的工具吧。</p><h3 id="环境支持"><a href="#环境支持" class="headerlink" title="环境支持"></a>环境支持</h3><ul><li>资源下载</li></ul><p><a href="https://repo.huaweicloud.com/java/jdk/#/">下载 JDK 1.8</a></p><p><a href="https://www.scala-lang.org/download/2.11.12.html#/">下载 Scala 2.11.12</a></p><ul><li>安装说明</li></ul><p><a href="https://www.jianshu.com/p/717a5b92e01a#/">win10下jdk1.8安装和环境变量的配置</a></p><p><a href="https://www.cnblogs.com/mawangwang/p/12250018.html#/">scala安装教程及简单配置</a></p><h3 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h3><ul><li>IDEA</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 插件</span>Cosy Java CodingAtom Material IconsPDF ViewerRainbow BracketsScalaIDE Eval Reset</code></pre></div><ul><li>VScode</li><li>Git </li><li>Maven </li><li>Notepad++</li><li>Navicat</li><li>CRT</li><li>MySQL</li></ul><h3 id="办公工具"><a href="#办公工具" class="headerlink" title="办公工具"></a>办公工具</h3><ul><li>Typora</li><li>幕布</li><li>Xmind</li><li>Snipaste</li><li>uTools</li><li>WPS</li><li>Everything</li><li>火绒</li><li>任务栏透明</li><li>chrome 浏览器</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 插件</span>简悦油猴插件<span class="hljs-operator">-</span>CSDN广告屏蔽</code></pre></div><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p><a href="https://wallhaven.cc/#/">壁纸网站</a></p><p><a href="https://pan.baidu.com/s/1TMU388jytBRRx4y8XCNkAw">百度网盘</a></p><p>提取码: 2n0f </p><p><a href="https://developer.aliyun.com/article/681235#/">国内开源镜像站点</a></p><p><a href="https://developer.aliyun.com/mirror/maven#/">阿里 Maven 镜像</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL中的行转列和列转行</title>
    <link href="/2021/06/22/SQL%E4%B8%AD%E7%9A%84%E8%A1%8C%E8%BD%AC%E5%88%97%E5%92%8C%E5%88%97%E8%BD%AC%E8%A1%8C/"/>
    <url>/2021/06/22/SQL%E4%B8%AD%E7%9A%84%E8%A1%8C%E8%BD%AC%E5%88%97%E5%92%8C%E5%88%97%E8%BD%AC%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<h3 id="MySQL-的行转列"><a href="#MySQL-的行转列" class="headerlink" title="MySQL 的行转列"></a>MySQL 的行转列</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> <span class="hljs-operator">+</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-operator">+</span> max<span class="hljs-operator">/</span>sum 函数</code></pre></div><h3 id="MySQL-的列转行"><a href="#MySQL-的列转行" class="headerlink" title="MySQL 的列转行"></a>MySQL 的列转行</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> 指定语句 <span class="hljs-operator">+</span> <span class="hljs-keyword">union</span> 拼接即可<span class="hljs-keyword">union</span> 去重<span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span> 不去重FLink 中 <span class="hljs-keyword">union</span> 不去重，相当于 <span class="hljs-keyword">SQL</span>中的 <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span></code></pre></div><h3 id="Hive-行转列"><a href="#Hive-行转列" class="headerlink" title="Hive 行转列"></a>Hive 行转列</h3><div class="code-wrapper"><pre><code class="hljs sql"># 基本思路：列拼接输出# 涉及函数concat(str1,str2,...) # 字段或字符串拼接concat_ws(sep, str1,str2) # 以分隔符拼接每个字符串collect_set(col) #将某字段的值进行去重汇总，产生<span class="hljs-keyword">array</span>类型字段# 代码实现<span class="hljs-keyword">select</span> collect_set( concat_ws(<span class="hljs-string">&#x27;:&#x27;</span>,s_id,s_name,s_sex) ) <span class="hljs-keyword">from</span> student;</code></pre></div><h3 id="Hive-列转行"><a href="#Hive-列转行" class="headerlink" title="Hive 列转行"></a>Hive 列转行</h3><div class="code-wrapper"><pre><code class="hljs sql"># 涉及函数# explode函数# explode(col) 将hive一列中复杂的<span class="hljs-keyword">array</span>或者map结构拆分成多行# <span class="hljs-keyword">lateral</span> <span class="hljs-keyword">view</span> 侧视图# 用于和split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。# 代码实现<span class="hljs-keyword">select</span> deptno,name <span class="hljs-keyword">from</span> emp <span class="hljs-keyword">lateral</span> <span class="hljs-keyword">view</span> explode(names) tmp_tb <span class="hljs-keyword">as</span> name;</code></pre></div><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>mysql 行转列和列转行 </p><p><a href="https://www.jianshu.com/p/0e6113241979">https://www.jianshu.com/p/0e6113241979</a> </p><p>hive 行转列和列转行</p><p><a href="https://www.jianshu.com/p/26d85daef92c">https://www.jianshu.com/p/26d85daef92c</a> </p><p>Hive笔记之collect_list/collect_set（列转行）</p><p><a href="https://www.cnblogs.com/cc11001100/p/9043946.html">https://www.cnblogs.com/cc11001100/p/9043946.html</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Awk和Shell</title>
    <link href="/2021/06/10/Awk%E5%92%8CShell/"/>
    <url>/2021/06/10/Awk%E5%92%8CShell/</url>
    
    <content type="html"><![CDATA[<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><h3 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h3><ul><li><code>awk [选项参数] &#39;script&#39; var=value file(s)</code></li><li>基本语法<ul><li>$0 代表整个文本行</li><li>$1 代表文本行中的第 1 个数据字段</li><li>printf 打印输出</li><li>默认每行按空格或TAB分割，使用$n来获取段号</li></ul></li></ul><h3 id="段连接符OFS"><a href="#段连接符OFS" class="headerlink" title="段连接符OFS"></a>段连接符OFS</h3><ul><li><code>awk &#39;&#123;OFS=&quot;#&quot;&#125;&#123;print $1,$2,$3&#125;&#39; test_awk.txt</code></li></ul><h3 id="指定分隔符-F"><a href="#指定分隔符-F" class="headerlink" title="指定分隔符 -F"></a>指定分隔符 -F</h3><ul><li><code>awk -F &quot;:&quot; &#39;&#123;print $1&#125;&#39; test_awk2.txt</code></li></ul><h3 id="内容匹配"><a href="#内容匹配" class="headerlink" title="内容匹配"></a>内容匹配</h3><ul><li><code>格式&#39;/这里写具体的正则表达式/&#39;</code></li><li>正则规则<ul><li>1、^linux 以linux开头的行</li><li>2、$php 以php结尾的行</li><li>3、. 匹配任意单字符</li><li>4、.+ 匹配任意多个字符</li><li>5、 .* 匹配0个或多个字符(可有可无)</li><li>6、 [0-9a-z] 匹配中括号内任意一个字符</li><li>7、 (linux)+ 出现多次Linux单词</li><li>8、 (web){2} web出现两次以上</li><li>9、\ 屏蔽转义</li></ul></li><li>匹配到aaa或者ddd,就打印全部内容<ul><li><code>awk -F &#39;:&#39; &#39;/aaa|ddd/ &#123;print $0&#125;&#39; test_awk2.txt</code></li></ul></li></ul><h3 id="段内容判断"><a href="#段内容判断" class="headerlink" title="段内容判断"></a>段内容判断</h3><ul><li>支持赋值,条件表达式,关系运算符等</li></ul><h3 id="段之间比较"><a href="#段之间比较" class="headerlink" title="段之间比较"></a>段之间比较</h3><ul><li><code>awk -F &#39;:&#39; &#39;$3&lt;$4 &#123;print $0&#125;&#39; test_awk2.txt</code></li></ul><h3 id="NR行号和NF段数"><a href="#NR行号和NF段数" class="headerlink" title="NR行号和NF段数"></a>NR行号和NF段数</h3><ul><li>概念<ul><li>NF段数</li><li>NR行号从1开始</li><li>nl命令在linux系统中用来计算文件中行号</li><li><code>nl test_awk2.txt | head -2</code></li></ul></li><li>从test_awk2.txt前3行，把第1段内容替换为test，指定分隔符为|，显示行号</li><li><code>awk -F &#39;:&#39; &#39;&#123;OFS=&quot;|&quot;&#125; NR&lt;=3 &amp;&amp; $1=&quot;test&quot; &#123;print NR, $0&#125;&#39; test_awk2.txt</code></li></ul><h3 id="分段求和"><a href="#分段求和" class="headerlink" title="分段求和"></a>分段求和</h3><ul><li>格式<ul><li>BEGIN{} {} END{}</li><li>BEGIN{}在读取数据之前做的事情, 可以理解为: 前.</li><li>{} 在读取过程中做的事情, 可以理解为: 中.</li><li>END{} 在读取数据之后做的事情, 可以理解为: 后.</li></ul></li><li>对test_awk2.txt中的第2段求和<ul><li><code>awk -F &#39;:&#39; &#39;BEGING&#123;&#125;&#123;total=total+$2&#125;END&#123;print total&#125;&#39; test_awk2.txt</code></li><li><code>awk -F &#39;:&#39; &#39;BEGIN&#123;&#125;&#123;total=total+$2&#125;END&#123;print total&#125;&#39; test_awk2.txt</code></li></ul></li></ul><h3 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h3><ul><li>统计当前目录所有文本文件的大小</li></ul><div class="code-wrapper"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;BEGIN&#123;&#125;&#123;total=total+$5&#125; END&#123;print(total)&#125;&#x27;</span></code></pre></div><ul><li>打印99乘法表</li></ul><div class="code-wrapper"><pre><code class="hljs bash">awk <span class="hljs-string">&#x27;BEGIN&#123; for(i=1;i&lt;=9;i++)&#123; for(j=1;j&lt;=i;j++)&#123; printf(&quot;%dx%d=%d%s&quot;, i, j, i*j, &quot;\t&quot; ) &#125; printf(&quot;\n&quot;) &#125; &#125;&#x27;</span></code></pre></div><ul><li>求总成绩</li></ul><p>文本文件</p><div class="code-wrapper"><pre><code class="hljs apache"><span class="hljs-attribute">Marry</span>    <span class="hljs-number">2143</span> <span class="hljs-number">78</span> <span class="hljs-number">84</span> <span class="hljs-number">77</span><span class="hljs-attribute">Jack</span>     <span class="hljs-number">2321</span> <span class="hljs-number">66</span> <span class="hljs-number">78</span> <span class="hljs-number">45</span><span class="hljs-attribute">Tom</span>     <span class="hljs-number">2122</span> <span class="hljs-number">48</span> <span class="hljs-number">77</span> <span class="hljs-number">71</span><span class="hljs-attribute">Mike</span>     <span class="hljs-number">2537</span> <span class="hljs-number">87</span> <span class="hljs-number">97</span> <span class="hljs-number">95</span><span class="hljs-attribute">Bob</span>      <span class="hljs-number">2415</span> <span class="hljs-number">40</span> <span class="hljs-number">57</span> <span class="hljs-number">62</span></code></pre></div><p>脚本文件</p><div class="code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/awk -f</span><span class="hljs-meta">#</span><span class="bash">运行前</span>BEGIN &#123;    math = 0    english = 0    computer = 0     printf &quot;NAME    NO.   MATH  ENGLISH  COMPUTER   TOTAL\n&quot;    printf &quot;---------------------------------------------\n&quot;&#125;<span class="hljs-meta">#</span><span class="bash">运行中</span>&#123;    math+=$3    english+=$4    computer+=$5    printf &quot;%-6s %-6s %4d %8d %8d %8d\n&quot;, $1, $2, $3,$4,$5, $3+$4+$5&#125;<span class="hljs-meta">#</span><span class="bash">运行后</span>END &#123;    printf &quot;---------------------------------------------\n&quot;    printf &quot;  TOTAL:%10d %8d %8d \n&quot;, math, english, computer    printf &quot;AVERAGE:%10.2f %8.2f %8.2f\n&quot;, math/NR, english/NR, computer/NR&#125;</code></pre></div><h2 id="shell编程"><a href="#shell编程" class="headerlink" title="shell编程"></a>shell编程</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><ul><li>shell脚本执行方式Shell 是一个用 C 语言编写的程序，通过 Shell 用户可以访问操作系统内核服务。</li><li>查看系统安装 <code>shellcat /etc/shells</code></li><li>查看Linux系统默认的SHELL解释器的<code>echo $SHELL</code></li></ul><h3 id="格式-1"><a href="#格式-1" class="headerlink" title="格式"></a>格式</h3><ul><li><code>#!/bin/bash</code></li></ul><h3 id="4种运行方式"><a href="#4种运行方式" class="headerlink" title="4种运行方式"></a>4种运行方式</h3><ul><li>sh执行</li><li>工作目录执行</li><li>绝对路径执行</li><li>source hello.sh</li></ul><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><ul><li>用户变量</li><li>环境变量</li><li>特殊变量</li><li>$#命令行参数的个数</li><li>$n 表示第n个参数</li><li>$0 当前程序的名称</li><li>$? 前一个命令或许或函数的返回码</li><li>$*以“参数1 参数2 。。。”形式保存所有参数</li><li>$@ 以“参数1”“参数2”。。。形式保存所有参数</li><li>$$ 本程序的（进程ID号）PID</li><li>$! 上一个命令的PID</li></ul><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ul><li>优先使用双引号</li><li>拼接字符串</li><li><code>wenhou_1=&quot;你好,$yourname .&quot;</code></li><li><code>wenhou_2=&quot;你好,&quot;$yourname&quot; .&quot;</code></li><li><code>wenhou_3=&quot;你好,\&quot;$yourname\&quot; .&quot;</code></li><li>获取字符串长度</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>string=<span class="hljs-string">&quot;jobs&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;string&#125;</span>    <span class="hljs-comment"># 输出结果: jobs</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;#string&#125;</span>   <span class="hljs-comment"># 输出结果: 4</span></code></pre></div><ul><li>提取子字符串</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>string=<span class="hljs-string">&quot;敢于亮剑决不后退&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;string:2:2&#125;</span>    <span class="hljs-comment"># 输出结果为: 亮剑</span></code></pre></div><ul><li>查找字符串（记得有漂号）</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>string=<span class="hljs-string">&quot;i am a boy&quot;</span><span class="hljs-built_in">echo</span> `expr index <span class="hljs-string">&quot;<span class="hljs-variable">$string</span>&quot;</span> am`</code></pre></div><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><ul><li>支持包括：算术、关系、布尔、字符串等运算符</li></ul><h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h3><ul><li>数字语句判断<ul><li>-eq 检测两个数是否相等，相等返回 true。</li><li>-ne检测两个数是否不相等，不相等返回 true。</li><li>gt检测左边的数是否大于右边的，如果是，则返回 true。</li><li>lt检测左边的数是否小于右边的，如果是，则返回 true。</li><li>-ge检测左边的数是否大于等于右边的，如果是，则返回 true。</li><li>-le检测左边的数是否小于等于右边的，如果是，则返回 true。</li></ul></li><li>字符串语句判断<ul><li>n STRING 字符串长度不为零</li><li>z STRING 字符串长度为0</li><li>= 判断两个字符串是否一样</li><li>!=判断两个字符串是否不一样</li></ul></li><li>文件语句判断<ul><li>f 存在且是普通文件</li><li>-d 存在且是目录</li><li>h 存在且是符号链接</li><li>e 文件存在</li><li>–r 文件存在并且可读</li><li>–w 文件存在并且可写</li><li>–x 文件存在并且可执行</li></ul></li><li>test命令：可以代替[]<ul><li>为0表示为真，为1表示为假</li></ul></li><li>let命令：执行一个或多个表达式</li><li>if语句格式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> condition    //条件, 条件要用 [] 包裹.             <span class="hljs-keyword">then</span>                command1     //符合条件后, 就会执行这里的内容                command2                ...                commandN             <span class="hljs-keyword">fi</span></code></pre></div><ul><li>if else 语法格式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> condition    //条件, 用[]包裹        <span class="hljs-keyword">then</span>            command1    //符合条件后, 执行的内容            command2           ...            commandN        <span class="hljs-keyword">else</span>            <span class="hljs-built_in">command</span>        //不符合条件后, 执行的内容.         <span class="hljs-keyword">fi</span></code></pre></div><ul><li>if else-if else</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">if</span>  condition1        //条件1        <span class="hljs-keyword">then</span>            command1        //满足条件1后, 执行的内容        <span class="hljs-keyword">elif</span>  condition2     //条件2        <span class="hljs-keyword">then</span>             command2        //满足条件2后, 执行的内容        <span class="hljs-keyword">else</span>            commandN        //所有条件都不满足, 则执行这里.        <span class="hljs-keyword">fi</span></code></pre></div><ul><li>第一种for循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> 变量 <span class="hljs-keyword">in</span> 值1 值2 值3…            <span class="hljs-keyword">do</span>            程序            <span class="hljs-keyword">done</span></code></pre></div><ul><li>第二种for循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> ((初始值；循环控制条件；变量变化))            <span class="hljs-keyword">do</span>            程序            <span class="hljs-keyword">done</span></code></pre></div><ul><li>while循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">while</span> 条件        <span class="hljs-keyword">do</span>            程序        <span class="hljs-keyword">done</span></code></pre></div><ul><li>死循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 方式1</span><span class="hljs-keyword">while</span> :<span class="hljs-comment"># 方式2</span><span class="hljs-keyword">while</span> <span class="hljs-literal">true</span><span class="hljs-comment"># 方式3</span><span class="hljs-keyword">for</span> ((;;))</code></pre></div><ul><li>case判断语句</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-keyword">case</span> 值  <span class="hljs-keyword">in</span>        模式1)            command1            command2            ...            commandN            ;;        模式2）            command1            command2            ...            commandN            ;;        <span class="hljs-keyword">esac</span></code></pre></div><ul><li>跳出循环break和continue操作</li></ul><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><ul><li>语法格式</li></ul><div class="code-wrapper"><pre><code class="hljs bash">[ <span class="hljs-keyword">function</span> ] <span class="hljs-function"><span class="hljs-title">funname</span></span>()&#123;            action;            [<span class="hljs-built_in">return</span> int;]        &#125;</code></pre></div><ul><li>格式解释<ol><li>可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。 </li><li>参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255）</li></ol></li><li>注意事项<ol><li>函数返回值在调用该函数后通过 $? 来获得。 </li><li>所有函数在使用前必须定义。这意味着必须将函数放在脚本开始部分，直至shell解释器首次发现它时，才可以使用。 调用函数仅使用其函数名即可。 </li><li>函数的返回值只能是0-255区间的数据, 否则返回的内容可能不是我们想要的结果.  一般定义一个函数, 是为了实现特殊的功能, 当这个功能执行完成后, 返回一个状态信息. 0成功, 1失败等, 我们根据这个信息再来做其他的操作即可. </li><li>$? 严格意义来讲并不是获取返回值的, 而是获取上一个的执行后状态码信息(0-255之间)默认情况下如果状态码为0 表示成功执行, 如果为其他值, 则表示执行有问题</li></ol></li><li>如何具体接收方法返回值的问题<ul><li>在方法外定义变量</li><li>用输出语句输出,在方法外用变量接收该值</li></ul></li><li>有参数的函数操作<ul><li>$1表示第一个参数,以此类推</li></ul></li></ul><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><ul><li>定义数组  <div class="code-wrapper"><pre><code class="hljs bash">my_array=(A B <span class="hljs-string">&quot;C&quot;</span> D)array_name[0]=value0array_name[1]=value1array_name[2]=value2</code></pre></div></li><li>读取数组  <div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;第一个元素为: <span class="hljs-variable">$&#123;my_array[0]&#125;</span>&quot;</span></code></pre></div></li><li>获取数组所有元素  <div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组的全部元素为: <span class="hljs-variable">$&#123;my_array[*]&#125;</span>&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组的全部元素为: <span class="hljs-variable">$&#123;my_array[@]&#125;</span>&quot;</span></code></pre></div></li><li>获取数组长度  <div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组元素个数为: <span class="hljs-variable">$&#123;#my_array[*]&#125;</span>&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;数组元素个数为: <span class="hljs-variable">$&#123;#my_array[@]&#125;</span>&quot;</span></code></pre></div></li><li>遍历数组:结合2中for循环</li></ul><h3 id="select语句"><a href="#select语句" class="headerlink" title="select语句"></a>select语句</h3><ul><li>概念<ul><li>擅长于交互式场合。用户可以从一组不同的值中进行选择.</li></ul></li><li><code>格式PS3= //界面提示符select var in ... ; do　commond done  .... now $var can be used ...</code></li><li>注意事项:break 命令退出循环，或exit 命令终止脚本</li></ul><h3 id="加载其它变量"><a href="#加载其它变量" class="headerlink" title="加载其它变量"></a>加载其它变量</h3><ul><li>格式1：<code>. filename</code></li><li>格式2：<code>source filename</code> （推荐使用）</li></ul><h3 id="综合案例-1"><a href="#综合案例-1" class="headerlink" title="综合案例"></a>综合案例</h3><ul><li>1.猜数字小游戏</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment">#生成100以内的随机数 提示用户猜测 猜对为止</span><span class="hljs-comment">#random 系统自带，值为0-32767任意数</span><span class="hljs-comment">#随机数1-100</span>num=$[RANDOM%100+1]<span class="hljs-comment">#read 提示用户猜数字</span><span class="hljs-comment">#if判断</span><span class="hljs-keyword">while</span>  :<span class="hljs-keyword">do</span><span class="hljs-built_in">read</span> -p <span class="hljs-string">&quot;计算机生成了一个 1‐100 的随机数,你猜: &quot;</span> cai    <span class="hljs-keyword">if</span> [ <span class="hljs-variable">$cai</span> -eq <span class="hljs-variable">$num</span> ]    <span class="hljs-keyword">then</span>       <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;恭喜,猜对了&quot;</span>       <span class="hljs-built_in">exit</span>    <span class="hljs-keyword">elif</span> [ <span class="hljs-variable">$cai</span> -gt <span class="hljs-variable">$num</span> ]    <span class="hljs-keyword">then</span>           <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;不巧,猜大了&quot;</span>      <span class="hljs-keyword">else</span>           <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;不巧,猜小了&quot;</span> <span class="hljs-keyword">fi</span><span class="hljs-keyword">done</span></code></pre></div><ul><li>2.数据库定时备份</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment">#完成数据库的定时备份</span><span class="hljs-comment">#备份的路径</span>BACKUP=/<span class="hljs-built_in">export</span>/data/db<span class="hljs-comment">#当前时间作为文件名</span>DATETIME=$(date +%Y_%m_%d_%H%M%S)<span class="hljs-comment">#可以通过输出变量来调试</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$DATETIME</span><span class="hljs-comment">#使用变量的时候，也可以用&#123;&#125;花括号的方式把变量名包起来，如下：</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;DATETIME&#125;</span> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;---------------------开始备份数据库---------------------&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;---------------------备份的路径是<span class="hljs-variable">$BACKUP</span>/<span class="hljs-variable">$DATETIME</span>.tar.gz---------------------&quot;</span><span class="hljs-comment">#主机ip地址</span>HOST=192.168.88.100<span class="hljs-comment">#数据库用户名</span>DB_USER=root<span class="hljs-comment">#数据库密码</span>DB_PWD=123456<span class="hljs-comment">#数据库名</span>DATABASE=test_shop<span class="hljs-comment">#创建备份路径</span><span class="hljs-comment">#如果备份的文件夹路径存在的话，就直接使用，否则就创建</span>[ ! -d <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>&quot;</span> ] &amp;&amp; mkdir -p <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>&quot;</span><span class="hljs-comment">#执行mysql的备份数据库的指令</span>mysqldump -u<span class="hljs-variable">$&#123;DB_USER&#125;</span> -p<span class="hljs-variable">$&#123;DB_PWD&#125;</span> --host=<span class="hljs-variable">$&#123;HOST&#125;</span> <span class="hljs-variable">$&#123;DATABASE&#125;</span> | gzip &gt; <span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span>.sql.gz<span class="hljs-comment">#打包备份文件</span><span class="hljs-built_in">cd</span> <span class="hljs-variable">$&#123;BACKUP&#125;</span>tar -czvf <span class="hljs-variable">$&#123;DATETIME&#125;</span>.tar.gz <span class="hljs-variable">$&#123;DATETIME&#125;</span><span class="hljs-comment">#删除临时目录</span>rm -rf <span class="hljs-variable">$&#123;BACKUP&#125;</span>/<span class="hljs-variable">$&#123;DATETIME&#125;</span><span class="hljs-comment">#删除10天前的备份文件</span>find <span class="hljs-variable">$&#123;BACKUP&#125;</span> -mtime +10 -name <span class="hljs-string">&quot;*.tar.gz&quot;</span> -<span class="hljs-built_in">exec</span> rm -rf &#123;&#125; \;<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-------------------------备份成功-------------------------&quot;</span></code></pre></div><h2 id="小技巧总结"><a href="#小技巧总结" class="headerlink" title="小技巧总结"></a>小技巧总结</h2><h3 id="给变量赋值的特殊写法"><a href="#给变量赋值的特殊写法" class="headerlink" title="给变量赋值的特殊写法"></a>给变量赋值的特殊写法</h3><div class="code-wrapper"><pre><code class="hljs bash">a=`Linux命令`a=$(Linux命令)<span class="hljs-comment"># 这两个命令的执行结果是一样的, 都是把Linux命令的执行结果给变量</span></code></pre></div><h3 id="关于数字的运算"><a href="#关于数字的运算" class="headerlink" title="关于数字的运算"></a>关于数字的运算</h3><div class="code-wrapper"><pre><code class="hljs bash">$((<span class="hljs-number">10</span> + <span class="hljs-number">20</span>))$[10 + 20]`expr 10 + 20 `</code></pre></div><h3 id="关于for-while的条件"><a href="#关于for-while的条件" class="headerlink" title="关于for, while的条件"></a>关于for, while的条件</h3><div class="code-wrapper"><pre><code class="hljs bash">如果是 <span class="hljs-keyword">for</span>(()) 或者 <span class="hljs-keyword">while</span> (()) 这种方式, 不用通过$引入变量, 可以直接用.如果是 <span class="hljs-keyword">for</span>[] 或者 <span class="hljs-keyword">while</span>[]这种方式, 必须通过$引入变量, 才可以继续使用.</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>存档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hive性能优化</title>
    <link href="/2021/05/20/hive%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <url>/2021/05/20/hive%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="基础优化"><a href="#基础优化" class="headerlink" title="基础优化"></a>基础优化</h2><ul><li>Shuffle 阶段压缩</li><li>hive的数据压缩<ul><li>Snappy</li></ul></li><li>hive的数据存储格式<ul><li>ORC</li><li>TextFile</li></ul></li><li>fetch抓取</li><li>本地模式</li><li>join的优化<ul><li>小表在前，小表放入缓冲区</li><li>谓词下推，先过滤再 join</li></ul></li><li>SQL优化的方案<ul><li>列裁剪</li><li>分区裁剪</li><li>group by 优化</li><li>count（distinct）优化</li></ul></li><li>动态分区调整</li><li>并行编译执行</li><li>严格模式<ul><li>拒绝可能影响效率的 SQL 语句</li></ul></li><li>jvm的重用</li><li>推测执行</li><li>小文件合并</li><li>矢量化查询</li><li>读取零拷贝</li><li>关联优化器<ul><li>多个MR共享使用一个shuffle结果</li></ul></li></ul><h2 id="Join-优化"><a href="#Join-优化" class="headerlink" title="Join 优化"></a>Join 优化</h2><h3 id="map-join"><a href="#map-join" class="headerlink" title="map join"></a>map join</h3><ul><li>核心<ul><li>将某个符合条件的表放置在内存中, 然后和另一个在map端进行join工作</li></ul></li><li>解决问题<ul><li>1.数据倾斜问题</li><li>2.提升查询的效率</li></ul></li><li>适用场景<ul><li>小表和大表join场景</li></ul></li><li>使用条件<ul><li>1.必须有一张表为小表<ul><li>set hive.auto.convert.join.noconditionaltask.size=512000000</li><li>设置小表最大数据量,默认值为 20M</li></ul></li><li>2.必须开启map Join优化支持<ul><li>set hive.auto.convert.join=true; 默认就是true</li></ul></li><li>3.不限制任何类型的表</li></ul></li></ul><h3 id="bucket-map-join"><a href="#bucket-map-join" class="headerlink" title="bucket map join"></a>bucket map join</h3><ul><li>适应场景<ul><li>中型表 和 大表 join</li></ul></li><li>使用条件<ul><li>1.开启bucket map join: set hive.optimize.bucketmapjoin = true;</li><li>2.一个表的bucket数是另一个表bucket数的整数倍</li><li><ol start="3"><li>bucket列 == join列</li></ol></li><li>4.必须是应用在map join的场景中</li><li>5.两个表必须是分桶表</li></ul></li></ul><h3 id="SMB-map-join"><a href="#SMB-map-join" class="headerlink" title="SMB map join"></a>SMB map join</h3><ul><li>适应场景<ul><li>大表 和 大表join</li></ul></li><li>使用条件<ul><li>1.开启bucket map join: set hive.optimize.bucketmapjoin = true;</li><li>2.两个表的分桶数量是一致的</li><li>3.bucket列 == join列 == sort列</li><li>4.必须是应用在bucket map join的场景中</li><li>5.两个表必须是分桶表</li></ul></li><li>整理SMB条件<ul><li><ol><li>保证 join的表必须是桶表:</li></ol><ul><li>set hive.enforce.bucketing=true; 写入数据强制分桶</li></ul></li><li><ol start="2"><li>在建表的时候, 必须设置分桶排序字段, 而且需要保证, 分桶字段 == join字段 == 排序字段create table test_smb_2( mid string, age_id string ) CLUSTERED BY(mid) SORTED BY(mid) INTO 500 BUCKETS; set hive.enforce.sorting=true; – 开启强制排序操作</li></ol></li><li><ol start="3"><li>两个分桶表的分桶的数量必须是一致的</li></ol></li><li><ol start="4"><li>必须建立在bucket map join基础上:</li></ol><ul><li>set hive.optimize.bucketmapjoin = true;</li></ul></li><li><ol start="5"><li>开启SMB join支持</li></ol><ul><li>set hive.auto.convert.sortmerge.join=true;</li><li>set hive.auto.convert.sortmerge.join.noconditionaltask=true;</li></ul></li><li><ol start="6"><li>必须开启自动尝试SMB连接</li></ol><ul><li>set hive.optimize.bucketmapjoin.sortedmerge = true;</li></ul></li></ul></li></ul><h2 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><ul><li>减少最终数据的扫描量, 从而提高效率</li></ul><h3 id="原始索引"><a href="#原始索引" class="headerlink" title="原始索引"></a>原始索引</h3><ul><li>特点<ul><li>可以针对hive表中任意字段构建索引</li><li>构建索引后, 在查询数据时候, 根据索引查询, 减少MR读取数据扫描量</li></ul></li><li>弊端<ul><li>索引不能自动更新, 当表中数据发生变更后, 需要手动重建索引</li><li>此性能是一般(数据量越大, 性能越差)</li></ul></li><li>应用场景<ul><li>生产中一般不采用原始索引</li><li>在hive3.0版本后, 不支持hive原始索引</li></ul></li></ul><h3 id="行组索引row-group-index"><a href="#行组索引row-group-index" class="headerlink" title="行组索引row group index"></a>行组索引row group index</h3><ul><li>特点<ul><li>一旦开启后, 会将表的每一个字段的最小 最大值都放置在索引中</li></ul></li><li>使用条件<ul><li>1.必须应用在表结构为ORC类型</li><li>2.在插入数据的时候, 要保证后续需要索引的字段, 是有序插入的</li><li>3.行组索引主要是应用在数值类型的字段上</li><li>4.在创建表的时候, 必须开启 行组索引<ul><li>‘orc.create.index’=’true’</li></ul></li><li><ol start="5"><li>开启hive自动使用索引(CM)</li></ol><ul><li>hive.optimize.index.filter=true</li></ul></li></ul></li><li>应用场景<ul><li>主要是应用 数值类型字段上, 并且执行 &gt; &lt; &gt;= &lt;= = 相关的操作中</li></ul></li><li>使用方案<ul><li>1.在建表的时候, 需要开启行组索引CREATE TABLE lxw1234_orc2 ( 字段 ….)stored AS ORCTBLPROPERTIES( ‘orc.compress’=’SNAPPY’,- 开启行组索引 ‘orc.create.index’=’true’)</li><li>2.在向表中插入数据的时候, 保证对应需要索引的列 进行有序插入</li></ul></li></ul><h3 id="开发过滤索引Bloom-Filter-index"><a href="#开发过滤索引Bloom-Filter-index" class="headerlink" title="开发过滤索引Bloom Filter index"></a>开发过滤索引Bloom Filter index</h3><ul><li>说明<ul><li>将需要构建索引的字段的值, 在索引信息(script)中进行放置</li><li>当查询的时候, 根据索引字段查询, 首先会先查询这个script索引信息中, 是否包含这个值</li><li>如果包含, 直接查询这个script片段, 如果不包含, 直接跳过</li></ul></li><li>使用条件<ul><li>1.必须应用在表结构为ORC类型</li><li>2.在建表的时候, 需要指定为那些字段构建开发过滤索引:<ul><li>‘orc.bloom.filter.columns’=’pcid,字段2, 字段3’</li></ul></li><li>3.必须要应用在 等值连接场景中 (不局限数据类型)</li></ul></li><li>应用场景<ul><li>应用在等值连接场景(不限制数据类型)</li></ul></li><li>使用方案<ul><li>1.需要开启行组索引</li><li>2.指定字段开启BloomFilter索引CREATE TABLE lxw1234_orc2 ( 字段) stored AS ORCTBLPROPERTIES( ‘orc.compress’=’SNAPPY’, ‘orc.create.index’=’true’,- pcid字段开启BloomFilter索引 “orc.bloom.filter.columns”=”pcid”)</li></ul></li></ul><h3 id="总结索引使用方法"><a href="#总结索引使用方法" class="headerlink" title="总结索引使用方法"></a>总结索引使用方法</h3><ul><li>行组索引建议常开<ul><li>在查询过程中, 如果正常插入数据是有序的, 并且根据这个字段查询操作</li><li>自动使用行组索引, 从而提升效率, 而且增加行组索引, 不会导致太多冗余数据出现</li></ul></li><li>开发过滤索引看需求<ul><li>将后续会经常的进行等值连接的字段, 构建为开发过滤的索引</li><li>需要大家有一定的预判能力</li></ul></li></ul><h2 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h2><h3 id="join-的数据倾斜"><a href="#join-的数据倾斜" class="headerlink" title="join 的数据倾斜"></a>join 的数据倾斜</h3><ul><li>方案1: 采用 map join</li><li>方案2: 将数据倾斜的key单独找一个MR处理,最后合并结果<ul><li>运行期解决<ul><li>说明<ul><li>知道一定有数据倾斜, 但是不知道哪一个key会有数据倾斜</li></ul></li><li>处理思路<ul><li>在运行的过程中, 对每个key的数量进行计数, 当发现key对应的value的条数比较大的时候, 认为key出现了数据倾斜问题</li><li>将整个键值从当面MR移出去, 单独找一个MR来处理</li></ul></li><li>配置方式<ul><li>set hive.optimize.skewjoin=true;</li><li>开启运行期数据倾斜的处理,默认为FALSE</li><li>set hive.skewjoin.key=100000;</li><li>当key数据量到达多少的时候, 认为出现数据倾斜</li></ul></li></ul></li><li>编译期解决<ul><li>说明<ul><li>提前知道哪个key会出现数据倾斜</li></ul></li><li>处理思路<ul><li>创建表的时候, 提前指定那个key会出现数据倾斜</li><li>后期执行操作的时候hive会在编译期将key单独移出去, 单独找一个MR运行</li></ul></li><li>配置方式<ul><li>配置参数<ul><li>set hive.optimize.skewjoin.compiletime=true;</li><li>是否开启编译期数据倾斜解决</li></ul></li><li>创建表的参数<ul><li>SKEWED BY (key) ON (1,5,6)</li><li>倾斜的字段和需要拆分的key值</li><li>[STORED AS DIRECTORIES];-</li><li>为倾斜值创建子目录单独存放</li></ul></li></ul></li></ul></li><li>开启MR合并优化<ul><li>说明<ul><li>两个MR执行完成后, 直接将数据写入到最终目录下, 直接作为最终结果</li><li>建议和join倾斜的配置同时开启</li></ul></li><li>设置参数<ul><li>set hive.optimize.union.remove=true;</li><li>开启对union的优化配置</li></ul></li></ul></li><li>生产环境做法<ul><li>两个组合使用, 提前知道提前定义好, 不知道的采用运行期解决, 达到效率最高</li><li>并且可以解决所有的join倾斜问题</li></ul></li></ul></li></ul><h3 id="group-by-的数据倾斜"><a href="#group-by-的数据倾斜" class="headerlink" title="group by 的数据倾斜"></a>group by 的数据倾斜</h3><ul><li>说明<ul><li>当某一个的数据量远远大于其它组的数据量, 这个时候就容易出现数据倾斜</li></ul></li><li>解决方案<ul><li>方案1: 小的combiner操作<ul><li>说明<ul><li>利用规约提前处理k2v2</li></ul></li><li>设置参数<ul><li>hive.map.aggr=true;</li><li>开启map端的combiner操作</li></ul></li></ul></li><li>方案2:负载均衡策略(大的combiner)<ul><li>说明<ul><li>利用2个MR来解决, 第一个MR负责将数据打散,保证每一个reduce都能接收到大致相等的数据量</li><li>得出一个局部结果,第二个MR对局部结果进一步处理,得出最终结果(自定义分区)</li></ul></li><li>设置参数<ul><li>hive.groupby.skewindata=true;</li><li>开启groupby负载均衡 默认为FALSE</li></ul></li></ul></li><li>注意事项<ul><li>在多个列上进行的去重操作与hive环境变量 hive.groupby.skewindata 存在冲突。</li></ul></li></ul></li></ul><h3 id="如何判断执行的SQL存在数据倾斜"><a href="#如何判断执行的SQL存在数据倾斜" class="headerlink" title="如何判断执行的SQL存在数据倾斜"></a>如何判断执行的SQL存在数据倾斜</h3><ul><li>1.查看MR日志,对比reduce执行时间</li><li>2.运行过程中,通过HUE来观察</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>HiveSQL优化方法与实践（全）</p><p><a href="https://mp.weixin.qq.com/s/C8236cW8E4HMwox-UJFrgA">https://mp.weixin.qq.com/s/C8236cW8E4HMwox-UJFrgA</a> </p><p>原来 8 张图，就可以搞懂「零拷贝」了！</p><p><a href="https://mp.weixin.qq.com/s/P0IP6c_qFhuebwdwD8HM7w">https://mp.weixin.qq.com/s/P0IP6c_qFhuebwdwD8HM7w</a></p>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hbase 无法删除表问题及解决办法</title>
    <link href="/2021/03/20/Hbase%20%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E8%A1%A8%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2021/03/20/Hbase%20%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E8%A1%A8%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul><li>正常删除表格的方法</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 禁用表</span><span class="hljs-built_in">disable</span> <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span><span class="hljs-comment"># 删除表</span>drop <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span></code></pre></div><p>但是操作过程中出现如下的问题</p><ul><li>已经禁用表</li></ul><div class="code-wrapper"><pre><code class="hljs bash">hbase(main):005:0&gt; <span class="hljs-built_in">disable</span> <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span>ERROR: Table TRIPDB:trip_sample is disabled!For usage try <span class="hljs-string">&#x27;help &quot;disable&quot;&#x27;</span>Took 0.1485 seconds</code></pre></div><ul><li>但是无法删除表，删除报错，死循环</li></ul><div class="code-wrapper"><pre><code class="hljs bash">hbase(main):004:0&gt; drop <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span>ERROR: Table org.apache.hadoop.hbase.TableNotDisabledException: Not DISABLED; tableName=TRIPDB:trip_sample, state=ENABLING        at org.apache.hadoop.hbase.master.HMaster.checkTableModifiable(HMaster.java:2517)        at org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.prepareDelete(DeleteTableProcedure.java:241)        at org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.executeFromState(DeleteTableProcedure.java:89)        at org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.executeFromState(DeleteTableProcedure.java:56)        at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:189)        at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:850)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1473)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1241)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access<span class="hljs-variable">$800</span>(ProcedureExecutor.java:75)        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor<span class="hljs-variable">$WorkerThread</span>.run(ProcedureExecutor.java:1761) should be disabled!For usage try <span class="hljs-string">&#x27;help &quot;drop&quot;&#x27;</span>Took 0.7635 seconds                                                                                                                                                  hbase(main):005:0&gt; <span class="hljs-built_in">disable</span> <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span>ERROR: Table TRIPDB:trip_sample is disabled!For usage try <span class="hljs-string">&#x27;help &quot;disable&quot;&#x27;</span></code></pre></div><h3 id="产生原因"><a href="#产生原因" class="headerlink" title="产生原因"></a>产生原因</h3><p>1、未开启 Zookeeper 就开启 Hbase 创建表，元数据没有同步到 ZK</p><p>2、 disable  tableName 命令执行未完成就删除 Hbase， 以至于表格被锁定</p><p>3、数据输入过大，HMaster 崩溃</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><ul><li>删除 Hbase 对应表格元数据</li></ul><div class="code-wrapper"><pre><code class="hljs bash">scan <span class="hljs-string">&quot;hbase:meta&quot;</span><span class="hljs-comment"># 找到要删除的表格的 rowkey</span>                                                                                                                                              TRIPDB:trip_sample                               column=table:state, timestamp=1632540393079, value=\x08\x03                                                                                     TRIPDB:trip_sample,,1632540392481.c3d71389e53809 column=info:regioninfo, timestamp=1632540396531, value=&#123;ENCODED =&gt; c3d71389e538092c4eda1b01b5a0a441, NAME =&gt; <span class="hljs-string">&#x27;TRIPDB:trip_sample,,1632540392481</span><span class="hljs-string"> 2c4eda1b01b5a0a441.                              .c3d71389e538092c4eda1b01b5a0a441.&#x27;</span>, STARTKEY =&gt; <span class="hljs-string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="hljs-string">&#x27;&#x27;</span>&#125;                                                                              TRIPDB:trip_sample,,1632540392481.c3d71389e53809 column=info:sn, timestamp=1632540396531, value=node1,16020,1632540299845                                                                        2c4eda1b01b5a0a441.                                                                                                                                                                              TRIPDB:trip_sample,,1632540392481.c3d71389e53809 column=info:state, timestamp=1632540396531, value=OPENING                                                                                       2c4eda1b01b5a0a441.<span class="hljs-comment"># 也可以通过报错查看 rowkey</span>scan <span class="hljs-string">&quot;TRIPDB:trip_sample&quot;</span></code></pre></div><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 根据 rowkey 删除整行数据</span>deleteall <span class="hljs-string">&#x27;hbase:meta&#x27;</span>,<span class="hljs-string">&quot;TRIPDB:trip_sample,,1632540392481.c3d71389e53809&quot;</span></code></pre></div><ul><li>删除 Zookeeper 中的表格元数据，在 table  和 table-lock 中</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 登陆Zookeeper 客户端</span>bin/zkCli.sh<span class="hljs-comment"># 删除对应表格元数据</span>ls /hbase/tablels /hbase/table-lockrmr /hbase/tablermr /hbase/table-lock</code></pre></div><ul><li>删除 HDFS 上对应数据</li></ul><div class="code-wrapper"><pre><code class="hljs bash">hdfs dfs ls /hbase/data/TRIPDB/hdfs dfs rm -r /hbase/data/TRIPDB/trip_sample</code></pre></div><ul><li>修复 Hbase 元数据</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment">#命令行执行</span>hbase hbck -fixMeta</code></pre></div><ul><li>重启 ZK 和 Hbase</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 关闭</span>bin/zkServer.sh stopbin/stop-hbase.sh stop<span class="hljs-comment"># 重启</span>bin/zkServer.sh startbin/stop-hbase.sh start</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Debug记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hbase</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive数仓缓慢渐变维之拉链表</title>
    <link href="/2021/03/05/Hive%E6%95%B0%E4%BB%93%E7%BC%93%E6%85%A2%E6%B8%90%E5%8F%98%E7%BB%B4%E4%B9%8B%E6%8B%89%E9%93%BE%E8%A1%A8/"/>
    <url>/2021/03/05/Hive%E6%95%B0%E4%BB%93%E7%BC%93%E6%85%A2%E6%B8%90%E5%8F%98%E7%BB%B4%E4%B9%8B%E6%8B%89%E9%93%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="缓慢渐变维"><a href="#缓慢渐变维" class="headerlink" title="缓慢渐变维"></a>缓慢渐变维</h3><p>主要是为了解决, 是否需要在数仓中维护历史变化的数据操作</p><p>注意：如果不维护一个数据的历史变化信息, 那么在进行数仓分析的时候, 是有可能对未来分析的结果产生影响</p><p><strong>实现缓慢维的3种方式</strong></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-operator">*</span><span class="hljs-operator">*</span>SCD1<span class="hljs-operator">*</span><span class="hljs-operator">*</span>对于历史变化的数据, 是进行维护操作, 直接进行覆盖即可此种操作仅适合于对于错误数据处理<span class="hljs-operator">*</span><span class="hljs-operator">*</span>SCD2(拉链表)<span class="hljs-comment">--常用**</span>对原有表, 增加两个新的字段, 一个是起始的时间字段,一个是截止时间字段当有数据发生变更后, 只需要对上一次数据进行标记起止范围, 新增一个新的变更后的数据即可, 由此产生一个拉链数据状态   好处: 不会修改原有记录数据操作, 利于维护操作, 而且可以对多个历史版本进行数据存储操作  弊端: 造成数据冗余, 占用更大的磁盘空间<span class="hljs-operator">*</span><span class="hljs-operator">*</span>SCD3<span class="hljs-operator">*</span><span class="hljs-operator">*</span>直接对原有表, 进行新增列的方案, 一旦有数据发生变更, 新增一列字段, 标记当前最新数据即可,以此来维护历史变化数据 适合于磁盘空间不足, 而且只需要维护少量历史变化情况  优点: 尽可能减少数据冗余情况,   弊端: 不利于维护, 仅能维护少量的历史变化版本</code></pre></div><h3 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h3><ul><li>拉链表处理逻辑</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>创建临时表<span class="hljs-number">2.</span>查询新数据，即抽取数据中 end_time<span class="hljs-operator">=</span>&quot;9999-99-99&quot;<span class="hljs-number">3.</span>查询旧数据，就原拉链表的数据，如果主键 id 与抽取数据中的重复，且 end_time<span class="hljs-operator">=</span>&quot;9999-99-99&quot;,需要改成 end_time<span class="hljs-operator">=</span>业务日期 <span class="hljs-operator">-</span> <span class="hljs-number">1</span>天<span class="hljs-number">4.</span>合并新旧数据到临时表<span class="hljs-number">5.</span>临时表插入原表</code></pre></div><ul><li>基本流程图</li><li><img src="https://i.loli.net/2021/09/18/ptV2hILPlW1uZUB.png" alt="CD2"></li></ul>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>牛客网SQL练习总结</title>
    <link href="/2021/02/13/%E7%89%9B%E5%AE%A2%E7%BD%91SQL%E7%BB%83%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <url>/2021/02/13/%E7%89%9B%E5%AE%A2%E7%BD%91SQL%E7%BB%83%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a>补充知识</h2><h3 id="补充知识整理"><a href="#补充知识整理" class="headerlink" title="补充知识整理"></a>补充知识整理</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>一张表可以多次被引用使用<span class="hljs-number">2.</span>筛选条件包含某个值, 这个值可以通过子查询求出, 再通过 <span class="hljs-keyword">where</span> 条件判断<span class="hljs-number">3.</span>子查询没有符合要求的条件会直接返回 <span class="hljs-keyword">null</span><span class="hljs-number">4.</span><span class="hljs-keyword">insert</span> ignore <span class="hljs-keyword">into</span> 相当于 replace<span class="hljs-number">5.</span>创建视图格式:  <span class="hljs-keyword">create</span> <span class="hljs-keyword">view</span> actor_name_view <span class="hljs-keyword">as</span> <span class="hljs-operator">+</span> 字段列表(查询结果等)<span class="hljs-number">6.</span>查询强制走索引: <span class="hljs-keyword">from</span> 表名 <span class="hljs-operator">+</span> force index(索引名)<span class="hljs-number">7.</span>创建表字段后<span class="hljs-operator">+</span> <span class="hljs-keyword">default</span> <span class="hljs-string">&#x27;默认值&#x27;</span>, 即赋予默认值<span class="hljs-number">8.</span>如果需要对数据进行更新操作, 一定是通过update 或者 replace 进行<span class="hljs-number">9.</span>需要排除最大值最小值再进行计算的题目,可以通过子查询排除, 或者通过 first_value last_value 开窗函数进行<span class="hljs-number">10.</span>exsits 用来判断是否存在某种条件的记录，存在就返回 <span class="hljs-literal">true</span> ,不存在则返回 <span class="hljs-literal">false</span><span class="hljs-number">11.</span>第 <span class="hljs-number">1</span> 的隐含条件是 该字段数值大于 它的数量为 <span class="hljs-number">0</span> , 大于等于它的数量为 <span class="hljs-number">1</span> , 以此类推<span class="hljs-number">12.</span>聚合字段的部分值，可以让其它值为<span class="hljs-number">0</span>，为<span class="hljs-keyword">null</span>，聚合的时候不影响结果</code></pre></div><h3 id="非空校验方法"><a href="#非空校验方法" class="headerlink" title="非空校验方法"></a>非空校验方法</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--如果x为null，就返回y，否则返回x</span>nvl(x,y)<span class="hljs-comment">--返回集合中第一个不为null的值，如果全部为null就返回null</span>colease(x,y,e,d,f)<span class="hljs-comment">--如果x为null，就返回y，否则返回x</span>ifnull(x,y)</code></pre></div><h3 id="创建和删除索引"><a href="#创建和删除索引" class="headerlink" title="创建和删除索引"></a>创建和删除索引</h3><p><a href="https://www.nowcoder.com/practice/e1824daa0c49404aa602cf0cb34bdd75">SQL37 对first_name创建唯一索引uniq_idx_firstname</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- MySQL中4种方式给字段添加索引</span><span class="hljs-comment">-- 创建主键索引,索引值必须是唯一的，且不能为NULL。</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> <span class="hljs-keyword">PRIMARY</span> KEY (col_list);<span class="hljs-comment">-- 创建唯一索引,值唯一</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> <span class="hljs-keyword">UNIQUE</span> index_name (col_list);<span class="hljs-comment">-- 创建普通索引,索引值可以重复出现</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> INDEX index_name (col_list);<span class="hljs-comment">-- 创建全文索引, 指定索引</span><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">ADD</span> FULLTEXT index_name (col_list);<span class="hljs-comment">-- 删除索引</span><span class="hljs-keyword">DROP</span> INDEX index_name <span class="hljs-keyword">ON</span> tbl_name;<span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">DROP</span> INDEX index_name；<span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> tbl_name <span class="hljs-keyword">DROP</span> <span class="hljs-keyword">PRIMARY</span> KEY;</code></pre></div><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><div class="code-wrapper"><pre><code class="hljs sql">在MySQL中，创建触发器语法如下：<span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TRIGGER</span> trigger_nametrigger_time trigger_event <span class="hljs-keyword">ON</span> tbl_name<span class="hljs-keyword">FOR</span> <span class="hljs-keyword">EACH</span> <span class="hljs-type">ROW</span>trigger_stmt其中：trigger_name：标识触发器名称，用户自行指定；trigger_time：标识触发时机，取值为 BEFORE 或 AFTER；trigger_event：标识触发事件，取值为 <span class="hljs-keyword">INSERT</span>、UPDATE 或 <span class="hljs-keyword">DELETE</span>；tbl_name：标识建立触发器的表名，即在哪张表上建立触发器；trigger_stmt：触发器程序体，可以是一句<span class="hljs-keyword">SQL</span>语句，或者用 <span class="hljs-keyword">BEGIN</span> 和 <span class="hljs-keyword">END</span> 包含的多条语句，每条语句结束要分号结尾。</code></pre></div><p><a href="https://www.notion.so/7e920bb2e1e74c4e83750f5c16033e2e">SQL41 构造一个触发器audit_log</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">trigger</span> audit_logafter <span class="hljs-keyword">insert</span> <span class="hljs-keyword">on</span> employees_test<span class="hljs-keyword">for</span> <span class="hljs-keyword">each</span> <span class="hljs-type">row</span><span class="hljs-keyword">begin</span><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> audit <span class="hljs-keyword">values</span>(new.id,new.name);<span class="hljs-keyword">end</span></code></pre></div><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><ul><li>窗口函数完整模式</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 指定字段求和并排序，窗口范围所有行</span><span class="hljs-built_in">sum</span>(x) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> b  <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> unbounded preceding  <span class="hljs-keyword">and</span> unbounded following )<span class="hljs-comment">-- 指定字段求和并排序，窗口范围是从改值前面的行到当前行</span><span class="hljs-built_in">sum</span>(x) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> b  <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> unbounded preceding  <span class="hljs-keyword">and</span> <span class="hljs-keyword">current</span> <span class="hljs-type">row</span> )<span class="hljs-comment">-- 指定字段求和并做排序，将所有行当前字段数据中，当前行值+2 和 -2范围之内的所有值求和 </span><span class="hljs-built_in">sum</span>(x) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a <span class="hljs-keyword">range</span> <span class="hljs-keyword">between</span> <span class="hljs-number">2</span> preceding  <span class="hljs-keyword">and</span> <span class="hljs-number">2</span> followling )</code></pre></div><ul><li>常用窗口函数</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 排名窗口函数</span><span class="hljs-built_in">row_number</span>()<span class="hljs-built_in">rank</span>()<span class="hljs-built_in">dense_rank</span>()<span class="hljs-comment">-- 聚合窗口函数</span>sum 求和count 总数max 最大值min 最小值avg 平均值<span class="hljs-comment">--其它窗口函数</span>lag 上一行的值lead 下一行的值first_value 该字段第一行的值last_value 该字段最后一行的值</code></pre></div><h3 id="其它常用函数"><a href="#其它常用函数" class="headerlink" title="其它常用函数"></a>其它常用函数</h3><div class="code-wrapper"><pre><code class="hljs sql">concat(字符<span class="hljs-number">1</span>, 连接符,字符<span class="hljs-number">2</span>) 指定连接符连接字符 <span class="hljs-number">1</span> 和字符 <span class="hljs-number">2</span>group_concat（X，Y）X是要连接的字段，Y是连接时用的符号，可省略，默认为逗号。此函数必须与<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> 配合使用。统计字符串长度：<span class="hljs-keyword">char_length</span>(<span class="hljs-string">&#x27;string&#x27;</span>) <span class="hljs-operator">/</span> <span class="hljs-keyword">char_length</span>(column_name)<span class="hljs-number">1</span>、返回值为字符串string或者对应字段长度，长度的单位为字符，一个多字节字符（例如，汉字）算作一个单字符；<span class="hljs-number">2</span>、不管汉字还是数字或者是字母都算是一个字符；<span class="hljs-number">3</span>、任何编码下，多字节字符都算是一个字符；参考资料来源：https:<span class="hljs-operator">/</span><span class="hljs-operator">/</span>blog.csdn.net<span class="hljs-operator">/</span>iris_xuting<span class="hljs-operator">/</span>article<span class="hljs-operator">/</span>details<span class="hljs-operator">/</span><span class="hljs-number">53763894</span>length(<span class="hljs-string">&#x27;string&#x27;</span>)<span class="hljs-operator">/</span>length(column_name)<span class="hljs-number">1</span>、utf8字符集编码下,一个汉字是算三个字符,一个数字或字母算一个字符。<span class="hljs-number">2</span>、其他编码下,一个汉字算两个字符, 一个数字或字母算一个字符。字符串替换：replace(s,s1,s2)，将字符串 s2 替代字符串 s 中的字符串 s1MySQL常用函数：https:<span class="hljs-operator">/</span><span class="hljs-operator">/</span>www.runoob.com<span class="hljs-operator">/</span>mysql<span class="hljs-operator">/</span>mysql<span class="hljs-operator">-</span>functions.html截取字符串函数 substr(X,Y,Z) 或 substr(X,Y) X是要截取的字符串 Y是字符串的起始位置Z是要截取字符串的长度<span class="hljs-keyword">left</span> 和 <span class="hljs-keyword">right</span> 函数切割，后面 <span class="hljs-keyword">left</span>(a,<span class="hljs-number">7</span>) a是要切割的字符串，<span class="hljs-number">7</span>是切割位数date_sub(<span class="hljs-type">date</span>,expr) <span class="hljs-type">date</span><span class="hljs-operator">/</span>datetime 减去expr 值后返回对应 <span class="hljs-type">date</span><span class="hljs-operator">/</span>datetime                   </code></pre></div><h2 id="求排名"><a href="#求排名" class="headerlink" title="求排名"></a>求排名</h2><h3 id="求排名第N个的"><a href="#求排名第N个的" class="headerlink" title="求排名第N个的"></a>求排名第N个的</h3><p><a href="https://www.nowcoder.com/practice/ec1ca44c62c14ceb990c3c40def1ec6c">SQL2 查找入职员工时间排名倒数第三的员工所有信息</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 方法1:先求出排名求第N个, 再倒排</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> (<span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> employees<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> hire_date <span class="hljs-keyword">desc</span>limit <span class="hljs-number">0</span>,<span class="hljs-number">3</span>) e<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> e.hire_datelimit <span class="hljs-number">1</span>;<span class="hljs-comment">-- 方法2:子查询,排名字段比自己高大于N的即可, 缺点是如果数据少于排名就不能求出</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> employees e1<span class="hljs-keyword">where</span> <span class="hljs-number">2</span><span class="hljs-operator">=</span>(<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">from</span> employees e2 <span class="hljs-keyword">where</span> e1.hire_date <span class="hljs-operator">&lt;</span> e2.hire_date);<span class="hljs-comment">-- 方法3:子查询, 比排名N+1中最高的就是第N个</span><span class="hljs-comment">-- 方法4:开窗函数，不计算重复就用row_number ，统计重复就dense_rank，子查询 &lt;=N 即可。</span></code></pre></div><h3 id="求某个分组里面排名第N个"><a href="#求某个分组里面排名第N个" class="headerlink" title="求某个分组里面排名第N个"></a>求某个分组里面排名第N个</h3><p><a href="https://www.nowcoder.com/practice/4a052e3e1df5435880d4353eb18a91c6">SQL12 获取每个部门中当前员工薪水最高的相关信息</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 1.窗口函数求出每个成员在分组里面的排名</span><span class="hljs-comment">-- 2.子查询让排名等于N</span><span class="hljs-comment">-- 3.可以做个非空校验, 如果没有符合的的rank, 就返回指定值.</span><span class="hljs-keyword">select</span> dept_no,emp_no,salary <span class="hljs-keyword">as</span> maxSalary<span class="hljs-keyword">from</span>(<span class="hljs-keyword">select</span> d.dept_no,d.emp_no,s.salary,<span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> d.dept_no <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> s.salary <span class="hljs-keyword">desc</span>)  <span class="hljs-keyword">as</span> r1<span class="hljs-keyword">from</span> dept_emp d, salaries s<span class="hljs-keyword">where</span> d.emp_no <span class="hljs-operator">=</span>s.emp_no) ss<span class="hljs-keyword">where</span> r1<span class="hljs-operator">=</span><span class="hljs-number">1</span><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> dept_no;</code></pre></div><h2 id="用户登陆相关"><a href="#用户登陆相关" class="headerlink" title="用户登陆相关"></a>用户登陆相关</h2><h3 id="最近登陆日期"><a href="#最近登陆日期" class="headerlink" title="最近登陆日期"></a>最近登陆日期</h3><p><a href="https://www.nowcoder.com/practice/7cc3c814329546e89e71bb45c805c9ad">SQL67 牛客每个人最近的登录日期(二）</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--基本思路就是求出最近登陆的日期和 user_id 作为字典表，里面对应的user_id 就是最近登陆的用户，查询即可。</span><span class="hljs-keyword">select</span> u.name,c.name,l.date<span class="hljs-keyword">from</span> login l, <span class="hljs-keyword">user</span> u , client c<span class="hljs-keyword">where</span> l.user_id<span class="hljs-operator">=</span>u.id <span class="hljs-keyword">and</span> l.client_id<span class="hljs-operator">=</span>c.id<span class="hljs-keyword">and</span> (l.user_id,l.date) <span class="hljs-keyword">in</span>(<span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">max</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> u.name;</code></pre></div><h3 id="次日留存率"><a href="#次日留存率" class="headerlink" title="次日留存率"></a>次日留存率</h3><p><a href="https://www.nowcoder.com/practice/16d41af206cd4066a06a3a0aa585ad3d">SQL68 牛客每个人最近的登录日期(三)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 1.筛选出初次登陆的新用户</span><span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>)<span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id;<span class="hljs-comment">--2.新用户登陆后, 第二天登陆的情况</span><span class="hljs-keyword">select</span> round(<span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id) <span class="hljs-operator">/</span> (<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> user_id) <span class="hljs-keyword">from</span> login) ,<span class="hljs-number">3</span>) <span class="hljs-keyword">as</span> p<span class="hljs-keyword">from</span> login<span class="hljs-keyword">where</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,date_add(<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>),<span class="hljs-type">INTERVAL</span> <span class="hljs-number">1</span> <span class="hljs-keyword">day</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)</code></pre></div><h3 id="每日新增用户数"><a href="#每日新增用户数" class="headerlink" title="每日新增用户数"></a>每日新增用户数</h3><p><a href="https://www.nowcoder.com/practice/e524dc7450234395aa21c75303a42b0a">SQL69 牛客每个人最近的登录日期(四)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--1.求出新用户登陆日期</span><span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">as</span> <span class="hljs-type">date</span> <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id;<span class="hljs-comment">--2.通过case when 判断, 如果当天登陆用户中和新用户登陆表相同则记为1,否则是0.</span><span class="hljs-keyword">select</span> tmp.date,<span class="hljs-built_in">sum</span>(tmp.t) <span class="hljs-keyword">as</span> <span class="hljs-keyword">new</span><span class="hljs-keyword">from</span>(<span class="hljs-keyword">select</span> <span class="hljs-type">date</span>,<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id,<span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">as</span> <span class="hljs-type">date</span> <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id) <span class="hljs-keyword">then</span> <span class="hljs-number">1</span><span class="hljs-keyword">else</span> <span class="hljs-number">0</span><span class="hljs-keyword">end</span> <span class="hljs-keyword">as</span> t<span class="hljs-keyword">from</span> login) tmp<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>;</code></pre></div><h3 id="新用户次日留存率"><a href="#新用户次日留存率" class="headerlink" title="新用户次日留存率"></a>新用户次日留存率</h3><p><a href="https://www.nowcoder.com/practice/ea0c56cd700344b590182aad03cc61b8">SQL70 牛客每个人最近的登录日期(五)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--分子:当前日期+ 1day 中是前一天新用户的用户</span><span class="hljs-comment">--分母 当前日期中是新用户, 即当前日期是该用户所有登陆日期中最小的</span><span class="hljs-comment">--当前日期-1day =date   等于 当前日期= date + 1day</span><span class="hljs-keyword">select</span> <span class="hljs-type">date</span>,ifnull(round (<span class="hljs-built_in">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id, <span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)  <span class="hljs-keyword">and</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id, date_add(<span class="hljs-type">date</span>,<span class="hljs-type">interval</span> <span class="hljs-number">-1</span> <span class="hljs-keyword">day</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id) <span class="hljs-keyword">then</span> <span class="hljs-number">1</span><span class="hljs-keyword">else</span> <span class="hljs-number">0</span><span class="hljs-keyword">end</span>)  <span class="hljs-operator">/</span><span class="hljs-built_in">sum</span>(<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> (user_id,<span class="hljs-type">date</span>) <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> user_id, <span class="hljs-built_in">min</span>(<span class="hljs-type">date</span>) <span class="hljs-keyword">from</span> login <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id)<span class="hljs-keyword">then</span> <span class="hljs-number">1</span><span class="hljs-keyword">else</span> <span class="hljs-number">0</span><span class="hljs-keyword">end</span>),  <span class="hljs-number">3</span>),<span class="hljs-number">0</span>)<span class="hljs-keyword">as</span> p<span class="hljs-keyword">from</span> login<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>;</code></pre></div><h3 id="用户登陆并统计核心行为统计"><a href="#用户登陆并统计核心行为统计" class="headerlink" title="用户登陆并统计核心行为统计"></a>用户登陆并统计核心行为统计</h3><p><a href="https://www.nowcoder.com/practice/572a027e52804c058e1f8b0c5e8a65b4">SQL71 牛客每个人最近的登录日期(六)</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 1.先求出每个用户登陆时候的刷题信息和姓名</span><span class="hljs-comment">-- 2.根据日期排序, 再根据姓名排序</span><span class="hljs-comment">-- 3.题目是求到某一天累计刷题多少, 我们可以通过子查询筛选符合的</span><span class="hljs-keyword">select</span> p1.user_id,p1.date,<span class="hljs-built_in">sum</span>(p1.number) <span class="hljs-keyword">as</span> number<span class="hljs-keyword">from</span> passing_number p1,passing_number p2<span class="hljs-keyword">where</span>  p1.user_id<span class="hljs-operator">=</span>p2.user_id <span class="hljs-keyword">and</span> p1.date<span class="hljs-operator">&gt;=</span>p2.date<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> p1.user_id,p1.date;<span class="hljs-comment">-- 最终SQL</span><span class="hljs-keyword">select</span>u.name,p.date,p.number<span class="hljs-keyword">from</span> login l,(<span class="hljs-keyword">select</span> p1.user_id,p1.date,<span class="hljs-built_in">sum</span>(p2.number) <span class="hljs-keyword">as</span> number<span class="hljs-keyword">from</span> passing_number p1 <span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> passing_number p2<span class="hljs-keyword">on</span>  p1.user_id<span class="hljs-operator">=</span>p2.user_id <span class="hljs-keyword">and</span> p1.date<span class="hljs-operator">&gt;=</span>p2.date<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> p1.user_id,p1.date) p ,<span class="hljs-keyword">user</span> u <span class="hljs-keyword">where</span> l.user_id<span class="hljs-operator">=</span>p.user_id <span class="hljs-keyword">and</span> l.date<span class="hljs-operator">=</span>p.date <span class="hljs-keyword">and</span> l.user_id<span class="hljs-operator">=</span>u.id<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> p.date,u.name;</code></pre></div><h3 id="求连续登陆N天的用户"><a href="#求连续登陆N天的用户" class="headerlink" title="求连续登陆N天的用户"></a>求连续登陆N天的用户</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--求连续三天登录的人数的方法</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">or</span> replace temporary <span class="hljs-keyword">view</span> mytab <span class="hljs-keyword">as</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-01&#x27;</span> logintime <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-02&#x27;</span> logintime <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-03&#x27;</span> logintime <span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;zs&#x27;</span> name, <span class="hljs-string">&#x27;2021-08-04&#x27;</span> logintime;<span class="hljs-comment">--思路一，将having count(1)&gt;=n，就是求连续n天登录的人数</span><span class="hljs-comment">--连续三天登陆等于登陆日期-登陆的次数排序=第一天登陆</span><span class="hljs-keyword">with</span> t1 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> name,logintime <span class="hljs-keyword">from</span> mytab),     t2 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,                    date_sub(logintime,<span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> name <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> logintime)) <span class="hljs-keyword">as</span> temp_date             <span class="hljs-keyword">from</span> t1 )<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> name) cnt<span class="hljs-keyword">from</span> t2<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> name, temp_date<span class="hljs-keyword">having</span> <span class="hljs-built_in">count</span>(<span class="hljs-number">1</span>)<span class="hljs-operator">&gt;=</span><span class="hljs-number">3</span>;<span class="hljs-comment">--思路二，将下面的date_add(logintime,n-1)，lead(logintime,n-1) ，就是求连续n天登录的人数</span><span class="hljs-keyword">with</span> t1 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> name,logintime <span class="hljs-keyword">from</span> mytab),     t2 <span class="hljs-keyword">as</span> ( <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>,                    date_add(logintime,<span class="hljs-number">2</span>) <span class="hljs-keyword">as</span> expectdate,                    <span class="hljs-built_in">lead</span>(logintime,<span class="hljs-number">2</span>) <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> name <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> logintime) realdate             <span class="hljs-keyword">from</span> t1 )<span class="hljs-keyword">select</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> name) cnt  <span class="hljs-keyword">from</span> t2 <span class="hljs-keyword">where</span> expectdate<span class="hljs-operator">=</span>realdate;</code></pre></div><h2 id="中位数相关"><a href="#中位数相关" class="headerlink" title="中位数相关"></a>中位数相关</h2><h3 id="求中位数"><a href="#求中位数" class="headerlink" title="求中位数"></a>求中位数</h3><p><a href="https://www.nowcoder.com/practice/502fb6e2b1ad4e56aa2e0dd90c6edf3c">考试分数（四）</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--需求</span><span class="hljs-comment">--请你写一个sql语句查询各个岗位分数升序排列之后的中位数位置的范围，并且按job升序排序</span><span class="hljs-comment">--补充知识</span><span class="hljs-comment">--中位数的特征：</span><span class="hljs-comment">--当个数为偶数时，中位数的起始位置等于个数/2，结束位置等于个数/2+1</span><span class="hljs-comment">--当个数为奇数时，中位数的起始位置等于向上取整（个数/2），结束位置等于向上取整（个数/2）</span><span class="hljs-comment">--用除以2的余数是否为0来判断奇偶，%2=0</span><span class="hljs-comment">--记得取整数，本题用ceiling函数向上取整（返回不小于该数的最小整数值）或round(数，0)四舍五入取整都可以</span><span class="hljs-comment">--我的写法</span><span class="hljs-keyword">SELECT</span>job,<span class="hljs-built_in">ceil</span>(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span>) <span class="hljs-keyword">as</span> <span class="hljs-keyword">start</span>,<span class="hljs-keyword">case</span> <span class="hljs-keyword">when</span> <span class="hljs-built_in">count</span>(id)<span class="hljs-operator">%</span><span class="hljs-number">2</span><span class="hljs-operator">=</span><span class="hljs-number">0</span><span class="hljs-keyword">then</span> <span class="hljs-built_in">ceil</span>(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span><span class="hljs-operator">+</span><span class="hljs-number">1</span>)<span class="hljs-keyword">else</span> <span class="hljs-built_in">ceil</span>(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span>)<span class="hljs-keyword">end</span> <span class="hljs-keyword">as</span> <span class="hljs-keyword">end</span><span class="hljs-keyword">from</span> grade<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> job<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> job;# 大神简化版 ,后面的<span class="hljs-operator">+</span><span class="hljs-number">1</span>再除以<span class="hljs-number">2</span>, <span class="hljs-keyword">select</span> job, round(<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">/</span><span class="hljs-number">2</span>), round((<span class="hljs-built_in">count</span>(id)<span class="hljs-operator">+</span><span class="hljs-number">1</span>)<span class="hljs-operator">/</span><span class="hljs-number">2</span>)<span class="hljs-keyword">from</span> grade <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> job<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> job;</code></pre></div><h3 id="求中位数以上的情况"><a href="#求中位数以上的情况" class="headerlink" title="求中位数以上的情况"></a>求中位数以上的情况</h3><p><a href="https://www.nowcoder.com/practice/b626ff9e2ad04789954c2132c74c0512">考试分数（五）</a></p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">--需求</span><span class="hljs-comment">--请你写一个sql语句查询各个岗位分数的中位数位置上的所有grade信息，并且按id升序排序</span><span class="hljs-comment">--1.先求出中位数范围</span><span class="hljs-comment">--2.再通过开窗函数求出每个同学分数的自然排名</span><span class="hljs-comment">--3.按照id排序</span><span class="hljs-keyword">SELECT</span>tmp.id,tmp.job,tmp.score,tmp.t_rank<span class="hljs-keyword">from</span>(<span class="hljs-keyword">SELECT</span> id,job,score,  <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> score <span class="hljs-keyword">desc</span>) t_rank,  <span class="hljs-built_in">count</span>(score) <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> job) <span class="hljs-keyword">as</span> cnt  <span class="hljs-keyword">from</span> grade) <span class="hljs-keyword">as</span> tmp  <span class="hljs-keyword">where</span> tmp.t_rank<span class="hljs-operator">=</span><span class="hljs-built_in">floor</span>((tmp.cnt<span class="hljs-operator">+</span><span class="hljs-number">1</span>)<span class="hljs-operator">/</span><span class="hljs-number">2</span>) <span class="hljs-keyword">or</span> tmp.t_rank<span class="hljs-operator">=</span><span class="hljs-built_in">floor</span>((tmp.cnt<span class="hljs-operator">+</span><span class="hljs-number">2</span>)<span class="hljs-operator">/</span><span class="hljs-number">2</span>)  <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> tmp.id;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>面试准备</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis常见面试题</title>
    <link href="/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <url>/2021/01/07/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>学习和整理 Redis 相关的知识当中，这里汇总了一下经常被问到的 Redis 面试题</p><p>Redis 的八股无外乎这三个：缓存穿透、缓存击穿、缓存雪崩。</p><h1 id="分片集群问题"><a href="#分片集群问题" class="headerlink" title="分片集群问题"></a>分片集群问题</h1><h3 id="1-Redis的多数据机制了解多少"><a href="#1-Redis的多数据机制了解多少" class="headerlink" title="1.Redis的多数据机制了解多少"></a>1.Redis的多数据机制了解多少</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>Redis支持多个数据库,单机模式下有从db0到db15, 数据库之间不能共享<span class="hljs-number">2.</span>分片集群中只有一个数据库空间,不会使用到Redis的多数据库</code></pre></div><h3 id="2-懂Redis的批量操作吗"><a href="#2-懂Redis的批量操作吗" class="headerlink" title="2.懂Redis的批量操作吗"></a>2.懂Redis的批量操作吗</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>有mget,mset,hmget,hmset,hgetall,hvals,<span class="hljs-number">2.</span>分片集群中, 不同的key会分到不同的slot中, 不能直接使用mget,mset如何解决: 加上相同的前缀,用大括号&#123;&#125;包裹</code></pre></div><h3 id="3-Redis的集群机制中-你觉得有什么不足的地方吗"><a href="#3-Redis的集群机制中-你觉得有什么不足的地方吗" class="headerlink" title="3.Redis的集群机制中, 你觉得有什么不足的地方吗?"></a>3.Redis的集群机制中, 你觉得有什么不足的地方吗?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>如果<span class="hljs-keyword">value</span>是hash类型, 对象非常大, 即对应属性非常多, 也只能存入一个集群的节点中<span class="hljs-number">2.</span>批量操作也很麻烦, 属性太多写得很长</code></pre></div><h3 id="4-在Redis集群模式下-如何进行批量操作"><a href="#4-在Redis集群模式下-如何进行批量操作" class="headerlink" title="4.在Redis集群模式下, 如何进行批量操作?"></a>4.在Redis集群模式下, 如何进行批量操作?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>如何执行的key数量很少, 串行<span class="hljs-keyword">get</span>操作<span class="hljs-number">2.</span>如果一定要批量操作, 加上相同的前缀,前缀用&#123;&#125;包裹</code></pre></div><h3 id="4-5-什么是Redis的事务"><a href="#4-5-什么是Redis的事务" class="headerlink" title="4.5 什么是Redis的事务"></a>4.5 什么是Redis的事务</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>Redis事务是一些命令的集合, 执行的时候是串行执行<span class="hljs-number">2.</span>分片集群中,不同的key可能会被分到不同的slot中, Redis的事务不生效<span class="hljs-number">3.</span>Redis的事务不支持回滚操作, 基本用不上.</code></pre></div><h1 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h1><h3 id="5-什么是缓存穿透-如何解决"><a href="#5-什么是缓存穿透-如何解决" class="headerlink" title="5.什么是缓存穿透,如何解决"></a>5.什么是缓存穿透,如何解决</h3><div class="code-wrapper"><pre><code class="hljs sql">现象：客户端高并发不断向Redis请求一个不存在的Key，MySQL中也没有由于Redis没有，导致这个并发全部落在MySQL上解决<span class="hljs-number">1.</span>对于那些每秒访问频次过高的IP进行限制，拒绝访问<span class="hljs-number">2.</span>如果第一次redis中没有，读MYSQL，MySQL也没有，在Redis中设置一个<span class="hljs-operator">=</span><span class="hljs-operator">=</span>临时<span class="hljs-operator">=</span><span class="hljs-operator">=</span>默认值<span class="hljs-number">3.</span>利用BitMap类型构建布隆过滤器<span class="hljs-operator">*</span><span class="hljs-operator">*</span>(只保证MySQL数据库没有这个数据, 不保证一定有)<span class="hljs-operator">*</span><span class="hljs-operator">*</span></code></pre></div><h3 id="6-什么是缓存击穿-如何解决"><a href="#6-什么是缓存击穿-如何解决" class="headerlink" title="6.什么是缓存击穿,如何解决"></a>6.什么是缓存击穿,如何解决</h3><div class="code-wrapper"><pre><code class="hljs sql">现象：有一个Key，经常需要高并发的访问，这个Key有过期时间的，一旦达到过期时间<span class="hljs-operator">=</span><span class="hljs-operator">=</span>，这个Key被删除，所有高并发落到了MySQL中，被击穿了解决<span class="hljs-number">1.</span>资源充足的情况下，设置永不过期<span class="hljs-number">2.</span>对这个Key做一个互斥锁，只允许一个请求去读取，其他的所有请求先阻塞掉第一个请求redis中没有读取到，读了MySQL，再将这个数据放到Redis中释放所有阻塞的请求</code></pre></div><h3 id="7-什么是缓存雪崩-如何解决"><a href="#7-什么是缓存雪崩-如何解决" class="headerlink" title="7.什么是缓存雪崩 ,如何解决"></a>7.什么是缓存雪崩 ,如何解决</h3><div class="code-wrapper"><pre><code class="hljs sql">现象：大量的Key在同一个时间段过期，大量的Key的请求在Redis中都没有，都去请求MySQL，导致MySQL奔溃解决<span class="hljs-number">1.</span>资源充足允许的情况下，设置大部分的Key不过期<span class="hljs-number">2.</span>给所有Key设置过期时间时加上随机值，让Key不再同一时间过期</code></pre></div><h3 id="8-Redis中的Key如何设计"><a href="#8-Redis中的Key如何设计" class="headerlink" title="8.Redis中的Key如何设计?"></a>8.Redis中的Key如何设计?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>使用统一的命名规范<span class="hljs-number">2.</span>一般使用业务名(或数据库名)为前缀，用冒号分隔，例如，业务名:表名:id。例如：shop:usr:msg_code（电商:用户:验证码）<span class="hljs-number">4.</span>控制key名称的长度，不要使用过长的key<span class="hljs-number">5.</span>在保证语义清晰的情况下，尽量减少Key的长度。有些常用单词可使用缩写，例如，<span class="hljs-keyword">user</span>缩写为u，messages缩写为msg<span class="hljs-number">6.</span>名称中不要包含特殊字符、包含空格、单双引号以及其他转义字符</code></pre></div><h3 id="9-为什么Redis是单线程的"><a href="#9-为什么Redis是单线程的" class="headerlink" title="9.为什么Redis是单线程的?"></a>9.为什么Redis是单线程的?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>因为Redis是基于内存的操作，CPU不是Redis的瓶颈<span class="hljs-number">2.</span>Redis的瓶颈最有可能是机器内存的大小或者网络带宽<span class="hljs-number">3.</span>单线程容易实现，而且CPU不会成为瓶颈，所以没必要使用多线程增加复杂度<span class="hljs-number">4.</span>可以使用多Redis压榨CPU，提高性能</code></pre></div><h3 id="10-为什么Redis的性能很高"><a href="#10-为什么Redis的性能很高" class="headerlink" title="10.为什么Redis的性能很高?"></a>10.为什么Redis的性能很高?</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>基于内存操作, 快<span class="hljs-number">2.</span>用C语言编写, 数据结构简单, 对数据操作也简单<span class="hljs-number">3.</span>采用单线程, 避免不必要的线程切换和资源抢占<span class="hljs-number">4.</span>IO多路复用模型, 非阻塞IO</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>面试准备</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WordCount案例汇总</title>
    <link href="/2020/11/06/WordCount%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB/"/>
    <url>/2020/11/06/WordCount%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h2><p>整理一下曾经学习技术栈练习过的 WordCount 案例，总之很多计算引擎的样例都是 WordCount</p><p>经典永不过时，使用的很多函数和方法也是常用的。</p><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<span class="hljs-keyword">import</span> java.io.IOException;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Map规则, 用来实现把: k1, v1 -&gt; k2, v2, 需要 继承Mapper类, 重写map方法.</span><span class="hljs-comment"> * 各个数据解释:</span><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountMapTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;    <span class="hljs-comment">/**</span><span class="hljs-comment">     * 重写map方法,用来将K1 V2 转换成 K2 V2</span><span class="hljs-comment">     *</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k1</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> value   v1</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写出K2,V2</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><span class="hljs-comment">     */</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;        <span class="hljs-comment">//1.获取行偏移量,没有什么用处,我们用于测试看看的</span>        <span class="hljs-keyword">long</span> index = key.get();        System.out.println(<span class="hljs-string">&quot;行偏移量是: &quot;</span> + index);        <span class="hljs-comment">//2.获取整行数据</span>        String line = value.toString();        <span class="hljs-comment">//3.读取并做非空校验,判断值是否相等,也判断地址值是否相等</span>        <span class="hljs-keyword">if</span> (line != <span class="hljs-keyword">null</span> &amp;&amp; !<span class="hljs-string">&quot;&quot;</span>.equals(line)) &#123;            <span class="hljs-comment">//4.切割获取K2,V2</span>            String[] str = line.split(<span class="hljs-string">&quot; &quot;</span>);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; str.length; i++) &#123;                String s = str[i];                context.write(<span class="hljs-keyword">new</span> Text(s), <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>));            &#125;        &#125;    &#125;&#125;</code></pre></div><h3 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<span class="hljs-keyword">import</span> java.io.IOException;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 自定义的Reduce规则, 用来实现把: k2, v2的集合 -&gt; k3, v3, 需要 继承Reducer类, 重写reduce方法.</span><span class="hljs-comment"> * 各个数据解释:</span><span class="hljs-comment"> * k1: 行偏移量, 即:从哪里开始读取数据,默认从0开始.</span><span class="hljs-comment"> * v1: 整行数据, 这里是: &quot;hello hello&quot;, &quot;world world&quot;, &quot;hadoop hadoop&quot;....</span><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v2: 每个单词的次数, 例如: 1, 1, 1, 1, 1....</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * shuffle阶段: 分区, 排序, 规约, 分组之后, 数据如下:</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * k2: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v2(的集合): 每个单词的所有次数的集合, 例如: &#123;1, 1&#125;,  &#123;1, 1, 1&#125;, &#123;1, 1&#125;</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * k3: 单个的单词, 例如: &quot;hello&quot;, &quot;world&quot;, &quot;hadoop&quot;</span><span class="hljs-comment"> * v3: 每个单词的总次数, 例如: 2, 3, 2</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountReduceTask</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; </span>&#123;    <span class="hljs-comment">//重写reduce方法</span>    <span class="hljs-comment">/**</span><span class="hljs-comment">     * 重写reduce方法,用于把k2,v2 转换成k3,v3</span><span class="hljs-comment">     *</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key     k2</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> values  v2的集合(已经经过了分组)</span><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 内容对象,用来写k3,v3</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><span class="hljs-comment">     */</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>&#123;        <span class="hljs-comment">//1.获取k3,就是每个单词</span>        String word = key.toString();        <span class="hljs-comment">//2.获取v3,就是单词出现的次数</span>        <span class="hljs-comment">//2.1先对v2集合求和</span>        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;        <span class="hljs-keyword">for</span> (IntWritable value : values) &#123;            count += value.get();        &#125;        <span class="hljs-comment">//2.2写出v3</span>        <span class="hljs-comment">//context.write(new Text(word),new IntWritable(count));</span>        <span class="hljs-comment">//因为v2和v3是一样的,我们可以优化一下</span>        context.write(key, <span class="hljs-keyword">new</span> IntWritable(count));    &#125;&#125;</code></pre></div><h3 id="WordCountMain简写版"><a href="#WordCountMain简写版" class="headerlink" title="WordCountMain简写版"></a>WordCountMain简写版</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<span class="hljs-keyword">import</span> java.io.IOException;<span class="hljs-comment">/**</span><span class="hljs-comment"> * 这里写的是驱动类, 即: 封装MR程序的核心8步的. 它有两种写法:</span><span class="hljs-comment"> * 1. 官方示例版, 即: 完整版.   理解即可, 因为稍显复杂, 用的人较少.</span><span class="hljs-comment"> * 2. 简化版.  推荐掌握.</span><span class="hljs-comment"> * 这里是简化版写法</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);        <span class="hljs-comment">//2.封装MR程序核心8步</span>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span>        job.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/input/wordcount.txt&quot;</span>));        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span>        job.setMapperClass(WordCountMapTask.class);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.3 分区,用默认的</span>        <span class="hljs-comment">//2.4 排序,用默认的</span>        <span class="hljs-comment">//2.5 规约,用默认的</span>        <span class="hljs-comment">//2.6 分组,用默认的</span>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span>        job.setReducerClass(WorkCountReduceTask.class);        job.setOutputValueClass(Text.class);        job.setOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span>        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;file:///d:/test/wordcount/output&quot;</span>));        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);    &#125;&#125;</code></pre></div><h3 id="WordCountMain-jar包版"><a href="#WordCountMain-jar包版" class="headerlink" title="WordCountMain jar包版"></a>WordCountMain jar包版</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<span class="hljs-comment">/**</span><span class="hljs-comment"> * 这个代码一会儿是要打包成jar包, 然后放到Yarn集群中运行的, 需要做如下的几件事儿:</span><span class="hljs-comment"> * 1. 在驱动类中设置 jar包的启动类.</span><span class="hljs-comment"> * job.setJarByClass(WordCountMain3.class);</span><span class="hljs-comment"> * 2. 修改数据源文件 和 目的地文件的路径, 改为: 外部传入.</span><span class="hljs-comment"> * TextInputFormat.addInputPath(job, new Path(args[0]));</span><span class="hljs-comment"> * TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><span class="hljs-comment"> * 3. 对我们当前的工程进行打包动作, 打包成: 胖jar, 具体操作为: 取消pom.xml文件中最后一个插件的注释, 然后打包即可.</span><span class="hljs-comment"> * 细节: 修改jar包名字为: wordcount.jar, 方便我们操作.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 4. 在HDFS集群中创建:   /wordcount/input/ 目录</span><span class="hljs-comment"> * 5. 把wordcount.txt 上传到该目录下.</span><span class="hljs-comment"> * 6. 把之前打好的 jar包也上传到 Linux系统中.</span><span class="hljs-comment"> * 7. 运行该jar包即可, 记得: 传入 数据源文件路径, 目的地目录路径.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 名词解释:</span><span class="hljs-comment"> * 胖jar: 指的是一个jar包中还包含有其他的jar包, 这样的jar包就称之为: 胖jar.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 问题1: 为什么需要打包成 胖jar?</span><span class="hljs-comment"> * 答案:</span><span class="hljs-comment"> * 因为目前我们的工程需要依赖 Hadoop环境, 而我们已经在pom.xml文件中配置了,</span><span class="hljs-comment"> * 如果运行的环境中(例如: Linux系统等)没有hadoop环境, 并且我们打包时也没有把hadoop环境打包进去,</span><span class="hljs-comment"> * 将来运行jar包的时候就会出错.</span><span class="hljs-comment"> * &lt;p&gt;</span><span class="hljs-comment"> * 问题2: 当前工程一定要打包成 胖jar吗?</span><span class="hljs-comment"> * 答案: 不用, 因为我们的 jar包一会儿是放到 Yarn集群中运行的, 它已经自带Hadoop环境, 所以这里可以不打包 胖jar, 只打包我们自己的代码.</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WorkCountMain3</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//1.创建Job任务,指定任务名 一个Job任务 = 一个MR程序</span>        Job job = Job.getInstance(<span class="hljs-keyword">new</span> Configuration(), <span class="hljs-string">&quot;wordcountMR&quot;</span>);        <span class="hljs-comment">//细节1: 在驱动类中设置 jar包的启动类.</span>        job.setJarByClass(WorkCountMain3.class);        <span class="hljs-comment">//2.封装MR程序核心8步</span>        <span class="hljs-comment">//2.1 封装输入组件,读取(数据源)中的数据,获取k1,v1</span>        job.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">0</span>]));        <span class="hljs-comment">//2.2 封装自定义的Maptask任务,把k1,v1 --&gt; k2,v2</span>        job.setMapperClass(WordCountMapTask.class);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.3 分区,用默认的</span>        <span class="hljs-comment">//2.4 排序,用默认的</span>        <span class="hljs-comment">//2.5 规约,用默认的</span>        <span class="hljs-comment">//2.6 分组,用默认的</span>        <span class="hljs-comment">//2.7 封装自定义的Reducetask任务,把k2,v2 --&gt; k3,v3</span>        job.setReducerClass(WorkCountReduceTask.class);        job.setOutputValueClass(Text.class);        job.setOutputValueClass(IntWritable.class);        <span class="hljs-comment">//2.8 封装输出组件,关联目的地文件,写入获取的k3,v3. 牢记必须有父目录,不能有子目录.</span>        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">1</span>]));        <span class="hljs-comment">//3.提交Job任务,等待任务执行完成反馈的状态, true等待结果  false只提交,不等待接收结果</span>        <span class="hljs-keyword">boolean</span> flag = job.waitForCompletion(<span class="hljs-keyword">true</span>);        <span class="hljs-comment">//4.退出当前进行的JVM程序 0正常退出, 非0异常退出</span>        System.exit(flag ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);    &#125;&#125;</code></pre></div><h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>基本流程</p><div class="code-wrapper"><pre><code class="hljs java">基本流程<span class="hljs-number">1.</span>传输文件路径到自定义的Actor类,并接收返回值,解析返回值得出统计结果<span class="hljs-number">2.</span>自定义Actor类, 接收文件路径并做解析统计单词再返回给发送者需要分别定义<span class="hljs-number">3</span>个类Main入口<span class="hljs-number">1.</span>用于发送文件路径,封装在自定义的单例类里面<span class="hljs-number">2.</span>接收返回值,并做判断是否完成传输, 如果完成就开始解析<span class="hljs-number">3.</span>通过apply方法解析结果,合并结果得出最后结果自定义的Actor类<span class="hljs-number">1.</span>接收文件路径信息,做分析统计<span class="hljs-number">2.</span>把结果封装在单例类中,返回给发送者自定义的单例类<span class="hljs-number">1.</span>用于封装发送信息的单例类<span class="hljs-number">2.</span>用于返回统计的单例类</code></pre></div><h3 id="MainActor"><a href="#MainActor" class="headerlink" title="MainActor"></a>MainActor</h3><div class="code-wrapper"><pre><code class="hljs java">`<span class="hljs-keyword">package</span> com.test.day04.wordcount<span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<span class="hljs-keyword">import</span> java.io.File<span class="hljs-keyword">import</span> scala.actors.Future<span class="hljs-comment">/**</span><span class="hljs-comment"> * 1.发送文件名给WordCountActor</span><span class="hljs-comment"> * 2.接收WordCountActor返回结果并合并</span><span class="hljs-comment"> */</span>object MainActor &#123;  <span class="hljs-comment">//发送文件名给WordCountActor</span>  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;    <span class="hljs-comment">//1.获取文件名</span>    val fileDir = <span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;./data&quot;</span>)    val files: Array[File] = fileDir.listFiles()    <span class="hljs-comment">// 测试是成功获取文件名</span>    <span class="hljs-comment">// files.foreach(println(_))</span>    <span class="hljs-comment">//2.发送给wordcountactor</span>    val future_Array: Array[Future[Any]] = files.map(f = file =&gt; &#123;      val name = file.toString      <span class="hljs-comment">//每一个文件名新建对应的线程</span>      val actor = <span class="hljs-keyword">new</span> WordCountActor      <span class="hljs-comment">//开启线程并发送给我认定任务</span>      actor.start()      <span class="hljs-comment">//发送的消息封装在这里面并获取结果</span>      val future: Future[Any] = actor !! WordCountTask(name)      future    &#125;)    <span class="hljs-comment">//接收WordCountActor返回结果并合并</span>    <span class="hljs-comment">//先判断是否全部文件都处理完毕都有结果,是再处理</span>    <span class="hljs-keyword">while</span> (!(future_Array.filter((x) =&gt; &#123;      !x.isSet    &#125;)).isEmpty) &#123;&#125;    <span class="hljs-comment">//走到这里, 证明我们可以处理,使用apply获取数据</span>    <span class="hljs-comment">//里面的键值对就是多个文件统计结果, 我们还需要合并去重</span>    val wordCount: Array[Map[String, Int]] = future_Array.map((x) =&gt; &#123;      val results: Any = x.apply()      val result = results.asInstanceOf[WordCountResult]      val map: Map[String, Int] = result.map      map    &#125;)    <span class="hljs-comment">//wordCount.foreach(println(_))</span>    <span class="hljs-comment">//测试结果</span>    <span class="hljs-comment">// Map(e -&gt; 2, f -&gt; 1, a -&gt; 1, b -&gt; 1, c -&gt; 1)</span>    <span class="hljs-comment">// Map(e -&gt; 1, a -&gt; 2, b -&gt; 1, c -&gt; 2, d -&gt; 3)</span>    <span class="hljs-comment">//合并结果, 先合并成一个Array</span>    val flatten: Array[(String, Int)] = wordCount.flatten    <span class="hljs-comment">//根据Map的key值分组</span>    val wordGroup: Map[String, Array[(String, Int)]] = flatten.groupBy((x) =&gt; &#123;      x._1    &#125;)    val finalResult: Map[String, Int] = wordGroup.map((x) =&gt; &#123;      val name = x._1      val size = x._2.size      name -&gt; size    &#125;)    finalResult.foreach(println(_))  &#125;&#125;</code></pre></div><h3 id="WordCountActor"><a href="#WordCountActor" class="headerlink" title="WordCountActor"></a>WordCountActor</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<span class="hljs-keyword">import</span> com.test.day04.wordcount.WordCountPackage.&#123;WordCountResult, WordCountTask&#125;<span class="hljs-keyword">import</span> scala.actors.Actor<span class="hljs-keyword">import</span> scala.io.Source<span class="hljs-comment">/**</span><span class="hljs-comment"> * 1.接收MainActor的文件名称并进行单词统计</span><span class="hljs-comment"> * 2.将单词统计结果返回给MainActor</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountActor</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Actor</span> </span>&#123;  <span class="hljs-function">override def <span class="hljs-title">act</span><span class="hljs-params">()</span>: Unit </span>= &#123;    <span class="hljs-comment">//接收消息</span>    loop &#123;      react &#123;        <span class="hljs-function"><span class="hljs-keyword">case</span> <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename)</span> </span>=&gt;          println(<span class="hljs-string">&quot;收到了文件名: &quot;</span> + filename)          <span class="hljs-comment">//解析消息, 通过Source解析消息, 定义文件来源再转化成列表</span>          <span class="hljs-comment">//一个元素就是一个一行数据</span>          val words: List[String] = Source.fromFile(filename).getLines().toList          <span class="hljs-comment">//切割获取每一条数据并合并成一个list集合</span>          val word_List: List[String] = words.flatMap((x) =&gt; &#123;            x.split(<span class="hljs-string">&quot; &quot;</span>)          &#125;)          <span class="hljs-comment">//按照单词进行分组, 然后聚合统计</span>          val word_Tuples: List[(String, Int)] = word_List.map((x) =&gt; &#123;            (x, <span class="hljs-number">1</span>)          &#125;)          val word_Map: Map[String, List[(String, Int)]] = word_Tuples.groupBy((x) =&gt; &#123;            x._1          &#125;)          val wordCountMap: Map[String, Int] = word_Map.map((x) =&gt; &#123;            val name: String = x._1            val size: Int = x._2.size            name -&gt; size          &#125;)          <span class="hljs-comment">//把统计结果反馈给Mainactor,装进WordCount</span>          sender ! WordCountResult(wordCountMap)      &#125;    &#125;  &#125;&#125;</code></pre></div><h3 id="WordCountPackage"><a href="#WordCountPackage" class="headerlink" title="WordCountPackage"></a>WordCountPackage</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.day04.wordcount<span class="hljs-comment">/**</span><span class="hljs-comment"> * 1.定义一个样例类, 描述单词统计信息</span><span class="hljs-comment"> * 2.定义一个样例类封装单词统计结果</span><span class="hljs-comment"> */</span>object WordCountPackage &#123;  <span class="hljs-comment">//1.定义一个样例类, 描述单词统计信息</span>  <span class="hljs-function"><span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountTask</span><span class="hljs-params">(filename: String)</span></span><span class="hljs-function"></span><span class="hljs-function">  <span class="hljs-comment">//2.定义一个样例类封装单词统计结果</span></span><span class="hljs-function">  <span class="hljs-keyword">case</span> class <span class="hljs-title">WordCountResult</span><span class="hljs-params">(map: Map[String, Int])</span></span><span class="hljs-function"></span><span class="hljs-function">&#125;</span></code></pre></div><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h3><ul><li>基本流程</li></ul><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-number">1.</span>创建上下文对象<span class="hljs-number">2.</span>读取文件<span class="hljs-number">3.</span>flatMap获取到每个单词<span class="hljs-number">4.</span>map将RDD变成 key-value结构<span class="hljs-number">5.</span>reduceByKey 求和统计<span class="hljs-number">6.</span>打印输出<span class="hljs-number">7.</span>关闭上下文对象</code></pre></div><ul><li>本地版</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day01<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WordCount</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//1.创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>).setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span>    <span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.textFile(<span class="hljs-string">&quot;src/main/data/words.txt&quot;</span>)    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span>    <span class="hljs-keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;      x.split(<span class="hljs-string">&quot; &quot;</span>)    &#125;)    <span class="hljs-comment">//4.继续对每个单词标记为1</span>    <span class="hljs-keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span>    <span class="hljs-keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)    <span class="hljs-comment">//6.生成最后的RDD, 将结果打印到控制台</span>    ouputRDD.foreach(println(_))    <span class="hljs-comment">//7.关闭上下文</span>    sc.stop()  &#125;&#125;</code></pre></div><ul><li>Linux版</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day01<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WordCount_Linux</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//0.创建输入路径和输出路径</span>    <span class="hljs-keyword">val</span> input_path = args(<span class="hljs-number">0</span>)    <span class="hljs-keyword">val</span> output_path = args(<span class="hljs-number">1</span>)    <span class="hljs-comment">//1.创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setAppName(<span class="hljs-string">&quot;WordCount&quot;</span>)    <span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-comment">//2.加载文本文件words.txt,生成一个RDD</span>    <span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.textFile(input_path)    <span class="hljs-comment">//3.对RRD进行扁平化成单词</span>    <span class="hljs-keyword">val</span> flatRDD = inputRDD.flatMap((x) =&gt; &#123;      x.split(<span class="hljs-string">&quot; &quot;</span>)    &#125;)    <span class="hljs-comment">//4.继续对每个单词标记为1</span>    <span class="hljs-keyword">val</span> wordOneRDD = flatRDD.map((_, <span class="hljs-number">1</span>))    <span class="hljs-comment">//5继续reduceByKey进行分组统计</span>    <span class="hljs-keyword">val</span> ouputRDD = wordOneRDD.reduceByKey(_ + _)    <span class="hljs-comment">//6.生成最后的RDD, 将结果上传到HDFS</span>    ouputRDD.saveAsTextFile(output_path)    <span class="hljs-comment">//7.关闭上下文</span>    sc.stop()  &#125;&#125;</code></pre></div><h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.sparksql<span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">Row</span>, <span class="hljs-type">SparkSession</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Author: Jface</span><span class="hljs-comment"> * @Date: 2021/9/9 23:34</span><span class="hljs-comment"> * @Desc: 使用 SparkSQL 读取文本文件做 Wordcount，分别使用 DSL 和 SQL 风格实现</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Wordcount</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//1.构建上下文对象，并导包</span>    <span class="hljs-keyword">val</span> spark: <span class="hljs-type">SparkSession</span> = <span class="hljs-type">SparkSession</span>.builder()      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)      .getOrCreate()    <span class="hljs-keyword">import</span> spark.implicits._    <span class="hljs-comment">//2.读取文本文件，获取 DataSet</span>    <span class="hljs-keyword">val</span> inputDataSet: <span class="hljs-type">Dataset</span>[<span class="hljs-type">String</span>] = spark.read.textFile(<span class="hljs-string">&quot;learnSpark/datas/wordcount.data&quot;</span>)    <span class="hljs-comment">//测试看看</span>    <span class="hljs-comment">//inputDataSet.printSchema()</span>    <span class="hljs-comment">//inputDataSet.show()</span>    <span class="hljs-comment">//3.使用 DSL风格实现，导包</span>    <span class="hljs-keyword">import</span> org.apache.spark.sql.functions._    <span class="hljs-comment">//3.1 过滤脏数据</span>    <span class="hljs-keyword">val</span> resultDataset01: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = inputDataSet.where($<span class="hljs-string">&quot;value&quot;</span>.isNotNull &amp;&amp; length(trim($<span class="hljs-string">&quot;value&quot;</span>)) &gt; <span class="hljs-number">0</span>)      <span class="hljs-comment">//3.2 切割并把 value 行转成列</span>      .select(explode(split(trim($<span class="hljs-string">&quot;value&quot;</span>), <span class="hljs-string">&quot;\\s+&quot;</span>)).as(<span class="hljs-string">&quot;word&quot;</span>))      <span class="hljs-comment">//3.3 分组并聚合</span>      .groupBy($<span class="hljs-string">&quot;word&quot;</span>)      .agg(count($<span class="hljs-string">&quot;word&quot;</span>).as(<span class="hljs-string">&quot;total&quot;</span>))      <span class="hljs-comment">//3.4 倒序并只求前5条信息~</span>      .orderBy($<span class="hljs-string">&quot;total&quot;</span>.desc)      .limit(<span class="hljs-number">5</span>)    <span class="hljs-comment">//resultDataset01.printSchema()</span>    <span class="hljs-comment">//resultDataset01.show(10, truncate = false)</span>    <span class="hljs-comment">//4.使用 SQL 风格实现</span>    <span class="hljs-comment">//4.1 注册临时视图</span>    <span class="hljs-comment">//4.2 编写 SQL 并执行</span>    inputDataSet.createOrReplaceTempView(<span class="hljs-string">&quot;tmp_view_lines&quot;</span>)    <span class="hljs-keyword">val</span> resultDataSet02: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = spark.sql(      <span class="hljs-string">&quot;&quot;&quot;</span><span class="hljs-string">        |with tmp as</span><span class="hljs-string">        | (select explode(split(trim(value), &quot;\\s+&quot;)) as word</span><span class="hljs-string">        |from tmp_view_lines</span><span class="hljs-string">        |where value is not null and length(trim(value)) &gt; 0 )</span><span class="hljs-string">        |select t.word ,count(1) as total</span><span class="hljs-string">        |from tmp t</span><span class="hljs-string">        |group by t.word</span><span class="hljs-string">        |order by total desc</span><span class="hljs-string">        |&quot;&quot;&quot;</span>.stripMargin)    resultDataSet02.printSchema()    resultDataSet02.show(<span class="hljs-number">5</span>, truncate = <span class="hljs-literal">false</span>)    <span class="hljs-comment">//5.关闭上下文对象</span>    spark.stop();  &#125;&#125;</code></pre></div><h3 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h3><ul><li>前期准备：安装 netcat</li></ul><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">// 在Linux上安装 netcat</span>yum install nc -yyum install nmap -y<span class="hljs-comment">// 向 9999 端口发送数据</span>nc -lk <span class="hljs-number">9999</span></code></pre></div><ul><li>Wordcount  by UpdateStateByKey</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day06.streaming<span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">ReceiverInputDStream</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: wordcount 案例，通过 UpdateStateByKey 实现宕机后状态恢复</span><span class="hljs-comment"> * 需要利用ncat 发数据， </span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S4ocketWordcountUpdateStateByKeyRecovery</span> </span>&#123;    <span class="hljs-comment">//设置路径</span>    <span class="hljs-keyword">val</span> <span class="hljs-type">CKP</span> =<span class="hljs-string">&quot;src/main/data/ckp/&quot;</span>+<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>)    <span class="hljs-comment">//1.创建上下文对象, 指定批处理时间间隔为5秒</span>    <span class="hljs-keyword">val</span> creatingFunc =()=&gt;    &#123;      <span class="hljs-keyword">val</span> conf: <span class="hljs-type">SparkConf</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()        .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))        .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)      <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)      <span class="hljs-comment">//2. 创建一个接收文本数据流的流对象</span>      <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))      <span class="hljs-comment">//3.设置checkpoint位置</span>      ssc.checkpoint(<span class="hljs-type">CKP</span>)      <span class="hljs-comment">//4.接收socket数据</span>      <span class="hljs-keyword">val</span> inputDStream: <span class="hljs-type">ReceiverInputDStream</span>[<span class="hljs-type">String</span>] = ssc.socketTextStream(<span class="hljs-string">&quot;node1&quot;</span>, <span class="hljs-number">9999</span>)      <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 5.wordcount, 并做累计统计</span>      <span class="hljs-comment">//自定义一个函数, 实现保存State状态和数据聚合</span>      <span class="hljs-comment">//seq里面是value的数组,[1,1,], state是上次的状态, 累计值</span>      <span class="hljs-keyword">val</span> updateFunc = (seq: <span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>], state: <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>]) =&gt; &#123;        <span class="hljs-keyword">if</span> (!seq.isEmpty) &#123;          <span class="hljs-keyword">val</span> this_value: <span class="hljs-type">Int</span> = seq.sum          <span class="hljs-keyword">val</span> last_value: <span class="hljs-type">Int</span> = state.getOrElse(<span class="hljs-number">0</span>)          <span class="hljs-keyword">val</span> new_state: <span class="hljs-type">Int</span> = this_value + last_value          <span class="hljs-type">Some</span>(new_state)        &#125;        <span class="hljs-keyword">else</span> &#123;          state        &#125;      &#125;      <span class="hljs-comment">//开始做wordcount,并打印输出</span>      <span class="hljs-keyword">val</span> wordDStream: <span class="hljs-type">DStream</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] = inputDStream.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))        .map((_, <span class="hljs-number">1</span>))        .updateStateByKey(updateFunc)      wordDStream.print()    ssc    &#125;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-keyword">val</span> ssc: <span class="hljs-type">StreamingContext</span> = <span class="hljs-type">StreamingContext</span>.getOrCreate(<span class="hljs-type">CKP</span>, creatingFunc)    <span class="hljs-comment">//启动流式应用</span>    ssc.start()    <span class="hljs-comment">//让应用一直处于监听状态</span>    ssc.awaitTermination()    <span class="hljs-comment">//合理关闭流式应用</span>    ssc.stop(<span class="hljs-literal">true</span>, <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><h3 id="SparkStreaming-amp-Kafka"><a href="#SparkStreaming-amp-Kafka" class="headerlink" title="SparkStreaming &amp; Kafka"></a>SparkStreaming &amp; Kafka</h3><ul><li>自动提交 Offset</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">InputDStream</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="hljs-type">ConsumerStrategies</span>, <span class="hljs-type">KafkaUtils</span>, <span class="hljs-type">LocationStrategies</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-keyword">import</span> scala.collection.mutable.<span class="hljs-type">Set</span><span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: Spark  Kafka自动提交offset</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S1KafkaAutoCommit</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))    <span class="hljs-comment">//准备kafka连接参数</span>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">true</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span>    )    <span class="hljs-keyword">val</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](      ssc,      <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,      <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)    )    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span>    <span class="hljs-keyword">val</span> resutDStream: <span class="hljs-type">DStream</span>[<span class="hljs-type">Unit</span>] = kafkaDStream.map(x =&gt; &#123;      println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)    &#125;)    <span class="hljs-comment">//打印数据</span>    resutDStream.print()    <span class="hljs-comment">//启动并停留</span>    ssc.start()    ssc.awaitTermination()    <span class="hljs-comment">//合理化关闭</span>    ssc.stop(stopSparkContext = <span class="hljs-literal">true</span>, stopGracefully = <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><ul><li>手动提交 Offset</li></ul><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="hljs-type">DStream</span>, <span class="hljs-type">InputDStream</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="hljs-type">CanCommitOffsets</span>, <span class="hljs-type">ConsumerStrategies</span>, <span class="hljs-type">HasOffsetRanges</span>, <span class="hljs-type">KafkaUtils</span>, <span class="hljs-type">LocationStrategies</span>, <span class="hljs-type">OffsetRange</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 topic</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S2KafkaCommit</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))    <span class="hljs-comment">//准备kafka连接参数</span>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">false</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span>    )    <span class="hljs-keyword">val</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](      ssc,      <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,      <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams)    )    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span>    kafkaDStream.foreachRDD(rdd =&gt; &#123;      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;        <span class="hljs-comment">//对每个批次进行处理</span>        <span class="hljs-comment">//提取并打印偏移量范围信息</span>        <span class="hljs-keyword">val</span> hasOffsetRanges: <span class="hljs-type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>]        <span class="hljs-keyword">val</span> offsetRanges: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>] = hasOffsetRanges.offsetRanges        println(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)        offsetRanges.foreach(println(_))        <span class="hljs-comment">//打印每个批次的具体信息</span>        rdd.foreach(x =&gt; &#123;          println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)        &#125;)        <span class="hljs-comment">//手动将偏移量访问信息提交到默认主题</span>        kafkaDStream.asInstanceOf[<span class="hljs-type">CanCommitOffsets</span>].commitAsync(offsetRanges)        println(<span class="hljs-string">&quot;成功提交了偏移量信息&quot;</span>)      &#125;    &#125;)    <span class="hljs-comment">//启动并停留</span>    ssc.start()    ssc.awaitTermination()    <span class="hljs-comment">//合理化关闭</span>    ssc.stop(<span class="hljs-literal">true</span>,   <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><ul><li><p>手动提交 Offset 到 MySQL</p><ul><li>S3KafkaOffsetToMysql</li></ul>  <div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><span class="hljs-keyword">import</span> org.apache.kafka.common.<span class="hljs-type">TopicPartition</span><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.<span class="hljs-type">InputDStream</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010._<span class="hljs-keyword">import</span> org.apache.spark.streaming.&#123;<span class="hljs-type">Seconds</span>, <span class="hljs-type">StreamingContext</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-keyword">import</span> scala.collection.mutable<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: Spark  Kafka 手动提交 offset 到默认 MySQL</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S3KafkaOffsetToMysql</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//创建上下文对象</span>    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()      .setAppName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(conf)    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))    <span class="hljs-comment">//准备kafka连接参数</span>    <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>(      <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;node1:9092,node2:9092,nodo3:9092&quot;</span>,      <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//key的反序列化规则</span>      <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>], <span class="hljs-comment">//value的反序列化规则</span>      <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-comment">//消费者组名称</span>      <span class="hljs-comment">//earliest:表示如果有offset记录从offset记录开始消费,如果没有从最早的消息开始消费</span>      <span class="hljs-comment">//latest:表示如果有offset记录从offset记录开始消费,如果没有从最后/最新的消息开始消费</span>      <span class="hljs-comment">//none:表示如果有offset记录从offset记录开始消费,如果没有就报错</span>      <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;latest&quot;</span>, <span class="hljs-comment">//offset重置位置</span>      <span class="hljs-string">&quot;auto.commit.interval.ms&quot;</span> -&gt; <span class="hljs-string">&quot;1000&quot;</span>, <span class="hljs-comment">//自动提交的时间间隔</span>      <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="hljs-literal">false</span>: java.lang.<span class="hljs-type">Boolean</span>) <span class="hljs-comment">//是否自动提交偏移量到kafka的专门存储偏移量的默认topic</span>    )    <span class="hljs-comment">//去MySQL查询上次消费的位置</span>    <span class="hljs-keyword">val</span> offsetMap: mutable.<span class="hljs-type">Map</span>[<span class="hljs-type">TopicPartition</span>, <span class="hljs-type">Long</span>] = <span class="hljs-type">OffsetUtil</span>.getOffsetMap(<span class="hljs-string">&quot;spark&quot;</span>, <span class="hljs-string">&quot;spark_kafka&quot;</span>)    <span class="hljs-comment">//连接kafka, 拉取一批数据, 得到DSteam</span>    <span class="hljs-keyword">var</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-literal">null</span>    <span class="hljs-comment">//第一次查询, MySQL没有 offset 数据</span>    <span class="hljs-keyword">if</span> (offsetMap.isEmpty) &#123;      kafkaDStream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](        ssc,        <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,        <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams) <span class="hljs-comment">//第一次就看 Kafka 发啥</span>      )    &#125;    <span class="hljs-comment">//第二次查询, MySQL中有 offset 数据</span>    <span class="hljs-keyword">else</span> &#123;      kafkaDStream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](        ssc,        <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,        <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<span class="hljs-type">Set</span>(<span class="hljs-string">&quot;spark_kafka&quot;</span>), kafkaParams, offsetMap) <span class="hljs-comment">//第二次开始就从 MySQL 获取</span>      )    &#125;    <span class="hljs-comment">//对每个批次进行处理</span>    kafkaDStream.foreachRDD(rdd =&gt; &#123;      <span class="hljs-keyword">if</span> (!rdd.isEmpty()) &#123;        <span class="hljs-comment">//提取并打印偏移量范围信息</span>        <span class="hljs-keyword">val</span> hasOffsetRanges: <span class="hljs-type">HasOffsetRanges</span> = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>]        <span class="hljs-keyword">val</span> offsetRanges: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>] = hasOffsetRanges.offsetRanges        println(<span class="hljs-string">&quot;它的行偏移量是: &quot;</span>)        offsetRanges.foreach(println(_))        <span class="hljs-comment">//打印每个批次的具体信息</span>        rdd.foreach(x =&gt; &#123;          println(<span class="hljs-string">s&quot;topic=<span class="hljs-subst">$&#123;x.topic()&#125;</span>,partition=<span class="hljs-subst">$&#123;x.partition()&#125;</span>,offset=<span class="hljs-subst">$&#123;x.offset()&#125;</span>,key=<span class="hljs-subst">$&#123;x.key()&#125;</span>,value=<span class="hljs-subst">$&#123;x.value()&#125;</span>&quot;</span>)        &#125;)        <span class="hljs-comment">//手动将偏移量访问信息提交到MySQL</span>        <span class="hljs-type">OffsetUtil</span>.saveOffsetRanges(<span class="hljs-string">&quot;spark&quot;</span>, offsetRanges)        println(<span class="hljs-string">&quot;成功提交了偏移量到MySQL&quot;</span>)      &#125;    &#125;)    <span class="hljs-comment">//启动并停留</span>    ssc.start()    ssc.awaitTermination()    <span class="hljs-comment">//合理化关闭</span>    ssc.stop(stopSparkContext = <span class="hljs-literal">true</span>, stopGracefully = <span class="hljs-literal">true</span>)  &#125;&#125;</code></pre></div><ul><li>OffsetUtil</li></ul>  <div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.streaming<span class="hljs-keyword">import</span> org.apache.kafka.common.<span class="hljs-type">TopicPartition</span><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.<span class="hljs-type">OffsetRange</span><span class="hljs-keyword">import</span> scala.collection.mutable.<span class="hljs-type">Map</span><span class="hljs-keyword">import</span> java.sql.&#123;<span class="hljs-type">DriverManager</span>, <span class="hljs-type">ResultSet</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: 定义一个单例对象, 定义 2 个方法</span><span class="hljs-comment"> *        方法1: 从 MySQL 读取行偏移量</span><span class="hljs-comment"> *        方法2: 将行偏移量保存的 MySQL</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">OffsetUtil</span> </span>&#123;  <span class="hljs-comment">/**</span><span class="hljs-comment">   * 定义一个单例方法, 将偏移量保存到MySQL数据库</span><span class="hljs-comment">   *</span><span class="hljs-comment">   * @param groupid     消费者组id</span><span class="hljs-comment">   * @param offsetRange 行偏移量对象</span><span class="hljs-comment">   */</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">saveOffsetRanges</span></span>(groupid: <span class="hljs-type">String</span>, offsetRange: <span class="hljs-type">Array</span>[<span class="hljs-type">OffsetRange</span>]) = &#123;    <span class="hljs-keyword">val</span> connection = <span class="hljs-type">DriverManager</span>.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>)    <span class="hljs-comment">//replace into表示之前有就替换,没有就插入</span>    <span class="hljs-keyword">val</span> ps = connection.prepareStatement(<span class="hljs-string">&quot;replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)&quot;</span>)    <span class="hljs-keyword">for</span> (o &lt;- offsetRange) &#123;      ps.setString(<span class="hljs-number">1</span>, o.topic)      ps.setInt(<span class="hljs-number">2</span>, o.partition)      ps.setString(<span class="hljs-number">3</span>, groupid)      ps.setLong(<span class="hljs-number">4</span>, o.untilOffset)      ps.executeUpdate()    &#125;    ps.close()    connection.close()  &#125;  <span class="hljs-comment">/**</span><span class="hljs-comment">   * 定义一个方法, 用于从 MySQL 中读取行偏移位置</span><span class="hljs-comment">   * @param groupid 消费者组id</span><span class="hljs-comment">   * @param topic   想要消费的数据主题</span><span class="hljs-comment">   */</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getOffsetMap</span></span>(groupid: <span class="hljs-type">String</span>, topic: <span class="hljs-type">String</span>) = &#123;    <span class="hljs-comment">//1.从数据库查询对应数据</span>    <span class="hljs-keyword">val</span> connection = <span class="hljs-type">DriverManager</span>.getConnection(<span class="hljs-string">&quot;jdbc:mysql://localhost:3306/d_spark&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>,      <span class="hljs-string">&quot;root&quot;</span>)    <span class="hljs-keyword">val</span> ps = connection.prepareStatement(<span class="hljs-string">&quot;select * from t_offset where groupid=?  and topic=?&quot;</span>)    ps.setString(<span class="hljs-number">1</span>, groupid)    ps.setString(<span class="hljs-number">2</span>, topic)    <span class="hljs-keyword">val</span> rs: <span class="hljs-type">ResultSet</span> = ps.executeQuery()    <span class="hljs-comment">//解析数据, 返回</span>    <span class="hljs-keyword">var</span> offsetMap = <span class="hljs-type">Map</span>[<span class="hljs-type">TopicPartition</span>, <span class="hljs-type">Long</span>]()    <span class="hljs-keyword">while</span> (rs.next()) &#123;      <span class="hljs-keyword">val</span> topicPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">TopicPartition</span>(rs.getString(<span class="hljs-string">&quot;topic&quot;</span>), rs.getInt(<span class="hljs-string">&quot;partition&quot;</span>))      offsetMap.put(topicPartition, (rs.getLong(<span class="hljs-string">&quot;offset&quot;</span>)))    &#125;    rs.close()    rs.close()    connection.close()    offsetMap  &#125;&#125;</code></pre></div></li></ul><h3 id="StructuredStreaming"><a href="#StructuredStreaming" class="headerlink" title="StructuredStreaming"></a>StructuredStreaming</h3><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.test.day07.structuredStreaming<span class="hljs-keyword">import</span> org.apache.spark.sql.&#123;<span class="hljs-type">DataFrame</span>, <span class="hljs-type">Dataset</span>, <span class="hljs-type">Row</span>, <span class="hljs-type">SparkSession</span>&#125;<span class="hljs-keyword">import</span> org.apache.spark.sql.streaming.<span class="hljs-type">StreamingQuery</span><span class="hljs-keyword">import</span> org.apache.spark.sql.types.&#123;<span class="hljs-type">IntegerType</span>, <span class="hljs-type">StringType</span>, <span class="hljs-type">StructField</span>, <span class="hljs-type">StructType</span>&#125;<span class="hljs-comment">/**</span><span class="hljs-comment"> * @Desc: wordcount 案例 之 读取文件</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">S2StructuredStreamingTextFile</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;    <span class="hljs-comment">//1.创建上下文对象</span>    <span class="hljs-keyword">val</span> spark: <span class="hljs-type">SparkSession</span> = <span class="hljs-type">SparkSession</span>.builder()      .appName(<span class="hljs-keyword">this</span>.getClass.getSimpleName.stripSuffix(<span class="hljs-string">&quot;$&quot;</span>))      .master(<span class="hljs-string">&quot;local[*]&quot;</span>)      .config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-number">4</span>)      .getOrCreate()    <span class="hljs-comment">//2.读取csv, 得到流式DataFrame, 每行就是每批次的行数据</span>    <span class="hljs-comment">//自定义 Schema 信息</span>    <span class="hljs-keyword">val</span> schema = <span class="hljs-keyword">new</span> <span class="hljs-type">StructType</span>(<span class="hljs-type">Array</span>(      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-type">StringType</span>),      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;age&quot;</span>, <span class="hljs-type">IntegerType</span>),      <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;hobby&quot;</span>, <span class="hljs-type">StringType</span>))    )    <span class="hljs-keyword">val</span> inputDF: <span class="hljs-type">DataFrame</span> = spark.readStream      .format(<span class="hljs-string">&quot;csv&quot;</span>)      .option(<span class="hljs-string">&quot;sep&quot;</span>, <span class="hljs-string">&quot;;&quot;</span>)      .schema(schema)      .load(<span class="hljs-string">&quot;src/main/data/input/persons&quot;</span>)    <span class="hljs-comment">//3.进行wordcount, DSL风格</span>    inputDF.printSchema()    <span class="hljs-comment">//用 DSL 风格实现</span>    <span class="hljs-keyword">import</span> spark.implicits._    <span class="hljs-keyword">val</span> <span class="hljs-type">DF</span>: <span class="hljs-type">Dataset</span>[<span class="hljs-type">Row</span>] = inputDF.where(<span class="hljs-string">&quot;age&lt;25&quot;</span>)      .groupBy(<span class="hljs-string">&quot;hobby&quot;</span>)      .count()      .orderBy($<span class="hljs-string">&quot;count&quot;</span>.desc)    <span class="hljs-comment">// 用 SQL 风格实现</span>    inputDF.createOrReplaceTempView(<span class="hljs-string">&quot;t_spark&quot;</span>)    <span class="hljs-keyword">val</span> <span class="hljs-type">DF2</span>: <span class="hljs-type">DataFrame</span> = spark.sql(      <span class="hljs-string">&quot;&quot;&quot;</span><span class="hljs-string">        |select</span><span class="hljs-string">        |hobby,</span><span class="hljs-string">        |count(1) as cnt</span><span class="hljs-string">        |from t_spark</span><span class="hljs-string">        |where age&lt;25</span><span class="hljs-string">        |group by hobby</span><span class="hljs-string">        |order by cnt desc</span><span class="hljs-string">        |&quot;&quot;&quot;</span>.stripMargin)    <span class="hljs-keyword">val</span> query: <span class="hljs-type">StreamingQuery</span> = <span class="hljs-type">DF</span>.writeStream      <span class="hljs-comment">//append 默认追加 输出新的数据, 只支持简单查询, 有聚合就不能使用</span>      <span class="hljs-comment">//complete:完整模式, 输出完整数据, 支持集合和排序</span>      <span class="hljs-comment">//update: 更新模式, 输出有更新的数据,  支持聚合但是不支持排序</span>      .outputMode(<span class="hljs-string">&quot;complete&quot;</span>)      .format(<span class="hljs-string">&quot;console&quot;</span>)      .option(<span class="hljs-string">&quot;rowNumber&quot;</span>, <span class="hljs-number">10</span>)      .option(<span class="hljs-string">&quot;truncate&quot;</span>, <span class="hljs-literal">false</span>)      <span class="hljs-comment">//4.启动流式查询</span>      .start()    <span class="hljs-comment">//5.驻留监听</span>    query.awaitTermination()    <span class="hljs-comment">//6.关闭流式查询</span>    query.stop()  &#125;&#125;</code></pre></div><h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="批处理-DataSet"><a href="#批处理-DataSet" class="headerlink" title="批处理 DataSet"></a>批处理 DataSet</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.flink.start;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.operators.Order;<span class="hljs-keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;<span class="hljs-keyword">import</span> org.apache.flink.api.java.operators.*;<span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span>: Jface</span><span class="hljs-comment"> * <span class="hljs-doctag">@Date</span>: 2021/9/5 12:37</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 基于Flink引擎实现批处理词频统计WordCount：过滤filter、排序sort等操作</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_01WordCount</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//1.准备环境-env</span>        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();        <span class="hljs-comment">//2.准备数据-source</span>        DataSource&lt;String&gt; inputDataSet = env.readTextFile(<span class="hljs-string">&quot;datas/wc.input&quot;</span>);        <span class="hljs-comment">//3.处理数据-transformation</span>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.1 过滤脏数据</span>        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataSet.filter(<span class="hljs-keyword">new</span> FilterFunction&lt;String&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">filter</span><span class="hljs-params">(String line)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span> != line &amp;&amp; line.trim().length() &gt; <span class="hljs-number">0</span>;            &#125;        &#125;)                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.2 切割</span>                .flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;                    <span class="hljs-meta">@Override</span>                    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String line, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                        <span class="hljs-keyword">for</span> (String s : line.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>)) &#123;                            out.collect(s);                        &#125;                    &#125;                &#125;)                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.3 转换二元组</span>                .map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;                    <span class="hljs-meta">@Override</span>                    <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String word)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                        <span class="hljs-keyword">return</span> Tuple2.of(word, <span class="hljs-number">1</span>);                    &#125;                &#125;)                <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 3.4 分组求和</span>                .groupBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);        <span class="hljs-comment">//4.输出结果-sink</span>        resultDataSet.printToErr();        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> sort 排序，全局排序需要设置分区数 1</span>        SortPartitionOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sortDataSet = resultDataSet.sortPartition(<span class="hljs-string">&quot;f1&quot;</span>, Order.DESCENDING)                .setParallelism(<span class="hljs-number">1</span>);        sortDataSet.printToErr();        <span class="hljs-comment">//只选择前3的数据</span>        GroupReduceOperator&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; resultDataSet2 = sortDataSet.first(<span class="hljs-number">3</span>);        resultDataSet2.print();        <span class="hljs-comment">//5.触发执行-execute，没有写出不需要触发执行</span>    &#125;&#125;</code></pre></div><h3 id="流处理-DataStream"><a href="#流处理-DataStream" class="headerlink" title="流处理 DataStream"></a>流处理 DataStream</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.stream;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现实时流式数据处理，监听端口并做 wordcount</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StreamWordcount</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;         <span class="hljs-comment">//1.准备环境-env</span>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();         <span class="hljs-comment">//2.准备数据-source</span>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);        <span class="hljs-comment">//3.处理数据-transformation</span>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataSet = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);                <span class="hljs-keyword">for</span> (String s : arr) &#123;                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span>                &#125;            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> Tuple2.of(value,<span class="hljs-number">1</span>);            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);        <span class="hljs-comment">//4.输出结果-sink</span>        resultDataSet.print();        <span class="hljs-comment">//5.触发执行-execute</span>        env.execute(StreamWordcount.class.getSimpleName());    &#125;&#125;</code></pre></div><p>流处理 Flink On Yarn</p><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test.submit;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.MapFunction;<span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<span class="hljs-keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;<span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 使用 FLink 计算引擎实现流式数据处理，从socket 接收数据并做 wordcount</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Wordcount</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-comment">//0.使用工具类，解析程序传递参数</span>        ParameterTool parameterTool = ParameterTool.fromArgs(args);        <span class="hljs-keyword">if</span> (parameterTool.getNumberOfParameters() != <span class="hljs-number">2</span>) &#123;            System.out.println(<span class="hljs-string">&quot;Usage: WordCount --host &lt;host&gt; --port &lt;port&gt; ............&quot;</span>);            System.exit(-<span class="hljs-number">1</span>);        &#125;        String host = parameterTool.get(<span class="hljs-string">&quot;host&quot;</span>);        parameterTool.getInt(<span class="hljs-string">&quot;port&quot;</span>, <span class="hljs-number">9999</span>);        <span class="hljs-comment">//1.准备环境-env</span>        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        <span class="hljs-comment">//2.准备数据-source</span>        DataStreamSource&lt;String&gt; inputDataStream = env.socketTextStream(<span class="hljs-string">&quot;192.168.88.161&quot;</span>, <span class="hljs-number">9999</span>);        <span class="hljs-comment">//3.处理数据-transformation</span>        <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 切割成单个单词 flatmap</span>        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultDataStream = inputDataStream.flatMap(<span class="hljs-keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(String value, Collector&lt;String&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                String[] arr = value.trim().split(<span class="hljs-string">&quot;\\s+&quot;</span>);                <span class="hljs-keyword">for</span> (String s : arr) &#123;                    out.collect(s);<span class="hljs-comment">//将每个单词拆分出去</span>                &#125;            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 单词--&gt; 元组形式，map</span>        &#125;).map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> Tuple2.of(value, <span class="hljs-number">1</span>);            &#125;            <span class="hljs-comment">//<span class="hljs-doctag">TODO:</span> 分组聚合 keyBy &amp; sum</span>        &#125;).keyBy(<span class="hljs-number">0</span>).sum(<span class="hljs-number">1</span>);        <span class="hljs-comment">//4.输出结果-sink</span>        resultDataStream.print();        <span class="hljs-comment">//5.触发执行-execute</span>        env.execute(Wordcount.class.getSimpleName());    &#125;&#125;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>Hadoop</tag>
      
      <tag>Scala</tag>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>管理配置文件的工具：Commons Configuration</title>
    <link href="/2020/10/20/%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%B7%A5%E5%85%B7CommonsConfiguration/"/>
    <url>/2020/10/20/%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%B7%A5%E5%85%B7CommonsConfiguration/</url>
    
    <content type="html"><![CDATA[<p>一般读取配置文件，或者说集群环境传参的方式有如下几种：</p><p>1、Main 程序留出参数入口，通过 args 接收参数，运行 jar 的时候传入参数</p><p>2、将配置文件放入 resources ，通过类加载器获取参数文件，或者创建专门工具类读取resources 中的配置文件信息</p><p>这两种方法各有优缺点，第一种虽然修改参数非常方便，但是当需要指定的参数较多时会繁琐；</p><p>第二种方式将配置文件一起打成 jar 包，当需要修改参数信息的时需要重写打包，也非常繁琐。</p><p>最近项目当中使用了一种的新的工具：Apache Commons Configuration，完美解决了我的需求</p><h2 id="Commons-Configuration-基本介绍"><a href="#Commons-Configuration-基本介绍" class="headerlink" title="Commons Configuration 基本介绍"></a>Commons Configuration 基本介绍</h2><p>Commons Configuration 软件库提供了一个通用配置接口，它使 Java 应用程序能够从各种来源读取配置数据。</p><p>支持各种格式的配置文件</p><ul><li>Properties files</li><li>XML documents</li><li>Windows INI files</li><li>Property list files (plist)</li><li>JNDI</li><li>JDBC Datasource</li><li>System properties</li><li>Applet parameters</li><li>Servlet parameters</li></ul><p>官网：<a href="https://commons.apache.org/proper/commons-configuration/">https://commons.apache.org/proper/commons-configuration/</a> </p><h3 id="Maven-依赖"><a href="#Maven-依赖" class="headerlink" title="Maven 依赖"></a>Maven 依赖</h3><div class="code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span>    <span class="hljs-comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.commons/commons-configuration2 --&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.commons<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>commons-configuration2<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.2<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>commons-beanutils<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>commons-beanutils<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.9.3<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span></code></pre></div><h3 id="新建-test-propertis-文件"><a href="#新建-test-propertis-文件" class="headerlink" title="新建 test.propertis 文件"></a>新建 test.propertis 文件</h3><div class="code-wrapper"><pre><code class="hljs ini"><span class="hljs-comment">### common configuration start</span><span class="hljs-attr">database.host</span> = db.acme.com<span class="hljs-attr">database.port</span> = <span class="hljs-number">8199</span><span class="hljs-attr">database.user</span> = admin<span class="hljs-attr">database.password</span> = ???<span class="hljs-attr">database.timeout</span> = <span class="hljs-number">60000</span><span class="hljs-comment">###common configuration end</span></code></pre></div><h3 id="读取测试"><a href="#读取测试" class="headerlink" title="读取测试"></a>读取测试</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.test;<span class="hljs-keyword">import</span> org.apache.commons.configuration2.Configuration;<span class="hljs-keyword">import</span> org.apache.commons.configuration2.builder.fluent.Configurations;<span class="hljs-keyword">import</span> org.apache.commons.configuration2.ex.ConfigurationException;<span class="hljs-keyword">import</span> java.io.File;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span>: Jface</span><span class="hljs-comment"> * <span class="hljs-doctag">@Desc</span>: 测试 Commons Configuration 的使用</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PropertiesFile</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">//1.初始化配置文件</span>        Configurations configs = <span class="hljs-keyword">new</span> Configurations();        Configuration config = <span class="hljs-keyword">null</span>;        <span class="hljs-comment">//2.读取配置文件内容</span>        <span class="hljs-keyword">try</span> &#123;            config = configs.properties(<span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;commons_configuration/src/main/resources/test.properties&quot;</span>));        &#125; <span class="hljs-keyword">catch</span> (ConfigurationException cex) &#123;            cex.printStackTrace();        &#125;        <span class="hljs-comment">//2.获取使用配置文件信息,打印测试</span>        System.out.println(config.getString(<span class="hljs-string">&quot;database.host&quot;</span>));    &#125;&#125;</code></pre></div><p>参考资料</p><p><a href="https://www.jianshu.com/p/625e833c1a49">https://www.jianshu.com/p/625e833c1a49</a> </p><p><a href="https://blog.csdn.net/wanghantong/article/details/79072474">https://blog.csdn.net/wanghantong/article/details/79072474</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>更好的日志框架：logback</title>
    <link href="/2020/10/10/%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/"/>
    <url>/2020/10/10/%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>Logback 是由 log4j 创始人设计的另一个开源日志组件</p><p>官方网站： <a href="http://logback.qos.ch/">http://logback.qos.ch</a> </p><p>它分为下面下个模块：</p><ul><li>logback-core：其它两个模块的基础模块</li><li>logback-classic：它是 log4j 的一个改良版本，同时它完整实现了 slf4j API使你可以很方便地更换成其它日志系统如 log4j 或 JDK14 Logging</li><li>logback-access：访问模块与 Servlet 容器集成提供通过 Http 来访问日志的功能</li></ul><h3 id="配置文件命名"><a href="#配置文件命名" class="headerlink" title="配置文件命名"></a>配置文件命名</h3><p>根据不同的日志系统，命名规则不尽相同</p><ul><li>Logback：<code>logback-spring.xml, logback-spring.groovy, logback.xml, logback.groovy</code></li><li>Log4j：<code>log4j-spring.properties, log4j-spring.xml, log4j.properties, log4j.xml</code></li><li>Log4j2：<code>log4j2-spring.xml, log4j2.xml</code></li><li>JDK (Java Util Logging)：<code>logging.properties</code></li></ul><h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><div class="code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">log4j.version</span>&gt;</span>1.7.7<span class="hljs-tag">&lt;/<span class="hljs-name">log4j.version</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">logback.version</span>&gt;</span>1.1.3<span class="hljs-tag">&lt;/<span class="hljs-name">logback.version</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span><span class="hljs-comment">&lt;!-- log4j日志 start--&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>ch.qos.logback<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>logback-core<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>ch.qos.logback<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>logback-classic<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;logback.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.slf4j<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>log4j-over-slf4j<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;log4j.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-comment">&lt;!-- log4j日志 end--&gt;</span></code></pre></div><h3 id="配置文件内容（以-logback-xml-为例）"><a href="#配置文件内容（以-logback-xml-为例）" class="headerlink" title="配置文件内容（以 logback.xml 为例）"></a>配置文件内容（以 logback.xml 为例）</h3><div class="code-wrapper"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>    <span class="hljs-comment">&lt;!--http://logback.qos.ch/manual/configuration.html--&gt;</span>    <span class="hljs-comment">&lt;!--设置日志输出到控制台--&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">appender</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;STDOUT&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;ch.qos.logback.core.ConsoleAppender&quot;</span>&gt;</span>        <span class="hljs-comment">&lt;!-- encoder的默认实现类是ch.qos.logback.classic.encoder.PatternLayoutEncoder --&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">encoder</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">pattern</span>&gt;</span>%date %level [%thread] %logger&#123;10&#125; [%file:%line] %msg%n<span class="hljs-tag">&lt;/<span class="hljs-name">pattern</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">encoder</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">appender</span>&gt;</span>    <span class="hljs-comment">&lt;!-- name值可以是包名或具体的类名：该包（包括子包）下的类或该类将采用此logger --&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">logger</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;com.test.flink&quot;</span> <span class="hljs-attr">level</span>=<span class="hljs-string">&quot;INFO&quot;</span> <span class="hljs-attr">additivity</span>=<span class="hljs-string">&quot;false&quot;</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">appender-ref</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;STDOUT&quot;</span> /&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">logger</span>&gt;</span>    <span class="hljs-comment">&lt;!--设置日志输出为文件--&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">appender</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;FILE&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;ch.qos.logback.core.FileAppender&quot;</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">file</span>&gt;</span>warn.log<span class="hljs-tag">&lt;/<span class="hljs-name">file</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">encoder</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">pattern</span>&gt;</span>%date %level [%thread] %logger&#123;10&#125; [%file:%line] %msg%n<span class="hljs-tag">&lt;/<span class="hljs-name">pattern</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">encoder</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">appender</span>&gt;</span>    <span class="hljs-comment">&lt;!--日志输出级别--&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">root</span> <span class="hljs-attr">level</span>=<span class="hljs-string">&quot;WARN&quot;</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">appender-ref</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;STDOUT&quot;</span> /&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">appender-ref</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;FILE&quot;</span> /&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">root</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre></div><h3 id="使用-logback"><a href="#使用-logback" class="headerlink" title="使用 logback"></a>使用 logback</h3><div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">import</span> org.slf4j.Logger;<span class="hljs-keyword">import</span> org.slf4j.LoggerFactory;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLogback</span> </span>&#123;  <span class="hljs-keyword">final</span> <span class="hljs-keyword">static</span> Logger logger = LoggerFactory.getLogger(TestLogback.class);  <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;    logger.info(<span class="hljs-string">&quot;Test logback &quot;</span>);  &#125;&#125;</code></pre></div><h3 id="参考连接"><a href="#参考连接" class="headerlink" title="参考连接"></a>参考连接</h3><p><a href="https://www.jianshu.com/p/638b4e2c4068">https://www.jianshu.com/p/638b4e2c4068</a> </p><p><a href="https://www.cnblogs.com/warking/p/5710303.html">https://www.cnblogs.com/warking/p/5710303.html</a></p>]]></content>
    
    
    <categories>
      
      <category>日常工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>Flink</tag>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Spark：分布式并行计算框架（三）</title>
    <link href="/2020/10/03/Spark-Review-Day03/"/>
    <url>/2020/10/03/Spark-Review-Day03/</url>
    
    <content type="html"><![CDATA[<h2 id="Spark-on-Hive"><a href="#Spark-on-Hive" class="headerlink" title="Spark on Hive"></a>Spark on Hive</h2><blockquote><p>面试题：<code>spark on hive和hive on spark</code>区别？？？？</p></blockquote><p><img src="https://i.loli.net/2021/09/09/ZIij3s9rePu7VOM.png" alt="1630310212506"></p><blockquote><p>典型<strong>基于Spark和Hive离线数仓</strong>架构技术图，简易版本：</p></blockquote><div class="code-wrapper"><pre><code class="hljs 1c"><span class="hljs-number">1</span>、SparkSQL分析数据<span class="hljs-number">2</span>、Hive 管理元数据<span class="hljs-string">|</span>Spark on Hive 架构，离线数据仓库分析</code></pre></div><p><img src="https://i.loli.net/2021/09/09/GYA4PBNx5oZV8aT.png" alt="1630309369960"></p><div class="code-wrapper"><pre><code class="hljs css">SparkSQL与Hive集成，本质就是Spark Application应用程序，读取加载HiveMetaStore元数据。database数据库<span class="hljs-selector-tag">table</span>表字段分区.....</code></pre></div><blockquote><p>​        Spark Thrift Server将Spark Applicaiton当做一个服务运行，提供Beeline客户端和JDBC方式访问，与Hive中<strong>HiveServer2</strong>服务一样的</p></blockquote><p><img src="https://i.loli.net/2021/09/09/S6XtOoECMzY91Fq.png" alt="1630310389289"></p><div class="code-wrapper"><pre><code class="hljs ini">1、启动MySQL数据库存储Hive 元数据MetaStore2、启动HiveMetaStore服务提供提供元数据服务，读取到Hive中管理元数据信息3、启动HDFS文件系统Hive表中数据存储在HDFS服务4、启动Spark Thrift JDBC/ODBC server将Spark Application当做服务启动，通过JDBC或ODBC方式连接，也可以beeline 命令行连接编写DDL和DML语句，操作管理数据与Hive集成，启动服务时，让其连接到HiveMetaStore服务在SPARK_HOME/conf创建hive-site.xml文件，添加Hive MetaStore地址信息即可。&lt;?xml <span class="hljs-attr">version</span>=<span class="hljs-string">&quot;1.0&quot;</span>?&gt;&lt;?xml-stylesheet <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;hive.metastore.uris&lt;/name&gt;        &lt;value&gt;thrift://node1:9083&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;启动服务/export/server/spark/sbin/start-thriftserver.sh \--hiveconf <span class="hljs-attr">hive.server2.thrift.port</span>=<span class="hljs-number">10000</span> \--hiveconf <span class="hljs-attr">hive.server2.thrift.bind.host</span>=node1 \--master local<span class="hljs-section">[2]</span> \--conf <span class="hljs-attr">spark.sql.shuffle.partitions</span>=<span class="hljs-number">2</span> \--conf <span class="hljs-attr">spark.serializer</span>=org.apache.spark.serializer.KryoSerializer5、beeline客户端连接/export/server/spark/bin/beelinebeeline&gt; !connect jdbc:hive2://node1:10000</code></pre></div><h2 id="Spark-内存管理"><a href="#Spark-内存管理" class="headerlink" title="Spark 内存管理"></a>Spark 内存管理</h2><blockquote><p>知识点：Spark Application运行架构</p></blockquote><p><img src="https://i.loli.net/2021/09/09/jKbPa9UTdv3VS7r.png" alt="Spark cluster components"></p><div class="code-wrapper"><pre><code class="hljs ada"><span class="hljs-number">1</span>、Driver Program创建SparkContext实例对象申请资源运行Executor、调度Job执行<span class="hljs-number">2</span>、Executors相当于线程池，里面执行<span class="hljs-keyword">Task</span>任务（以线程方式运行），每个<span class="hljs-keyword">Task</span>任务运行需要<span class="hljs-number">1</span>CoreCPU执行<span class="hljs-keyword">Task</span>任务、缓存RDD数据每个Executor资源：内存Memeory、CPUCore核数 -&gt; 执行<span class="hljs-keyword">Task</span>任务内存Memory主要被用于<span class="hljs-number">2</span>个部分：<span class="hljs-keyword">Task</span>任务执行时，需要内存Cache缓存数据，需要内存</code></pre></div><p><img src="https://i.loli.net/2021/09/09/FoNEep87UHqLACx.png" alt="1631180184608"></p><blockquote><p>在运行Spark Application时，需要设置如下三个参数：</p></blockquote><p><img src="https://i.loli.net/2021/09/09/4BqXng9zNCmsMx1.png" alt="1631180387861"></p><blockquote><p>Executor内存管理，从发展来说，经历2个阶段：</p></blockquote><ul><li>1）、Spark 1.6之前，==内存管理【静态内存管理】==<ul><li>直接将内存划分比例，存储Storage内存占比多少和执行Execution内存占比多少，固定死了</li><li>此种内存管理，存在不合理，比如内存浪费</li></ul></li><li>2）、Spark 1.6开始，提供：==统一内存管理（动态内存管理）==，到Spark 2.0时，仅仅支持此种管理方案<ul><li>约定：Storage存储和Execution执行内存占比，默认各占50%</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/09/eQTLtw7WhAFbiUn.png" alt="1630315362244"></p><ul><li>动态彼此相互借用<ul><li>如果Storage存储内存不足，但是Execution内存多余，可以借用存储数据，反之亦然</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/09/nQbt9yAMRCfdjT4.png" alt="1630315328271"></p><blockquote><p>Executor内存划分，包含四个部分，如下图所示：</p></blockquote><div class="code-wrapper"><pre><code class="hljs stylus"><span class="hljs-number">1</span>、预留内存：<span class="hljs-number">300</span>MBJVM进程自己使用，不允许用户使用<span class="hljs-number">2</span>、可用内存（UsableMemory）UsableMemory =  --executor-memory    -    <span class="hljs-number">300</span>MB<span class="hljs-number">1</span>）、UserMemory 用户内存可用内存占比：(<span class="hljs-number">1</span> - spark<span class="hljs-selector-class">.memory</span>.fraction) = <span class="hljs-number">0.4</span>可用内存占比： spark<span class="hljs-selector-class">.memory</span><span class="hljs-selector-class">.fraction</span> = <span class="hljs-number">0.6</span><span class="hljs-number">2</span>）、存储内存spark<span class="hljs-selector-class">.memory</span><span class="hljs-selector-class">.storageFraction</span> = <span class="hljs-number">0.5</span><span class="hljs-number">3</span>）、执行内存<span class="hljs-number">1</span>- spark<span class="hljs-selector-class">.memory</span><span class="hljs-selector-class">.storageFraction</span> = <span class="hljs-number">0.5</span></code></pre></div><p><img src="https://i.loli.net/2021/09/09/JP8fBFtXqz2srjL.png" alt="1630309139547"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Spark：分布式并行计算框架（二）</title>
    <link href="/2020/09/28/Spark-Review-Day02/"/>
    <url>/2020/09/28/Spark-Review-Day02/</url>
    
    <content type="html"><![CDATA[<h2 id="1、Spark-有哪些优化"><a href="#1、Spark-有哪些优化" class="headerlink" title="1、Spark 有哪些优化"></a>1、Spark 有哪些优化</h2><h3 id="第一、公共优化"><a href="#第一、公共优化" class="headerlink" title="第一、公共优化"></a>第一、公共优化</h3><blockquote><p>1、序列化（<code>Serialization</code>）</p></blockquote><div class="code-wrapper"><pre><code class="hljs mipsasm">Spark中默认序列化方式：<span class="hljs-keyword">Java </span>序列化（<span class="hljs-keyword">Java </span>serialization）要求数据类型必须实现序列化接口Serializable，比如HBase数据库读取数据时，封装到Result设置序列化为：Kryo 序列化比<span class="hljs-keyword">Java序列化性能提升10倍以上</span><span class="hljs-keyword"></span>设置：conf<span class="hljs-meta">.set</span>(<span class="hljs-string">&quot;spark.serializer&quot;</span>, <span class="hljs-string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</code></pre></div><blockquote><p>2、数据缓存（<code>Cache</code>）</p></blockquote><div class="code-wrapper"><pre><code class="hljs markdown">可以将RDD数据缓存，设置缓存级别StorageLevel<span class="hljs-code">要么内存、要么磁盘、要么内存和磁盘（是否序列化）、要么系统内存（可以不管）</span><span class="hljs-code"></span><span class="hljs-code">DataFrame和DataSet同样可以缓存，底层就是RDD</span><span class="hljs-code"></span>什么条件下缓存数据集呢？？？<span class="hljs-code">可以提升数据计算效率</span><span class="hljs-code">1. 数据集RDD被使用多次，考虑缓存，依据数据集大小合理设置缓存级别</span><span class="hljs-code">2. 数据集来之不易，并且使用不止一次</span></code></pre></div><blockquote><p>3、分区数目调整（<code>partition</code>）</p></blockquote><div class="code-wrapper"><pre><code class="hljs stylus">RDD分布式数据集，不可变的、分区的和并行计算集合，抽象的无论批处理还是流计算（微批处理）计算结果，往往数据量很小的，当保存到外部存储系统时，最好降低分区数目比如某日物流快递公司订单数据<span class="hljs-number">1000</span>W，按照省份统计<span class="hljs-number">34</span>条resultRDD<span class="hljs-selector-class">.coalease</span>(<span class="hljs-number">1</span>)<span class="hljs-selector-class">.forchePartition</span>将数据集保存外部系统时，最好针对每个分区数据操作</code></pre></div><h3 id="第二、批处理优化"><a href="#第二、批处理优化" class="headerlink" title="第二、批处理优化"></a>第二、批处理优化</h3><blockquote><p>基本上使用SparkSQL离线数据分析，考虑SparkSQL中哪些优化即可。</p></blockquote><div class="code-wrapper"><pre><code class="hljs llvm"><span class="hljs-number">1</span>、spark.sql.shuffle.partitions Shuffle时分区数目，默认值为<span class="hljs-number">200</span>，实际项目中可能太大，有可能太小Spark <span class="hljs-number">1</span>.<span class="hljs-keyword">x</span>和Spark <span class="hljs-number">2</span>.<span class="hljs-keyword">x</span>需要调整Spark <span class="hljs-number">3</span>.<span class="hljs-keyword">x</span>开始无需调整，新特性：自适应调整只能开启功能以后，SparkSQL程序在运行计算时，依据Shuffle数据量自动设置分区数目</code></pre></div><p><img src="https://i.loli.net/2021/09/07/52ERDKFvjp6qWkb.png" alt="图1"></p><div class="code-wrapper"><pre><code class="hljs pgsql"><span class="hljs-number">2</span>、广播变量大小调整SparkSQL在执行<span class="hljs-number">2</span>个表的数据<span class="hljs-keyword">Join</span>时，自动判断是否为大表<span class="hljs-keyword">JOIN</span>小表，如果有<span class="hljs-number">1</span>个小表的话，采用广播变量的方式广播数据，进行关联分析处理，避免产生Shuffle，提升性能。参数：spark.<span class="hljs-keyword">sql</span>.autoBroadcastJoinThreshold = <span class="hljs-number">10</span>M比如小表数据大小为<span class="hljs-number">12</span>M，完全可以采用广播<span class="hljs-keyword">JOIN</span>方式，此时调整 广播表里大小阈值</code></pre></div><p><img src="https://i.loli.net/2021/09/07/lFiBTV2c1WqhYOR.png" alt="图2"></p><h3 id="第三、流计算优化"><a href="#第三、流计算优化" class="headerlink" title="第三、流计算优化"></a>第三、流计算优化</h3><blockquote><p>要么使用SparkStreaming，要么使用StructuredStreaming分析数据，底层批处理（微批处理）。</p></blockquote><div class="code-wrapper"><pre><code class="hljs stylus"><span class="hljs-number">1</span>、限制高峰数据量限制每批次处理数据量，给定一个阈值，通过压力测试确定的值<span class="hljs-number">1</span>）、SparkStreaming参数：spark<span class="hljs-selector-class">.streaming</span><span class="hljs-selector-class">.kafka</span>.maxRatePerPartition</code></pre></div><p><img src="https://i.loli.net/2021/09/07/UAnNqGPmu4cKHtJ.png" alt="图3"></p><div class="code-wrapper"><pre><code class="hljs pf"><span class="hljs-number">2</span>）、StructuredStreaming参数：<span class="hljs-keyword">max</span>OffsetsPreTrigger，每次触发时，消费最大偏移量</code></pre></div><p><img src="https://i.loli.net/2021/09/07/A2txYP86wpjNGXO.png" alt="图4"></p><div class="code-wrapper"><pre><code class="hljs">针对SparkStreaming来说，为了更好处理流式数据，避免数据量太多，导致应用性能下降：反压机制原理，依据当前处理数据量和处理时间，决定下一批次处理数据量</code></pre></div><div class="code-wrapper"><pre><code class="hljs awk"><span class="hljs-number">2</span>、数据本地性等待时间https:<span class="hljs-regexp">//</span>spark.apache.org<span class="hljs-regexp">/docs/</span><span class="hljs-number">2.4</span>.<span class="hljs-number">5</span>/tuning.html<span class="hljs-comment">#data-locality</span></code></pre></div><p>​        <a href="https://www.cnblogs.com/cc11001100/p/10301716.html">https://www.cnblogs.com/cc11001100/p/10301716.html</a></p><div class="code-wrapper"><pre><code class="hljs ada"><span class="hljs-number">1</span>、PROCESS_LOCAL：进程本地性，<span class="hljs-keyword">Task</span>任务和Data数据在同一个JVM进程中顾名思义，要处理的数据就在同一个本地进程中，即数据和<span class="hljs-keyword">Task</span>在同一个Executor JVM中，这种情况就是RDD的数据在之前就已经被缓存过了，因为BlockManager是以Executor为单位的，所以只要<span class="hljs-keyword">Task</span>所需要的Block在所属的Executor的BlockManager上已经被缓存，这个数据本地性就是PROCESS_LOCAL，这种是最好的locality，这种情况下数据不需要在网络中传输。</code></pre></div><p><img src="https://i.loli.net/2021/09/07/vte8XGdYcxpCraw.png" alt="图5"></p><div class="code-wrapper"><pre><code class="hljs asciidoc">2、NODE<span class="hljs-emphasis">_LOCAL：</span><span class="hljs-emphasis">数据在同一台节点上，但是并不不在同一个jvm中，比如数据在同一台节点上的另外一个Executor上，速度要比PROCESS_LOCAL略慢。还有一种情况是读取HDFS的块就在当前节点上，数据本地性也是NODE_</span>LOCAL。</code></pre></div><p><img src="https://i.loli.net/2021/09/07/2T8MwE5Sin6V7UH.png" alt="图6"></p><div class="code-wrapper"><pre><code class="hljs asciidoc">3、NO_PREF：<span class="hljs-code">数据从哪里访问都一样，表示数据本地性无意义，其实指的是从MySQL、MongoDB之类的数据源读取数据。</span>4、RACK<span class="hljs-emphasis">_LOCAL：机架本地性</span><span class="hljs-emphasis">数据在同一机架上的其它节点，需要经过网络传输，速度要比NODE_</span>LOCAL慢。</code></pre></div><p><img src="https://i.loli.net/2021/09/07/t1VIFcsWBr4ZgPM.png" alt="图7"></p><div class="code-wrapper"><pre><code class="hljs fortran"><span class="hljs-number">5</span>、<span class="hljs-built_in">ANY</span>：数据在其它更远的网络上，甚至都不在同一个机架上，比RACK_LOCAL更慢，一般情况下不会出现这种级别，万一出现了可能是有什么异常需要排查下原因。</code></pre></div><blockquote><p>​        在流式数据处理计算中，设置数据本地性等待时间越小越好，不需要等待，直接使用某个数据即可，哪怕时最差：ANY都可以。</p></blockquote><div class="code-wrapper"><pre><code class="hljs fortran">参数：spark.locality.<span class="hljs-keyword">wait</span>，默认为值<span class="hljs-number">3</span>s可以设置为spark.locality.<span class="hljs-keyword">wait</span>=<span class="hljs-number">10</span>ms</code></pre></div><h2 id="2、SparkSQL-底层JOIN实现方式"><a href="#2、SparkSQL-底层JOIN实现方式" class="headerlink" title="2、SparkSQL 底层JOIN实现方式"></a>2、SparkSQL 底层JOIN实现方式</h2><blockquote><p>SparkSQL底层处理2个表JOIN时方式：3种，自动选择合理方式处理数据</p></blockquote><p>参考：<a href="https://www.cnblogs.com/JP6907/p/10721436.html">https://www.cnblogs.com/JP6907/p/10721436.html</a></p><p>对于Spark来说有3中Join的实现，每种Join对应着不同的应用场景：</p><ul><li>1、<strong>Broadcast Hash Join ： 适合一张较小的表和一张大表进行join</strong></li><li>2、<strong>Shuffle Hash Join :  适合一张小表和一张大表进行join，或者是两张小表之间的join</strong></li><li>3、<strong>Sort Merge Join ： 适合两张较大的表之间进行join</strong></li></ul><p><strong>前两者都基于的是Hash Join，只不过在hash join之前需要先shuffle还是先broadcast。</strong></p><h3 id="第一、Broadcast-Hash-Join"><a href="#第一、Broadcast-Hash-Join" class="headerlink" title="第一、Broadcast Hash Join"></a>第一、Broadcast Hash Join</h3><div class="code-wrapper"><pre><code class="hljs gradle">大表和小表进行<span class="hljs-keyword">JOIN</span>将小表数据采用广播变量的方式，把数据广播到大表的每个分区中，进行<span class="hljs-keyword">JOIN</span></code></pre></div><p><img src="https://i.loli.net/2021/09/07/yG1Qb5OcD7U3du8.png" alt="图8"></p><div class="code-wrapper"><pre><code class="hljs sql">Broadcast <span class="hljs-keyword">Join</span>的条件有以下几个：<span class="hljs-number">1</span>、被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的值，默认是<span class="hljs-number">10</span>M （或者加了broadcast <span class="hljs-keyword">join</span>的hint）<span class="hljs-number">2</span>、基表不能被广播，比如<span class="hljs-keyword">left</span> <span class="hljs-keyword">outer</span> <span class="hljs-keyword">join</span>时，只能广播右表</code></pre></div><h3 id="第二、Shuffle-Hash-Join"><a href="#第二、Shuffle-Hash-Join" class="headerlink" title="第二、Shuffle Hash Join"></a>第二、Shuffle Hash Join</h3><div class="code-wrapper"><pre><code class="hljs pf">大表与小表（数据量也挺大，相对大表来说，小一点）对<span class="hljs-number">2</span>个表的数据按照相同的字段进行重分区，并且分区数目相同Shuffle ：对<span class="hljs-number">2</span>个表的数据进行重分区，字段相同，分区数目相同<span class="hljs-built_in">table</span>A表<span class="hljs-number">1</span>个分区  仅仅   <span class="hljs-built_in">table</span>B表<span class="hljs-number">1</span>个分区  数据进行JOIN</code></pre></div><p><img src="https://i.loli.net/2021/09/07/nHlFXP6vsZEiqz1.png" alt="图9"></p><h3 id="第三、Sort-Merge-Join"><a href="#第三、Sort-Merge-Join" class="headerlink" title="第三、Sort Merge Join"></a>第三、Sort Merge Join</h3><div class="code-wrapper"><pre><code class="hljs vbnet">大表对大表类似Hive中SMB <span class="hljs-keyword">JOIN</span>，表的数据分区相同、分桶数目相同，并且桶数据排序的首先，<span class="hljs-number">2</span>张表进行重分区，分区字段<span class="hljs-keyword">key</span>相同，分区数目相同然后，对每个分区中数据进行排序最后，表对表的 分区数据进行<span class="hljs-keyword">JOIN</span>，此时关联时，不会扫描整个分区中数据</code></pre></div><p><img src="https://i.loli.net/2021/09/07/EYsfzkWZe4Hwl5t.png" alt="图10"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Spark：分布式并行计算框架（一）</title>
    <link href="/2020/09/22/Spark-Review-Day01/"/>
    <url>/2020/09/22/Spark-Review-Day01/</url>
    
    <content type="html"><![CDATA[<h2 id="0、前言说明"><a href="#0、前言说明" class="headerlink" title="0、前言说明"></a>0、前言说明</h2><p>整理和汇总一下 Spark 容易混淆的概念和理论。</p><h2 id="1、Spark-框架概念"><a href="#1、Spark-框架概念" class="headerlink" title="1、Spark 框架概念"></a>1、Spark 框架概念</h2><p><img src="https://i.loli.net/2021/09/06/LBadXZ5iyTtYvSg.png" alt="图1"></p><div class="code-wrapper"><pre><code class="hljs fortran">Apache Spark™ is a unified analytics engine for large-<span class="hljs-built_in">scale</span> <span class="hljs-keyword">data</span> processing.<span class="hljs-number">1</span>、unified 统一Spark 框架可以对任意业务需求进行数据分析批处理：SparkCore、交互式分析：SparkSQL、流式计算：SparkStreaming和StructuredStreaming图计算：SparkGraphX、机器学习：SparkMLlib数据科学：PySpark和SparkR<span class="hljs-number">2</span>、 large-<span class="hljs-built_in">scale</span> 大规模海量数据类似MapReduce对海量数据处理分析</code></pre></div><p><img src="https://i.loli.net/2021/09/06/h8eibz9poSO13Pu.png" alt="图2"></p><div class="code-wrapper"><pre><code class="hljs applescript">unified 统一不仅仅是分析，也可以指：Spark 分析数据时，可以时任意数据源SparkSQL提供一套外部数据源接口，任何数据源只要实现接口，可以读写数据<span class="hljs-built_in">read</span>或<span class="hljs-built_in">write</span></code></pre></div><h2 id="2、Spark与MapReduce相比"><a href="#2、Spark与MapReduce相比" class="headerlink" title="2、Spark与MapReduce相比"></a>2、Spark与MapReduce相比</h2><blockquote><p>面试题：Spark 与MapReduce相比为什么快，有什么不同？？？？</p></blockquote><div class="code-wrapper"><pre><code class="hljs arduino">其一、Spark处理数据时，可以将中间处理结果数据存储到内存中；第二、Spark Job调度以DAG方式，并且每个任务<span class="hljs-built_in">Task</span>执行以线程（Thread）方式，并不是像MapReduce以进程（<span class="hljs-built_in">Process</span>）方式执行。</code></pre></div><p><img src="https://i.loli.net/2021/09/06/XtxmcTysVnEBSrR.png" alt="图3"></p><blockquote><p>Spark与MapReduce最大不同：<a href="">提供数据结构RDD弹性分布式数据集。</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/NpW5Pehzy2AwM7J.png" alt="图4"></p><h2 id="3、RDD是什么，如何理解"><a href="#3、RDD是什么，如何理解" class="headerlink" title="3、RDD是什么，如何理解"></a>3、RDD是什么，如何理解</h2><blockquote><p>1、RDD是什么，官方定义：<a href="">不可变、分区的、并行计算的集合，抽象概念</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/sp8T1F39igfHqWv.png" alt="图5"></p><blockquote><p>2、每个RDD内部有5个特性</p><ul><li>分区组成、每个分区被计算处理、依赖一些列RDD</li><li>可选的：KeyValue类型RDD可以设置分区器，对每个分区数据处理时找到最佳位置（数据本地性）</li></ul></blockquote><p><img src="https://i.loli.net/2021/09/06/Nkbi3qxwF1fnEcO.png" alt="图6"></p><blockquote><p>3、常见RDD，<a href="">RDD抽象类，泛型，表示具体存储数据类型未知，可以是任何类型。</a></p></blockquote><p><img src="https://i.loli.net/2021/09/06/OKHzQcLeiv2uEfk.png" alt="图7"></p><div class="code-wrapper"><pre><code class="hljs arduino"><span class="hljs-number">1</span>、HadoopRDD，表示从文件系统加载数据封装的集合<span class="hljs-number">2</span>、MapPartitionsRDD，表示经过转换后的，比如fliter、map、flatMap等<span class="hljs-number">3</span>、ShuffleRDD，表示对RDD数据进行处理，产生shuffle时RDD</code></pre></div><h2 id="4、RDD、DataFrame和DataSet区别"><a href="#4、RDD、DataFrame和DataSet区别" class="headerlink" title="4、RDD、DataFrame和DataSet区别"></a>4、RDD、DataFrame和DataSet区别</h2><div class="code-wrapper"><pre><code class="hljs mathematica"><span class="hljs-number">1</span>、<span class="hljs-variable">RDD</span>是啥？？不可变、分区的、并行计算基本<span class="hljs-number">2</span>、<span class="hljs-variable">DataFrame</span><span class="hljs-variable">DataFrame</span> <span class="hljs-operator">=</span> <span class="hljs-variable">RDD</span><span class="hljs-punctuation">[</span><span class="hljs-built_in">Row</span><span class="hljs-punctuation">]</span> <span class="hljs-operator">+</span> <span class="hljs-variable">Schema</span>（字段名称、字段类型）知道内部结构当在处理分析数据，由于知道内部结构，所以可以先进行优化，在计算分析数据<span class="hljs-number">3</span>、<span class="hljs-built_in">Dataset</span><span class="hljs-variable">Spark</span> <span class="hljs-number">1.6</span>诞生<span class="hljs-built_in">Dataset</span> <span class="hljs-operator">=</span> <span class="hljs-variable">RDD</span> <span class="hljs-operator">+</span> <span class="hljs-variable">Schema</span>知道内部结构，也知道外部结构，编程更加安全和方便，内部数据存储使用特殊编码方式，节省空间<span class="hljs-number">4</span>、<span class="hljs-variable">Spark</span> <span class="hljs-number">2.0</span>开始<span class="hljs-variable">DataFrame</span>和<span class="hljs-built_in">Dataset</span>合并，其中<span class="hljs-variable">DataFrame</span>时<span class="hljs-built_in">Dataset</span>特殊形式，数据类型为<span class="hljs-built_in">Row</span>时数据结构<span class="hljs-variable">DataFrame</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">Dataset</span><span class="hljs-punctuation">[</span><span class="hljs-built_in">Row</span><span class="hljs-punctuation">]</span></code></pre></div><p><img src="https://i.loli.net/2021/09/06/iRcO1XegMI4rwN7.png" alt="图8"></p><blockquote><p>从Spark 2.x开始，建议大家使用数据结构为Dataset/DataFrame，因此使用SparkSQL模块分析数据。</p></blockquote><h2 id="5、Spark-on-YARN执行流程"><a href="#5、Spark-on-YARN执行流程" class="headerlink" title="5、Spark on YARN执行流程"></a>5、Spark on YARN执行流程</h2><blockquote><p>将Spark 应用程序（无论批处理还是流计算），提交运行到YARN集群上，提交流程。</p></blockquote><div class="code-wrapper"><pre><code class="hljs haml">Spark Application运行有2种DeployMode：1、client客户端DriverProgram运行在提交运行客户端主机上-<span class="ruby"> DriverProgram，调度Job执行</span><span class="ruby"></span>-<span class="ruby"> AppMaster，运行在NM中容器，申请资源运行Executors</span><span class="ruby"></span>-<span class="ruby"> Executors，运行在NM中容器，执行Task任务和缓存数据</span><span class="ruby"></span>2、cluster集群DriverProgram运行在YARN集群NodeManager容器中-<span class="ruby"> AppMaster/DriverProgram，运行在NM中容器，申请资源运行Executors和Job调度执行</span><span class="ruby"></span>-<span class="ruby"> Executors，运行在NM中容器，执行Task任务和缓存数据</span></code></pre></div><blockquote><p><code>yarn-client</code>，测试使用</p></blockquote><p><img src="https://i.loli.net/2021/09/06/Y9Uv8x61aCOydqp.png" alt="图9"></p><blockquote><p><code>yarn-cluster</code>，生成环境运行方式</p></blockquote><p><img src="https://i.loli.net/2021/09/06/hpHweAk9f4JGtNU.png" alt="图10"></p><p><img src="https://i.loli.net/2021/09/06/AthRJxlM8Ir34d9.png" alt="图11"></p><h2 id="6、SparkJob调度过程"><a href="#6、SparkJob调度过程" class="headerlink" title="6、SparkJob调度过程"></a>6、SparkJob调度过程</h2><blockquote><p>词频统计WordCount程序执行DAG图</p></blockquote><p><img src="https://i.loli.net/2021/09/06/UcAPhtBTkDafM9x.png" alt="图12"></p><div class="code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>、RDD<span class="hljs-comment">#Action函数，触发一个Job执行</span><span class="hljs-built_in">count</span>、foreach、foreachPartition等等<span class="hljs-number">2</span>、第一步、构建DAG图从触发<span class="hljs-keyword">Job开始RDD，采用回溯法（倒推法，从后向前），依据RDD依赖关系，构建Job对应DAG图</span><span class="hljs-keyword"></span>【依赖、回溯法】<span class="hljs-number">3</span>、第二步、划分DAG图为Stage从触发<span class="hljs-keyword">Job开始RDD，采用回溯法，如果2个RDD之间依赖为宽依赖，划分后面为一个Stage，依次类推</span><span class="hljs-keyword"></span>DAGScheduler划分完成以后，此时每个<span class="hljs-keyword">Job由多个Stage组成，各个Stage之间相互依赖关系</span><span class="hljs-keyword"></span>后面的Stage处理前面Stage的数据相邻<span class="hljs-number">2</span>个Stage之间产生<span class="hljs-keyword">Shuffle</span><span class="hljs-keyword"></span>Stage划分为<span class="hljs-number">2</span>种类型：第一、ResultStage，产生Result结果，每个<span class="hljs-keyword">Job中最后一个Stage</span><span class="hljs-keyword"></span>第二、<span class="hljs-keyword">ShuffleMapStage，除去Job中最后一个Stage其他Stage都是</span><span class="hljs-keyword"></span><span class="hljs-number">4</span>、第三步、按照<span class="hljs-keyword">Job中Stage顺序，从前向后执行Stage中Task任务</span><span class="hljs-keyword"></span>每个Stage中有多个Task任务，逻辑相同，处理数据不同而已将Stage中Task任务打包为TaskSet，发送给Executor执行TaskScheduler问题<span class="hljs-number">1</span>：每个Stage中Task数目如何确定？？由Stage中最后一个RDD分区数目确定问题<span class="hljs-number">2</span>：每个Stage中Task任务计算模式是什么？？？？管道计算模式，Pipeline模式</code></pre></div><p><img src="https://i.loli.net/2021/09/06/sJbErFfSDTlMBGp.png" alt="图13"></p><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> cn.test.spark<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span><span class="hljs-keyword">import</span> org.apache.spark.&#123;<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>&#125;<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SparkWordCount</span> </span>&#123;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<span class="hljs-keyword">val</span> sc: <span class="hljs-type">SparkContext</span> = <span class="hljs-type">SparkContext</span>.getOrCreate(<span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">&quot;local[1]&quot;</span>).setAppName(<span class="hljs-string">&quot;SparkWordCount&quot;</span>))<span class="hljs-keyword">val</span> inputRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.parallelize(<span class="hljs-type">Seq</span>(<span class="hljs-string">&quot;111111111111111111&quot;</span>, <span class="hljs-string">&quot;222222222222222222&quot;</span>, <span class="hljs-string">&quot;3333333333333333333&quot;</span>), numSlices = <span class="hljs-number">2</span>)println(<span class="hljs-string">s&quot;RDD 分区数目：<span class="hljs-subst">$&#123;inputRDD.getNumPartitions&#125;</span>&quot;</span>)<span class="hljs-keyword">val</span> resultRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = inputRDD.filter(line =&gt; &#123;println(<span class="hljs-string">&quot;filter........................&quot;</span>)<span class="hljs-comment">// 直接返回</span><span class="hljs-literal">true</span>&#125;).flatMap(line =&gt; &#123;println(<span class="hljs-string">&quot;flatMap........................&quot;</span>)<span class="hljs-type">Seq</span>(line)&#125;).map(line =&gt; &#123;println(<span class="hljs-string">&quot;map........................&quot;</span>)line&#125;)<span class="hljs-comment">/*</span><span class="hljs-comment">filter........................</span><span class="hljs-comment">flatMap........................</span><span class="hljs-comment">map........................</span><span class="hljs-comment"></span><span class="hljs-comment">filter........................</span><span class="hljs-comment">flatMap........................</span><span class="hljs-comment">map........................</span><span class="hljs-comment"></span><span class="hljs-comment">filter........................</span><span class="hljs-comment">flatMap........................</span><span class="hljs-comment">map........................</span><span class="hljs-comment"> */</span><span class="hljs-keyword">val</span> count = resultRDD.count()println(<span class="hljs-string">s&quot;Count = <span class="hljs-subst">$&#123;count&#125;</span>&quot;</span>)&#125;&#125;</code></pre></div><p><img src="https://i.loli.net/2021/09/06/pX9NdjS1q7s2yzM.png" alt="图14"></p><h2 id="7、Spark中依赖类型"><a href="#7、Spark中依赖类型" class="headerlink" title="7、Spark中依赖类型"></a>7、Spark中依赖类型</h2><blockquote><p>主要RDD依赖分为2类：</p><ul><li><strong>窄依赖</strong>：1对1，父RDD1个分区数据对应子RDD1个分区数据</li><li><code>宽依赖</code>：1对多，父RDD1个分区数据对应子RDD多个分区数据</li></ul></blockquote><p><img src="https://i.loli.net/2021/09/06/uPGhXzADYrBcZOw.png" alt="图15"></p><div class="code-wrapper"><pre><code class="hljs mipsasm">宽依赖：<span class="hljs-keyword">Shuffle依赖</span><span class="hljs-keyword"></span>相邻RDD产生<span class="hljs-keyword">Shuffle</span><span class="hljs-keyword"></span>Spark <span class="hljs-keyword">Shuffle </span>分为<span class="hljs-number">2</span>个部分：<span class="hljs-number">1</span>、<span class="hljs-keyword">Shuffle </span>Writer（上游Stage产生）将<span class="hljs-keyword">Shuffle数据写磁盘，有3种方式，具体由底层自动选择</span><span class="hljs-keyword"></span><span class="hljs-number">2</span>、<span class="hljs-keyword">ShuffleReader（下游Stage产生）</span><span class="hljs-keyword"></span>读取<span class="hljs-keyword">Shuffle到磁盘中数据，进行处理</span></code></pre></div><p><img src="https://i.loli.net/2021/09/06/2Wmqk5Rigp63oVG.png" alt="图16"></p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive自动化建库建表</title>
    <link href="/2020/09/12/Hive%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BB%BA%E5%BA%93%E5%BB%BA%E8%A1%A8/"/>
    <url>/2020/09/12/Hive%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BB%BA%E5%BA%93%E5%BB%BA%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>项目数仓数据源太多，于是自己写了一个工具类，读取数据源的元数据信息，自动建库建表</p><p>以 MySQL 为例，代码如下。</p><h3 id="HiveUtil"><a href="#HiveUtil" class="headerlink" title="HiveUtil"></a>HiveUtil</h3><div class="code-wrapper"><pre><code class="hljs java">object HiveUtil &#123;  <span class="hljs-function">def <span class="hljs-title">main</span><span class="hljs-params">(args: Array[String])</span>: Unit </span>= &#123;    createHiveTable()  &#125;  <span class="hljs-function">def <span class="hljs-title">createHiveTable</span><span class="hljs-params">()</span> </span>= &#123;    <span class="hljs-comment">//连接MySQL，读取MySQL表名有哪些字段，字段类型，字段的注释</span>    val table_arr = Array(      <span class="hljs-string">&quot;area&quot;</span>,      <span class="hljs-string">&quot;claim_info&quot;</span>,      <span class="hljs-string">&quot;dd_table&quot;</span>,      <span class="hljs-string">&quot;mort_10_13&quot;</span>,      <span class="hljs-string">&quot;policy_acuary&quot;</span>,      <span class="hljs-string">&quot;policy_benefit&quot;</span>,      <span class="hljs-string">&quot;policy_client&quot;</span>,      <span class="hljs-string">&quot;policy_surrender&quot;</span>,      <span class="hljs-string">&quot;pre_add_exp_ratio&quot;</span>,      <span class="hljs-string">&quot;prem_cv_real&quot;</span>,      <span class="hljs-string">&quot;prem_std_real&quot;</span>)    val conn = DriverManager.getConnection(<span class="hljs-string">&quot;jdbc:mysql://node3:3306/insurance&quot;</span>, <span class="hljs-string">&quot;root&quot;</span>, <span class="hljs-string">&quot;123456&quot;</span>)    val ps: PreparedStatement = conn.prepareStatement(      s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><span class="hljs-string">         |SELECT</span><span class="hljs-string">         |       COLUMN_NAME,</span><span class="hljs-string">         |       COLUMN_TYPE,</span><span class="hljs-string">         |       COLUMN_COMMENT</span><span class="hljs-string">         |FROM information_schema.COLUMNS</span><span class="hljs-string">         |WHERE upper(TABLE_NAME)  = upper(?)</span><span class="hljs-string">         |  and upper(TABLE_SCHEMA)=upper(?)</span><span class="hljs-string">         |order by ORDINAL_POSITION</span><span class="hljs-string">         |&quot;</span><span class="hljs-string">&quot;&quot;</span>.stripMargin)    <span class="hljs-keyword">var</span> rs: ResultSet = <span class="hljs-function"><span class="hljs-keyword">null</span></span><span class="hljs-function">    <span class="hljs-title">for</span> <span class="hljs-params">(tablename &lt;- table_arr)</span> </span>&#123;      ps.setString(<span class="hljs-number">1</span>,tablename)      ps.setString(<span class="hljs-number">2</span>,<span class="hljs-string">&quot;insurance&quot;</span>)      rs = ps.executeQuery()      <span class="hljs-keyword">var</span> str =        s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><span class="hljs-string">           |drop table if exists $&#123;tablename&#125;;</span><span class="hljs-string">           |create table if not exists $&#123;tablename&#125; (\n&quot;</span><span class="hljs-string">&quot;&quot;</span>.<span class="hljs-function">stripMargin</span><span class="hljs-function">      <span class="hljs-title">while</span> <span class="hljs-params">(rs.next()</span>) </span>&#123;        val column_name: String = rs.getString(<span class="hljs-number">1</span>)        val column_type: String = rs.getString(<span class="hljs-number">2</span>)        val column_comment: String = rs.getString(<span class="hljs-number">3</span>)        <span class="hljs-keyword">var</span> temp_type = <span class="hljs-function">column_type</span><span class="hljs-function">        <span class="hljs-title">if</span> <span class="hljs-params">(temp_type.contains(<span class="hljs-string">&quot;int&quot;</span>)</span>) </span>&#123;          <span class="hljs-comment">//30000-&gt;int(5)-&gt;smallint</span>          <span class="hljs-comment">//300000000-&gt;int(5-16)-&gt;int</span>          <span class="hljs-comment">//3000000000000000000-&gt;int(16-32)-&gt;bigint</span>          val <span class="hljs-keyword">int</span>: Int = <span class="hljs-string">&quot;int(11)&quot;</span>.split(<span class="hljs-string">&quot;\\(|\\)&quot;</span>)(<span class="hljs-number">1</span>).<span class="hljs-function">toInt</span><span class="hljs-function">          <span class="hljs-title">if</span> <span class="hljs-params">(<span class="hljs-keyword">int</span> &gt; <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-keyword">int</span> &lt;= <span class="hljs-number">5</span>)</span> </span>&#123;            temp_type = <span class="hljs-string">&quot;smallint&quot;</span>          &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-keyword">int</span> &gt; <span class="hljs-number">5</span> &amp;&amp; <span class="hljs-keyword">int</span> &lt;= <span class="hljs-number">16</span>) &#123;            temp_type = <span class="hljs-string">&quot;int&quot;</span>          &#125; <span class="hljs-keyword">else</span> &#123;            temp_type = <span class="hljs-string">&quot;bigint&quot;</span>          &#125;        &#125;        <span class="hljs-keyword">if</span>(temp_type.contains(<span class="hljs-string">&quot;varchar&quot;</span>) || temp_type.contains(<span class="hljs-string">&quot;text&quot;</span>))&#123;          temp_type=<span class="hljs-string">&quot;string&quot;</span>        &#125;        <span class="hljs-comment">//println(column_name,column_type,column_comment)</span>        str += s<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;$&#123;column_name&#125;   $&#123;temp_type&#125;  comment &#x27;$&#123;column_comment&#125;&#x27;,\n&quot;</span><span class="hljs-string">&quot;&quot;</span>      &#125;      str = str.stripSuffix(<span class="hljs-string">&quot;,\n&quot;</span>)      str += <span class="hljs-string">&quot;) comment &#x27;&#x27; \n row format delimited fields terminated by &#x27;\\t&#x27; ; \n&quot;</span>      println(str)    &#125;    rs.close()    ps.close()    conn.close()    <span class="hljs-comment">//解析上面的元数据，拼接成hive版的ddl语句</span>  &#125;&#125;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sqoop自动化抽取数据与验证</title>
    <link href="/2020/09/12/Sqoop%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AA%8C%E8%AF%81/"/>
    <url>/2020/09/12/Sqoop%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AA%8C%E8%AF%81/</url>
    
    <content type="html"><![CDATA[<h3 id="前言说明"><a href="#前言说明" class="headerlink" title="前言说明"></a>前言说明</h3><p>最近项目业务数据源多种多样，用 Sqoop 抽取数据到数仓是一个体力活，底层又是基于 MapReduce 执行的，速度感人，关键是还得做数据校验</p><p>于是想着自己写个工具类，和自动建表建库类似，自动读取数据源表和字段信息，创建对应脚本，扔到 DolphinScheduler 上自动跑就完事。</p><h3 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h3><p>1. 自定义工具类，读取 MySQL 中 information_schema 库下的 TABLES 表 获取同名的表</p><p>2. 获取到表名的容器，然后按照固定格式以文本形式写到 HDFS上文件夹上</p><p>3. 脚本内容需要做数据校验并将校验结果，并且加上并行执行符号</p><p>4. DolphinScheduler 上新建工作流，定期执行脚本文件</p><h3 id="Sqoop-数据抽取脚本"><a href="#Sqoop-数据抽取脚本" class="headerlink" title="Sqoop 数据抽取脚本"></a>Sqoop 数据抽取脚本</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1</code></pre></div><h3 id="Sqoop-抽取数据-数据验证"><a href="#Sqoop-抽取数据-数据验证" class="headerlink" title="Sqoop 抽取数据+数据验证"></a>Sqoop 抽取数据+数据验证</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1<span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span>`mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<span class="hljs-comment">#2、查询hive的表dd_table的条数</span>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<span class="hljs-comment">#3、比较2边的数字是否一样。</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><span class="hljs-keyword">else</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><span class="hljs-keyword">fi</span></code></pre></div><h3 id="SqoopUtil"><a href="#SqoopUtil" class="headerlink" title="SqoopUtil"></a>SqoopUtil</h3><div class="code-wrapper"><pre><code class="hljs sql">object SqoopUtil &#123;  def main(args: <span class="hljs-keyword">Array</span>[String]): Unit <span class="hljs-operator">=</span> &#123;    createHiveTable()  &#125;  def createHiveTable() <span class="hljs-operator">=</span> &#123;    <span class="hljs-operator">/</span><span class="hljs-operator">/</span>连接MySQL，读取MySQL表名有哪些字段，字段类型，字段的注释    val table_arr <span class="hljs-operator">=</span> <span class="hljs-keyword">Array</span>(      &quot;area&quot;,      &quot;policy_acuary&quot;,      &quot;policy_benefit&quot;,      &quot;policy_client&quot;,      &quot;policy_surrender&quot;,    <span class="hljs-keyword">for</span> (tablename <span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span> table_arr) &#123;      <span class="hljs-operator">/</span><span class="hljs-operator">/</span>var str <span class="hljs-operator">=</span>      <span class="hljs-operator">/</span><span class="hljs-operator">/</span>  s&quot;&quot;&quot;/export/server/sqoop/bin/sqoop import  --connect jdbc:mysql://192.168.88.163:3306/insurance  --username root  --password 123456  --table $&#123;tablename&#125;  --hive-table insurance_ods.$&#123;tablename&#125;  --hive-import  --hive-overwrite  --fields-terminated-by &#x27;\\t&#x27;  -m 1 \n&quot;&quot;&quot;.stripMargin      var str1 <span class="hljs-operator">=</span>        s&quot;&quot;&quot;/export/server/sqoop/bin/sqoop import  \\           |--connect jdbc:m1ysql://192.168.88.163:3306/insurance  \\           |--username root  \\           |--password 123456  \\           |--table $&#123;tablename&#125;  \\           |--hive-table insurance_ods.$&#123;tablename&#125;  \\           |--hive-import  \\           |--hive-overwrite  \\           |--fields-terminated-by &#x27;\\t&#x27;  \\           |-m 1&quot;&quot;&quot;.stripMargin      println(str1)    &#125;  &#125;&#125;</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>数据仓库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>Sqoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据常用脚本</title>
    <link href="/2020/06/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"/>
    <url>/2020/06/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<h2 id="一键启动"><a href="#一键启动" class="headerlink" title="一键启动"></a>一键启动</h2><h3 id="一键启动常用服务"><a href="#一键启动常用服务" class="headerlink" title="一键启动常用服务"></a>一键启动常用服务</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-keyword">if</span> [ ! <span class="hljs-variable">$1</span> ]<span class="hljs-keyword">then</span> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;please input [start|stop]&quot;</span><span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span><span class="hljs-comment">#start hadoop</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> dfs ------------ &quot;</span>ssh root@node01 <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;HADOOP_HOME&#125;</span>/sbin/<span class="hljs-variable">$&#123;1&#125;</span>-dfs.sh&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> yarn ---------- &quot;</span>ssh root@node01 <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;HADOOP_HOME&#125;</span>/sbin/<span class="hljs-variable">$&#123;1&#125;</span>-yarn.sh&quot;</span>sleep 1s<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> zookeeper ----------&quot;</span><span class="hljs-comment">#start zookeeper</span><span class="hljs-keyword">for</span> (( i=1; i&lt;=3; i++ ))<span class="hljs-keyword">do</span><span class="hljs-comment"># &lt;&lt; E0F 只是一个标识，可以换做其他任意字符，多行复杂脚本使用</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> zk <span class="hljs-variable">$&#123;1&#125;</span> ...&quot;</span>  ssh root@node0<span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile; zkServer.sh <span class="hljs-variable">$&#123;1&#125;</span>&quot;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> <span class="hljs-variable">$&#123;1&#125;</span> 完成.&quot;</span><span class="hljs-keyword">done</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; ----------- <span class="hljs-variable">$1</span> kafka ------------&quot;</span><span class="hljs-comment"># start kafka</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$&#123;1&#125;</span> == <span class="hljs-string">&#x27;stop&#x27;</span> ]<span class="hljs-keyword">then</span><span class="hljs-keyword">for</span> (( i=1; i&lt;=3; i++ ))<span class="hljs-keyword">do</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> kafka <span class="hljs-variable">$&#123;1&#125;</span> ...&quot;</span>  ssh root@node0<span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-stop.sh&quot;</span>  <span class="hljs-keyword">if</span> [ `ps -ef|grep Kafka | wc -l` -gt 1 ]; <span class="hljs-keyword">then</span>  ssh root@node0<span class="hljs-variable">$i</span> `ps -ef | grep Kafka | grep -v grep | awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span> | xargs <span class="hljs-built_in">kill</span> -9`  <span class="hljs-keyword">fi</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> <span class="hljs-variable">$&#123;1&#125;</span> 完成.&quot;</span><span class="hljs-keyword">done</span><span class="hljs-keyword">else</span><span class="hljs-keyword">for</span> (( i=1; i&lt;=3; i++ ))<span class="hljs-keyword">do</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> kafka <span class="hljs-variable">$&#123;1&#125;</span> ...&quot;</span>  ssh root@node0<span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-<span class="hljs-variable">$&#123;1&#125;</span>.sh -daemon /export/servers/kafka/config/server.properties&quot;</span>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;node0<span class="hljs-variable">$i</span> <span class="hljs-variable">$&#123;1&#125;</span> 完成.&quot;</span><span class="hljs-keyword">done</span>sleep 1s<span class="hljs-keyword">fi</span><span class="hljs-comment"># start flink</span><span class="hljs-comment"># /export/servers/flink/bin/$&#123;1&#125;-cluster.sh</span><span class="hljs-comment"># start dolphinscheduler</span><span class="hljs-comment"># /opt/soft/dolphinscheduler/bin/start-all.sh</span><span class="hljs-comment"># systemctl restart nginx</span></code></pre></div><h3 id="查看服务启动情况"><a href="#查看服务启动情况" class="headerlink" title="查看服务启动情况"></a>查看服务启动情况</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> node01 node02 node03<span class="hljs-keyword">do</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot; &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; <span class="hljs-variable">$i</span> <span class="hljs-variable">$1</span> &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&quot;</span>ssh <span class="hljs-variable">$i</span> <span class="hljs-string">&quot;source /etc/profile;$*&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h2 id="Kafka-启动与关闭"><a href="#Kafka-启动与关闭" class="headerlink" title="Kafka 启动与关闭"></a>Kafka 启动与关闭</h2><h3 id="Kafka-一键启动"><a href="#Kafka-一键启动" class="headerlink" title="Kafka 一键启动"></a>Kafka 一键启动</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>        host=node<span class="hljs-variable">$&#123;number&#125;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>        /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;export JMX_PORT=9988;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-start.sh <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> started&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="Kafka-一键关闭"><a href="#Kafka-一键关闭" class="headerlink" title="Kafka 一键关闭"></a>Kafka 一键关闭</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>  host=node<span class="hljs-variable">$&#123;number&#125;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>  /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-stop.sh&quot;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> stoped&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><h3 id="Zookeeper-启动"><a href="#Zookeeper-启动" class="headerlink" title="Zookeeper 启动"></a>Zookeeper 启动</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>ZK_HOME=/<span class="hljs-built_in">export</span>/server/zookeeper-3.4.6<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>host=node<span class="hljs-variable">$&#123;number&#125;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>/usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;ZK_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;ZK_HOME&#125;</span>/bin/zkServer.sh start&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> started&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="Zookeeper-停止"><a href="#Zookeeper-停止" class="headerlink" title="Zookeeper 停止"></a>Zookeeper 停止</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>ZK_HOME=/<span class="hljs-built_in">export</span>/server/zookeeper-3.4.6<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>host=node<span class="hljs-variable">$&#123;number&#125;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>/usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;ZK_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;ZK_HOME&#125;</span>/bin/zkServer.sh stop&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> stoped&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="Zookeeper-查看状态"><a href="#Zookeeper-查看状态" class="headerlink" title="Zookeeper 查看状态"></a>Zookeeper 查看状态</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>ZK_HOME=/<span class="hljs-built_in">export</span>/server/zookeeper-3.4.6<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>host=node<span class="hljs-variable">$&#123;number&#125;</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>/usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;ZK_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;ZK_HOME&#125;</span>/bin/zkServer.sh status&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h2 id="Sqoop数据抽取与验证"><a href="#Sqoop数据抽取与验证" class="headerlink" title="Sqoop数据抽取与验证"></a>Sqoop数据抽取与验证</h2><h3 id="数据抽取"><a href="#数据抽取" class="headerlink" title="数据抽取"></a>数据抽取</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1</code></pre></div><h3 id="抽取与验证"><a href="#抽取与验证" class="headerlink" title="抽取与验证"></a>抽取与验证</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1<span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span>`mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<span class="hljs-comment">#2、查询hive的表dd_table的条数</span>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<span class="hljs-comment">#3、比较2边的数字是否一样。</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><span class="hljs-keyword">else</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><span class="hljs-keyword">fi</span></code></pre></div><h2 id="Hive-加载分区数据"><a href="#Hive-加载分区数据" class="headerlink" title="Hive 加载分区数据"></a>Hive 加载分区数据</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>dt=`date -d <span class="hljs-string">&#x27;1 days ago&#x27;</span> +<span class="hljs-string">&#x27;%Y%m%d&#x27;</span>`tableName=<span class="hljs-variable">$1</span>ssh node03 `/<span class="hljs-built_in">export</span>/server/hive/bin/hive -e <span class="hljs-string">&quot;use test_ods;alter table <span class="hljs-variable">$&#123;tableName&#125;</span> add partition(dt=<span class="hljs-variable">$&#123;dt&#125;</span>) location &#x27;hdfs://node1:8020/apps/warehouse/ods.db/<span class="hljs-variable">$&#123;tableName&#125;</span>/<span class="hljs-variable">$&#123;dt&#125;</span>&quot;</span>`<span class="hljs-keyword">if</span> [ $? -eq 0 ]; <span class="hljs-keyword">then</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;load <span class="hljs-variable">$tableName</span> partition <span class="hljs-variable">$dt</span> succesful.&quot;</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;load <span class="hljs-variable">$tableName</span> partition <span class="hljs-variable">$dt</span> error.&quot;</span><span class="hljs-keyword">fi</span></code></pre></div>]]></content>
    
    
    <categories>
      
      <category>存档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>Spark</tag>
      
      <tag>Sqoop</tag>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多表连接过滤条件在 on 和 where 的区别</title>
    <link href="/2020/05/24/%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5%E8%BF%87%E6%BB%A4%E6%9D%A1%E4%BB%B6%E5%9C%A8%20on%20%E5%92%8C%20where%20%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <url>/2020/05/24/%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5%E8%BF%87%E6%BB%A4%E6%9D%A1%E4%BB%B6%E5%9C%A8%20on%20%E5%92%8C%20where%20%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h3 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h3><p>最近项目中的小坑，记录一下。</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(    sid   <span class="hljs-type">int</span> <span class="hljs-keyword">primary</span> key  <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ,    cid   <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>,    t_sex <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">null</span>)    comment <span class="hljs-string">&#x27;学生表&#x27;</span>;<span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> t_score(    sid    <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>,    course <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">null</span>,    score  <span class="hljs-type">int</span>         <span class="hljs-keyword">null</span>)    comment <span class="hljs-string">&#x27;成绩表&#x27;</span>;<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test.student <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;李白&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;杜甫&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;白居易&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;苏轼&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>),(<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;李清照&#x27;</span>,<span class="hljs-string">&#x27;女&#x27;</span>),(<span class="hljs-number">7</span>,<span class="hljs-string">&#x27;谢道韫&#x27;</span>,<span class="hljs-string">&#x27;女&#x27;</span>),(<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;郭奉孝&#x27;</span>,<span class="hljs-string">&#x27;男&#x27;</span>);<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test.t_score <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">90</span>),(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">50</span>),(<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-number">99</span>),(<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">6</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">7</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>),(<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;语文&#x27;</span>,<span class="hljs-keyword">null</span>);</code></pre></div><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><ul><li>在 on 后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 内关联 条件放在 on 和 where 没有区别</span><span class="hljs-comment">-- 非空判断放在 on 和 where没有区别，成绩表有只有3个人的成绩，只有3个结果</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid<span class="hljs-keyword">where</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/JIUxdHebiaFhlBz.png" alt="Untitled"></p><ul><li>在 where后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">join</span> t_score ts<span class="hljs-keyword">where</span> s.sid<span class="hljs-operator">=</span>ts.sid <span class="hljs-keyword">and</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/tkaNXALl8FzIiYT.png" alt="Untitled"></p><h3 id="外连接"><a href="#外连接" class="headerlink" title="外连接"></a>外连接</h3><ul><li>在 on 后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 外连接结果有非常大的区别</span><span class="hljs-comment">-- 写在 on 条件上,有7个结果</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid  <span class="hljs-keyword">and</span>  ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/AIrX7xsPdEaqUty.png" alt="Untitled"></p><ul><li>在 where 后面</li></ul><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-comment">-- 写在 where 条件上,只有3个结果</span><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span><span class="hljs-keyword">from</span> student s<span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> t_score ts <span class="hljs-keyword">on</span> s.sid <span class="hljs-operator">=</span> ts.sid<span class="hljs-keyword">where</span> ts.score <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> ;</code></pre></div><p><img src="https://i.loli.net/2021/09/03/u4tXn2UqxcTOp1z.png" alt="Untitled"></p><h3 id="原因解析"><a href="#原因解析" class="headerlink" title="原因解析"></a>原因解析</h3><p>left join 的时候全部保留左边表格的内容，并保留右边表格能匹配上条件的内容<br>on 后面的就是连接条件，无论写什么只会对右边起效，不影响左表内容<br>where 后面的条件是对全局起效，就表关联之后的结果做筛选。</p>]]></content>
    
    
    <categories>
      
      <category>Debug记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据常用命令</title>
    <link href="/2020/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2020/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><div class="code-wrapper"><pre><code class="hljs sql"># 启动service mysqld <span class="hljs-keyword">start</span>systemctl <span class="hljs-keyword">start</span> mysql[d]# 关闭service mysqld stop#设置mysql开机启动chkconfig mysqld <span class="hljs-keyword">on</span></code></pre></div><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 全局组件启动与停止</span>start-all.shstop-all.sh<span class="hljs-comment"># HDFS 启动与停止</span>start-dfs.shstop-dfs.sh<span class="hljs-comment"># Yarn 启动与停止</span>start-yarn.shstop-yarn.sh<span class="hljs-comment"># HDFS 单个启动</span>hadoop-daemon.sh start namenode <span class="hljs-comment"># HDFS 多个启动</span>hadoop-daemons.sh start datanode<span class="hljs-comment"># Yarn 单个启动</span>yarn-daemon.sh start resourcemanager <span class="hljs-comment"># Yarn 多个启动</span>yarn-daemons.sh start nodemanager<span class="hljs-comment"># MR 历史 job记录，端口号 19888</span>mr-jobhistory-daemon.sh start historyserver<span class="hljs-comment"># 退出安全模式</span>hadoop dfsadmin -safemode leave</code></pre></div><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 启动Hive 的元数据服务</span>nohup /<span class="hljs-built_in">export</span>/server/hive-2.1.0/bin/hive --service metastore   &amp;<span class="hljs-comment"># 启动Hive 客户端服务</span>nohup /<span class="hljs-built_in">export</span>/server/hive-2.1.0/bin/hiveserver2 start &amp;<span class="hljs-comment"># beeline</span>!connect jdbc:hive2://node03:10000<span class="hljs-comment"># hive元数据初始化和更新</span>schematool -dbType mysql -initSchemaschematool -dbType mysql -upgradeSchema<span class="hljs-comment"># 使用动态分区</span><span class="hljs-comment"># 开启动态分区</span><span class="hljs-built_in">set</span> hive.exec.dynamic.partition=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 开启非严格模式 </span><span class="hljs-built_in">set</span> hive.exec.dynamic.partition.mode=nonstrict;<span class="hljs-comment"># 每个节点生成动态分区的最大个数</span><span class="hljs-built_in">set</span> hive.exec.max.dynamic.partitions.pernode=10000;<span class="hljs-comment"># 生成动态分区的最大个数</span><span class="hljs-built_in">set</span> hive.exec.max.dynamic.partitions=100000;<span class="hljs-comment"># 一个任务最多可以创建的文件数目</span><span class="hljs-built_in">set</span> hive.exec.max.created.files=150000;<span class="hljs-comment"># 限定一次最多打开的文件数</span><span class="hljs-built_in">set</span> dfs.datanode.max.xcievers=8192;<span class="hljs-comment">## Hive基础优化内容</span><span class="hljs-comment"># hive压缩</span><span class="hljs-built_in">set</span> hive.exec.compress.intermediate=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.exec.compress.output=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 写入时压缩生效</span><span class="hljs-built_in">set</span> hive.exec.orc.compression.strategy=COMPRESSION;<span class="hljs-comment"># 分桶</span><span class="hljs-built_in">set</span> hive.enforce.bucketing=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.enforce.sorting=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.optimize.bucketmapjoin = <span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.auto.convert.sortmerge.join=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 并行执行</span><span class="hljs-built_in">set</span> hive.exec.parallel=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.exec.parallel.thread.number=8;<span class="hljs-comment"># 小文件合并</span>-- <span class="hljs-built_in">set</span> mapred.max.split.size=2147483648;-- <span class="hljs-built_in">set</span> mapred.min.split.size.per.node=1000000000;-- <span class="hljs-built_in">set</span> mapred.min.split.size.per.rack=1000000000;<span class="hljs-comment"># 矢量化查询</span><span class="hljs-built_in">set</span> hive.vectorized.execution.enabled=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 关联优化器</span><span class="hljs-built_in">set</span> hive.optimize.correlation=<span class="hljs-literal">true</span>;<span class="hljs-comment"># 读取零拷贝</span><span class="hljs-built_in">set</span> hive.exec.orc.zerocopy=<span class="hljs-literal">true</span>;<span class="hljs-comment"># join数据倾斜</span><span class="hljs-built_in">set</span> hive.optimize.skewjoin=<span class="hljs-literal">true</span>;-- <span class="hljs-built_in">set</span> hive.skewjoin.key=100000;<span class="hljs-built_in">set</span> hive.optimize.skewjoin.compiletime=<span class="hljs-literal">true</span>;<span class="hljs-built_in">set</span> hive.optimize.union.remove=<span class="hljs-literal">true</span>;<span class="hljs-comment"># group倾斜</span><span class="hljs-built_in">set</span> hive.groupby.skewindata=<span class="hljs-literal">false</span>;</code></pre></div><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 全局启动</span>zkServer.sh start<span class="hljs-comment"># 标准启动</span>zookeeper-daemon.sh start</code></pre></div><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="启动与停止"><a href="#启动与停止" class="headerlink" title="启动与停止"></a>启动与停止</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 启动 Kafka 启动服务，需要先启动 zookeeper</span>kafka-server-start.sh config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;<span class="hljs-comment"># 关闭 Kafka 服务</span>kafka-server-stop.sh</code></pre></div><h3 id="封装启动脚本-记得给权限"><a href="#封装启动脚本-记得给权限" class="headerlink" title="封装启动脚本, 记得给权限"></a>封装启动脚本, 记得给权限</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>        host=node<span class="hljs-variable">$&#123;number&#125;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>        /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;export JMX_PORT=9988;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-start.sh <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/config/server.properties &gt;&gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span>        <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> started&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="封装关闭脚本，记得给权限"><a href="#封装关闭脚本，记得给权限" class="headerlink" title="封装关闭脚本，记得给权限"></a>封装关闭脚本，记得给权限</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span>KAFKA_HOME=/<span class="hljs-built_in">export</span>/server/kafka_2.12-2.4.1<span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> &#123;1..3&#125;<span class="hljs-keyword">do</span>  host=node<span class="hljs-variable">$&#123;number&#125;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;host&#125;</span>  /usr/bin/ssh <span class="hljs-variable">$&#123;host&#125;</span> <span class="hljs-string">&quot;cd <span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>;source /etc/profile;<span class="hljs-variable">$&#123;KAFKA_HOME&#125;</span>/bin/kafka-server-stop.sh&quot;</span>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;host&#125;</span> stoped&quot;</span><span class="hljs-keyword">done</span></code></pre></div><h3 id="彻底删除-kafka-并初始化"><a href="#彻底删除-kafka-并初始化" class="headerlink" title="彻底删除 kafka 并初始化"></a>彻底删除 kafka 并初始化</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 1.检查 server.properties 配置文件中的 delete.topic.enable=true，所有节点都需要设置，生效需要重启</span><span class="hljs-comment"># 2.删除 kafka 中的 topic test_data</span>bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --delete --topic test_data<span class="hljs-comment">#3.打开 zkCli.sh 删除三组配置</span>rm-rf /brokers/topics/test_datarm-rf /config/topics/test_datarm-rf /admin/delete_topics/test_data<span class="hljs-comment">#4.如果 kafka 集群没有关闭，关闭集群</span><span class="hljs-comment">#5.清空 log.dirs=/export/data/kafka/kafka-logs 目录，也就是 kafka 集群的数据目录</span>rm -rf /<span class="hljs-built_in">export</span>/data/kafka/kafka-logs/*<span class="hljs-comment">#6.重启 kafka 集群</span><span class="hljs-comment">#7.重新创建新的 topic</span>bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --topic test_data -- partitions 3 --replication-factor 2</code></pre></div><h3 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h3><div class="code-wrapper"><pre><code class="hljs bash">kafka-topics.sh --zookeeper node3:2181 --create --topic spark_kafka --partitions 3 --replication-factor 1kafka-topics.sh --zookeeper node3:2181 --list</code></pre></div><h3 id="启动生产者和消费者"><a href="#启动生产者和消费者" class="headerlink" title="启动生产者和消费者"></a>启动生产者和消费者</h3><div class="code-wrapper"><pre><code class="hljs bash">kafka-console-producer.sh --broker-list node3:9092 --topic spark_kafkakafka-console-consumer.sh --from-beginning --bootstrap-server node3:9092 --topic spark_kafkakafka-console-consumer.sh --from-beginning --bootstrap-server node3:9092 --topic __consumer_offsets</code></pre></div><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="启动spark-thriftserver"><a href="#启动spark-thriftserver" class="headerlink" title="启动spark-thriftserver"></a>启动spark-thriftserver</h3><div class="code-wrapper"><pre><code class="hljs bash">start-thriftserver.sh \  --hiveconf hive.server2.thrift.port=10001 \  --hiveconf hive.server2.thrift.bind.host=node3 \  --master <span class="hljs-built_in">local</span>[*]</code></pre></div><h3 id="启动-Spark-HistoryServer服务-端口号-18080"><a href="#启动-Spark-HistoryServer服务-端口号-18080" class="headerlink" title="启动 Spark HistoryServer服务, 端口号 18080"></a>启动 Spark HistoryServer服务, 端口号 18080</h3><div class="code-wrapper"><pre><code class="hljs bash">sbin/start-history-server.sh</code></pre></div><h3 id="structured-Streaming"><a href="#structured-Streaming" class="headerlink" title="structured Streaming"></a>structured Streaming</h3><div class="code-wrapper"><pre><code class="hljs bash">--memory sinkCREATE TABLE db_spark.tb_word_count (  id int NOT NULL AUTO_INCREMENT,  word varchar(255) NOT NULL,  count int NOT NULL,  PRIMARY KEY (id),  UNIQUE KEY word (word)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;REPLACE INTO  tb_word_count (id, word, count) VALUES (NULL, ?, ?);</code></pre></div><h3 id="spark-yarn-Pi-测试"><a href="#spark-yarn-Pi-测试" class="headerlink" title="spark yarn Pi 测试"></a>spark yarn Pi 测试</h3><div class="code-wrapper"><pre><code class="hljs bash">/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \--master yarn \--class org.apache.spark.examples.SparkPi \<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \10</code></pre></div><h3 id="WordCount-yarn"><a href="#WordCount-yarn" class="headerlink" title="WordCount yarn"></a>WordCount yarn</h3><div class="code-wrapper"><pre><code class="hljs bash">/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \--master yarn \--driver-memory 512m \--executor-memory 512m \--executor-cores 1 \--num-executors 2 \--queue default \--class cn.test.spark._2SparkWordCount \/opt/spark-chapter01-1.0-SNAPSHOT.jar</code></pre></div><h3 id="Spark-submit"><a href="#Spark-submit" class="headerlink" title="Spark-submit"></a>Spark-submit</h3><div class="code-wrapper"><pre><code class="hljs bash">【 Run application <span class="hljs-built_in">local</span> on 8 cores】/<span class="hljs-built_in">export</span>/server/spark/bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master <span class="hljs-built_in">local</span>[8] \<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \  100<span class="hljs-comment"># Run on a Spark standalone cluster in client deploy mode</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master spark://207.184.161.138:7077 \  --executor-memory 20G \  --total-executor-cores 100 \<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/examples/jars/spark-examples_2.11-2.4.5.jar \  1000<span class="hljs-comment"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master spark://207.184.161.138:7077 \  --deploy-mode cluster \  --supervise \  --executor-memory 20G \  --total-executor-cores 100 \  /path/to/examples.jar \  1000<span class="hljs-comment"># Run on a YARN cluster</span><span class="hljs-built_in">export</span> HADOOP_CONF_DIR=XXX./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master yarn \  --deploy-mode cluster \  <span class="hljs-comment"># can be client for client mode</span>  --executor-memory 20G \  --num-executors 50 \  /path/to/examples.jar \  1000<span class="hljs-comment"># Run a Python application on a Spark standalone cluster</span>./bin/spark-submit \  --master spark://207.184.161.138:7077 \  examples/src/main/python/pi.py \  1000<span class="hljs-comment"># Run on a Mesos cluster in cluster deploy mode with supervise</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master mesos://207.184.161.138:7077 \  --deploy-mode cluster \  --supervise \  --executor-memory 20G \  --total-executor-cores 100 \  http://path/to/examples.jar \  1000<span class="hljs-comment"># Run on a Kubernetes cluster in cluster deploy mode</span>./bin/spark-submit \  --class org.apache.spark.examples.SparkPi \  --master k8s://xx.yy.zz.ww:443 \  --deploy-mode cluster \  --executor-memory 20G \  --num-executors 50 \  http://path/to/examples.jar \  1000</code></pre></div><h2 id="Sqoop数据抽取和数据验证"><a href="#Sqoop数据抽取和数据验证" class="headerlink" title="Sqoop数据抽取和数据验证"></a>Sqoop数据抽取和数据验证</h2><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SQOOP_HOME=/<span class="hljs-built_in">export</span>/server/sqoop-1.4.7.bin_hadoop-2.6.0<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop import \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--table dd_table \--hive-table insurance_ods.dd_table \--hive-import \--hive-overwrite \--fields-terminated-by <span class="hljs-string">&#x27;\t&#x27;</span> \--delete-target-dir \-m 1<span class="hljs-comment">#1、查询MySQL的表dd_table的条数</span>mysql_log=`<span class="hljs-variable">$SQOOP_HOME</span>/bin/sqoop <span class="hljs-built_in">eval</span> \--connect jdbc:mysql://192.168.88.163:3306/insurance \--username root \--password 123456 \--query <span class="hljs-string">&quot;select count(1) from dd_table&quot;</span>`mysql_cnt=`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$mysql_log</span> | awk -F<span class="hljs-string">&#x27;|&#x27;</span> &#123;<span class="hljs-string">&#x27;print $4&#x27;</span>&#125; | awk &#123;<span class="hljs-string">&#x27;print $1&#x27;</span>&#125;`<span class="hljs-comment">#2、查询hive的表dd_table的条数</span>hive_log=`hive -e <span class="hljs-string">&quot;select count(1) from insurance_ods.dd_table&quot;</span>`<span class="hljs-comment">#3、比较2边的数字是否一样。</span><span class="hljs-keyword">if</span> [ <span class="hljs-variable">$mysql_cnt</span> -eq <span class="hljs-variable">$hive_log</span> ] ; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,是相等的&quot;</span><span class="hljs-keyword">else</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;mysql表的数据量=<span class="hljs-variable">$mysql_cnt</span>,hive表的数据量=<span class="hljs-variable">$hive_log</span>,不是相等的&quot;</span><span class="hljs-keyword">fi</span></code></pre></div><h2 id="FLink"><a href="#FLink" class="headerlink" title="FLink"></a>FLink</h2><h3 id="Flink-on-Yarn"><a href="#Flink-on-Yarn" class="headerlink" title="Flink on Yarn"></a>Flink on Yarn</h3><ul><li>Session 模式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 先创建 Session 会话， d 表示后台运行，s 表示每个 jm 的 slot 个数</span>flink/bin/yarn-session.sh -d -jm 1024 -tm 1024 -s 2<span class="hljs-comment"># 提交任务</span>flink/bin/flink run /<span class="hljs-built_in">export</span>/server/flink/examples/batch/WordCount.jar \--input hdfs://node1.test.cn:8020/wordcount/input</code></pre></div><ul><li>Job 分离模式</li></ul><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 直接提交任务，m 表示 jm 的地址,环境变量需要提前配置</span>/<span class="hljs-built_in">export</span>/server/flink/bin/flink run \-m yarn-cluster -yjm 1024 -ytm 1024 \/<span class="hljs-built_in">export</span>/server/flink/examples/batch/WordCount.jar \--input hdfs://node1.test.cn:8020/wordcount/input</code></pre></div><h2 id="其它命令"><a href="#其它命令" class="headerlink" title="其它命令"></a>其它命令</h2><h3 id="ES-启动"><a href="#ES-启动" class="headerlink" title="ES 启动"></a>ES 启动</h3><div class="code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /<span class="hljs-built_in">export</span>/server/es/elasticsearch-7.6.1//<span class="hljs-built_in">export</span>/server/es/elasticsearch-7.6.1/bin/elasticsearch &gt;&gt;/dev/null 2&gt;&amp;1 &amp;</code></pre></div><h3 id="markdown代码折叠"><a href="#markdown代码折叠" class="headerlink" title="markdown代码折叠"></a>markdown代码折叠</h3><div class="code-wrapper"><pre><code class="hljs markdown"><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span></span>点击查看完整代码<span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span></span><span class="xml"><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></span></code></pre></div><h3 id="免秘钥登录"><a href="#免秘钥登录" class="headerlink" title="免秘钥登录"></a>免秘钥登录</h3><div class="code-wrapper"><pre><code class="hljs bash">ssh-keygen -t rsassh-copy-id node1scp /root/.ssh/authorized_keys node2:/root/.sshscp /root/.ssh/authorized_keys node3:/root/.ssh</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>存档</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Shell</tag>
      
      <tag>Hive</tag>
      
      <tag>Spark</tag>
      
      <tag>Sqoop</tag>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="/2020/04/27/first_post/"/>
    <url>/2020/04/27/first_post/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/01/07/DO2Jlm6oKBYgEfN.jpg"></p><p>努力写博客, 总结经验教训, 学习永远在路上<br>感觉 GitHub Page 真的太方便了，随时随地可以开始写<br>打算把常用的资料文档命令放到博客上，debug 记录也放上来，还有学习笔记与项目总结</p>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
